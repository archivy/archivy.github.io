{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Logo design by Roy Quilor is licensed under CC BY-NC 4.0 Archivy Archivy is a self-hosted knowledge repository that allows you to safely preserve useful content that contributes to your own personal, searchable and extensible wiki. Features: If you add bookmarks, their webpages contents' will be saved to ensure that you will always have access to it, following the idea of digital preservation . Login module that allows you to host the service on a server Plugin system to allow people to publish and write extensions to archivy Notes are stored in an extended markdown format with footnotes, LaTeX math rendering, syntax highlighting and more. CLI that provides a nice backend interface to the app Backend API for flexibility and user enhancements Everything is a file! For ease of access and editing, all the content is stored in markdown files with yaml front matter. Extensible search with Elasticsearch and its Query DSL Dark Theme Upcoming: Links between different knowledge base items Multi User System with permission setup. Major improvements in UI Option to compile data to a static site. Quickstart Install archivy with pip install archivy . Other installations methods are listed here Then run this and enter a password to create a new user: $ archivy create-admin <username> Finally, execute archivy run to serve the app. You can open it at https://localhost:5000 and login with the credentials you entered before. You can then use archivy to create notes, bookmarks and then organize and store information. Changelog","title":"Index"},{"location":"#archivy","text":"Archivy is a self-hosted knowledge repository that allows you to safely preserve useful content that contributes to your own personal, searchable and extensible wiki. Features: If you add bookmarks, their webpages contents' will be saved to ensure that you will always have access to it, following the idea of digital preservation . Login module that allows you to host the service on a server Plugin system to allow people to publish and write extensions to archivy Notes are stored in an extended markdown format with footnotes, LaTeX math rendering, syntax highlighting and more. CLI that provides a nice backend interface to the app Backend API for flexibility and user enhancements Everything is a file! For ease of access and editing, all the content is stored in markdown files with yaml front matter. Extensible search with Elasticsearch and its Query DSL Dark Theme Upcoming: Links between different knowledge base items Multi User System with permission setup. Major improvements in UI Option to compile data to a static site.","title":"Archivy"},{"location":"#quickstart","text":"Install archivy with pip install archivy . Other installations methods are listed here Then run this and enter a password to create a new user: $ archivy create-admin <username> Finally, execute archivy run to serve the app. You can open it at https://localhost:5000 and login with the credentials you entered before. You can then use archivy to create notes, bookmarks and then organize and store information. Changelog","title":"Quickstart"},{"location":"CONTRIBUTING/","text":"This is a short guide on the things you should know if you'd like to contribute to Archivy. Setting up a dev environment. Fork the archivy repo and then clone the fork on your local machine. Create a virtual environment by running python -m venv venv/ . This will hold all archivy dependencies. Run source venv/bin/activate to activate this new environment. Run pip install requirements.txt to download all dependencies. If you'd like to work on an existing issue , please comment on the github thread for the issue to notify that you're working on it, and then create a new branch with a suitable name. For example, if you'd like to work on something about \"Improving the UI\", you'd call it improve_ui . Once you're done with your changes, you can open a pull request and we'll review them. Do not begin working on a new feature without first discussing it and opening an issue, as we might not agree with your vision. If your feature is more isolated and specific, it can also be interesting to develop a plugin for it, in which case we can help you with any questions related to plugin development, and would be happy to list your plugin on awesome-archivy . Thanks for contributing!","title":"Contributing"},{"location":"CONTRIBUTING/#setting-up-a-dev-environment","text":"Fork the archivy repo and then clone the fork on your local machine. Create a virtual environment by running python -m venv venv/ . This will hold all archivy dependencies. Run source venv/bin/activate to activate this new environment. Run pip install requirements.txt to download all dependencies. If you'd like to work on an existing issue , please comment on the github thread for the issue to notify that you're working on it, and then create a new branch with a suitable name. For example, if you'd like to work on something about \"Improving the UI\", you'd call it improve_ui . Once you're done with your changes, you can open a pull request and we'll review them. Do not begin working on a new feature without first discussing it and opening an issue, as we might not agree with your vision. If your feature is more isolated and specific, it can also be interesting to develop a plugin for it, in which case we can help you with any questions related to plugin development, and would be happy to list your plugin on awesome-archivy . Thanks for contributing!","title":"Setting up a dev environment."},{"location":"config/","text":"Once you've initialized your archivy install, a yaml archivy config is automatically generated. You can edit it by running archivy config . Here's an overview of the different values you can set and modify. General Variable Default Description USER_DIR System-dependent, see below. It is recommended to set this through archivy init Directory in which markdown data will be saved INTERNAL_DIR System-dependent, see below Directory where archivy internals will be stored (config, db...) PORT 5000 Port on which archivy will run HOST 127.0.0.1 Host on which the app will run. Elasticsearch All of these are children of the SEARCH_CONF object, like this in the yaml: SEARCH_CONF : enabled : url : # ... This part will not be configured by default unless you specify you wish to integrate with ES. Variable Default Description enabled 1 url http://localhost:9200 Url to the elasticsearch server search_conf Long dict of ES config options Configuration of Elasticsearch analyzer , mappings and general settings. INTERNAL_DIR and USER_DIR by default will be set by the appdirs python library: On Linux systems, it follows the XDG specification : ~/.local/share/archivy","title":"Config"},{"location":"config/#general","text":"Variable Default Description USER_DIR System-dependent, see below. It is recommended to set this through archivy init Directory in which markdown data will be saved INTERNAL_DIR System-dependent, see below Directory where archivy internals will be stored (config, db...) PORT 5000 Port on which archivy will run HOST 127.0.0.1 Host on which the app will run.","title":"General"},{"location":"config/#elasticsearch","text":"All of these are children of the SEARCH_CONF object, like this in the yaml: SEARCH_CONF : enabled : url : # ... This part will not be configured by default unless you specify you wish to integrate with ES. Variable Default Description enabled 1 url http://localhost:9200 Url to the elasticsearch server search_conf Long dict of ES config options Configuration of Elasticsearch analyzer , mappings and general settings. INTERNAL_DIR and USER_DIR by default will be set by the appdirs python library: On Linux systems, it follows the XDG specification : ~/.local/share/archivy","title":"Elasticsearch"},{"location":"difference/","text":"There are many great tools out there to create your knowledge base. So why should you use Archivy? Here are the ingredients that make Archivy stand out (of course many tools have other interesting components / focuses, and you should pick the one that resonates with what you want): Focus on scripting and extensibility : When I began developing archivy, I had plans for developing in the app's core additional features that would allow you to sync up to your digital presence, for exemple a Reddit extension that would download your upvoted posts, etc... I quickly realised that this would be a bad way to go, as many people often want maybe one feature, but not a ton of useless included extensions. That's when I decided to instead build a flexible framework for people to build installable plugins , as this allows a) users only download what they want and b) Archivy can focus on its core and users can build their own extensions for themselves and others. Importance of Digital Preservation : ^ I mentioned above the idea of a plugin for saving all your upvoted reddit posts. This is just an example of how Archivy is intended to be used not only as a knowledge base, but also a resilient stronghold for the data that used to be solely held by third-party services. This idea of automatically syncing and saving content you've found valuable could be expanded to HN upvoted posts, browser bookmarks, etc... 1 deployable OR you can just run it on your computer : The way archivy was engineered makes it possible for you to just run it on your laptop or pc, and still use it without problems. On the other hand, it is also very much a possibility to self-host it and expose it publicly for use from anywhere, which is why archivy has auth, and is also the motivator for our plans on adding a web editor . Powerful Search : Elasticsearch might be considered overkill for one's knowledge base, but as your knowledge base grows also with content from other data sources and automation, it can become a large amount of data. Elasticsearch provides very high quality search on this information at a swift speed. We still plan on adding an alternative search system that users can choose to use if they don't want to run ES. 2 https://beepb00p.xyz/hpi.html is an intriguing tool on this topic. \u21a9 See this thread if you have any tools in mind for this. \u21a9","title":"What makes Archivy different"},{"location":"docker/","text":"Guide to using Archivy with Docker This document contains enough information to help you get started with using Archivy as a container, in this case, with Docker(although you can use any other container runtime). This document will cover the following: Building a container image of Archivy Running Archivy as a container Quick start Running Archivy along with persistent data storage Running Archivy with environment variable injection Demo on \u2018 Play With Docker \u2019 Running Archivy using Docker Compose Running Archivy with Elasticsearch for full text search capabilities(with Docker Compose) Planned for the future: Running Archivy using Docker Swarm(container orchestrator) Running Archivy on Kubernetes as a production-ready setup Running Archivy on OpenStack as a production-ready setup(will be done only after the aforementioned points are completed) NOTE : Parts of the document may be incomplete as it is a work in progress. In time, more information will be added to each section/topic. If some part of the documentation is ambiguous, feel free to ask questions or make suggestions on the Issues page of the project. If necessary, additional revisions to the documentation can be made based on user feedback. Prerequisites Docker needs to be installed. You can check if Docker is installed by running $ docker --version Docker version 19 .03.12, build 48a66213fe If you don't have Docker installed, take a look at the official installation guide for your device. Building Archivy Building using Dockerfile The file Dockerfile is much more portable than the local-build.Dockerfile as you do not need any other additional files to build the image. The Dockerfile automatically downloads the source code during the build stage. Just download Dockerfile and run the following command to build the image: $ docker build -t archivy:1.0 -f /path/to/Dockerfile . This tags the image with the name archivy:1.0 . Technically, you do not need to mention the path to Dockerfile as long as the Dockerfile is titled Dockerfile . If the name is anything but that, you will need to mention the path using the -f flag. So the above command is the same as $ docker build -t archivy:1.0 . There's an easier way to build the image. You can pass the URL to the GitHub repository directly to docker , and as long as there\u2019s a file named Dockerfile in the root of the repository, Docker will build it. $ docker build -t archivy:1.0 https://github.com/Uzay-G/archivy.git#docker This will clone the GitHub repository and use the cloned repository as context. The Dockerfile at the root of the repository is used as Dockerfile . NOTE : The master branch of the Archivy repository does not contain any Dockerfiles. All files pertaining to Docker are available in the docker branch of this repository. Running Archivy Quick Start You can get an instance of Archivy up and running for testing purposes with the following one-liner: $ docker run -d --name archivy-test -p 5000 :5000 harshavardhanj/archivy --name \u2014\u2014 Name of the container(can be anything) -p/--publish host:container \u2014\u2014 Bind port on host to port on container The above command runs a container with the name archivy-test and binds port 5000 on the host to port 5000 on the container. That way, you can access the server at http://localhost:5000/ on the device. If you wish to access the server at http://localhost:9000/ instead, you would change the argument to -p 9000:5000 . This container runs in detached mode, meaning that no output is printed to the terminal. To view the logs, run $ docker logs -f [ name-of-container ] which will continuously stream the logs to the terminal. To exit, type ctrl+c . To just print the logs and exit, run the same command without the -f flag. NOTE : There\u2019s an image available on DockerHub for testing purposes. It is titled archivy and is available in the harshavardhanj repository on DockerHub. You can pull the image using $ docker pull harshavardhanj/archivy Running container in interactive mode You can also pass commands to the container by appending it to the docker run command. Keep in mind that the container will execute your commands and not run Archivy. This is useful if you want to find out what is going on inside the container. For example, $ docker run -it --name archivy-test -p 5000 :5000 harshavardhanj/archivy sh will start an interactive shell inside the container. Remember to pass the -it flags when you want access to a terminal inside the container. -i/--interactive \u2014 Keeps standard input open -t/--tty \u2014 Allocates a pseudo TTY With Data Persistence You can bind-mount any directory on your host to the data directory on the container in order to ensure that if and when the container is stopped/terminated, the data saved to the container isn\u2019t lost. This can be done as follows: $ docker run -d --name archivy -p 5000 :5000 -v /path/to/host/dir:/archivy/data harshavardhanj/archivy -v/--volume host:container \u2014\u2014 Bind-mount host path to container path The argument -v /path/to/host/dir:/archivy/data bind-mounts the directory /path/to/host/dir on the host to /archivy/data on the container. If you wish to mount, say /home/bob/data , you would first need to create the data directory at /home/bob , and then change the argument to -v /home/bob/data:/archivy/data . Environment Variable Injection You can inject environment variables while starting the container as follows: $ docker run -d --name archivy -p 5000 :5000 -v /path/to/host/dir:/archivy/data -e FLASK_DEBUG = 1 -e ELASTICSEARCH_ENABLED = 0 -e ELASTICSEARCH_URL = \"http://localhost:9200/\" harshavardhanj/archivy -e/--env KEY=value \u2014\u2014 Set the environment variable( key=value ) Multiple such environment variables can be specified during run time. For now, Archivy supports the following environment variables FLASK_DEBUG - Runs Flask in debug mode. More verbose output to console ELASTICSEARCH_ENABLED - Enables Elasticsearch support ELASTICSEARCH_URL - Sets the URL at which Elasticsearch listens If the values are not set by the user, they are assigned default values during run time. NOTE : If you wish to use Archivy with Elasticsearch, read on. The setup is explained in the Docker Compose section below. Try Demo on \u2018 Play With Docker \u2019 For those with DockerHub accounts, you can try a version of Archivy on Play With Docker by clicking on the badge below. This will require you to login to your DockerHub account. This version of archivy is based on the following Docker Compose file: version : '3.8' services : archivy : image : harshavardhanj/archivy ports : - \"5000:5000\" volumes : - archivyData:/archivy/data environment : - FLASK_DEBUG=0 - ELASTICSEARCH_ENABLED=0 volumes : archivyData : As is visible from the above compose file, this is a version of Archivy running without Elasticsearch enabled. Therefore, the search function will not work. Using Docker Compose Docker Compose is an easier way to bring up containers when compared to running lengthy docker run commands. The docker-compose.yaml file contains all the necessary information that you would normally provide to the docker run command, declared in a YAML format. Once this file is written, it as simple as running the following command in the same directory as the docker-compose.yml file: $ docker-compose up -d archivy This works as long as the compose file is named docker-compose.yml . If it has a different name, you will need to specify the path to the file as an argument to the -f flag as shown below: $ docker-compose -f ./compose.yml archivy up -d This repository contains two compose files(for now). A version of the simpler one is given below: version : '3.8' services : archivy : image : harshavardhanj/archivy ports : - \"5000:5000\" volumes : - archivyData:/archivy/data environment : - FLASK_DEBUG=1 - ELASTICSEARCH_ENABLED=0 volumes : archivyData : This file pulls the container image harshavardhanj/archivy from DockerHub. binds port 5000 on the host to port 5000 on the container. creates a named volume archivyData on the host and mounts it to the /archivy/data directory on the container(which is where all persistent data will be stored). Sets the following environment variables so that they can be used by Archivy during run time FLASK_DEBUG=1 ELASTICSEARCH_ENABLED=0 This would be the same as running the following commands: $ docker volume create archivyData $ docker run -d -p 5000 :5000 -v archivy:/archivy/data -e FLASK_DEBUG = 1 -e ELASTICSEARCH_ENABLED = 0 harshavardhanj/archivy When multiple container get involved, it becomes a lot easier to deal with compose files. Which compose file to use? There are currently two compose files in the repository: docker-compose.yml docker-compose-with-elasticsearch.yml If you would like to test Archivy, just download the docker-compose.yml and run $ docker-compose -f ./docker-compose.yml up -d The contents of the file are shown below: version : '3.8' services : archivy : image : harshavardhanj/archivy ports : - \"5000:5000\" volumes : - archivyData:/archivy/data environment : - FLASK_DEBUG=0 - ELASTICSEARCH_ENABLED=0 volumes : archivyData : This file pulls the archivy image. binds port 5000 on the host to port 5000 on the container. creates a named volume archivyData on the host and mounts it to the /archivy/data directory on the container(which is where all persistent data will be stored). NOTE : If you wish to bind-mount a folder to Archivy, modify the volumes as shown below: services : archivy : ... volumes : - ./archivyData:/archivy/data This will create a folder named archivyData in your current working directory. This is the folder in which all user-generated notes/bookmarks will be stored. If you wish to test Archivy\u2019s full capabilities with Elasticsearch, use the docker-compose-with-elasticsearch.yml . The usage of this file is explained in the next section. Running Archivy With Elasticsearch The compose file given below will do the following: Set up an instance/container of Elasticsearch that listens on port 9200. Set up an instance/container of Archivy that listens on port 5000. Ensure that the two can communicate with each by setting the approprate environment variables( ELASTICSEARCH_ENABLED and ELASTICSEARCH_URL ). Both containers have volumes attached to them to ensure data persistence. The containers will restart if the process running in the container fails and exits. version : '3.8' services : archivy : image : harshavardhanj/archivy ports : - target : 5000 published : 5000 protocol : tcp volumes : - archivyData:/archivy/data environment : - FLASK_DEBUG=0 - ELASTICSEARCH_ENABLED=1 - ELASTICSEARCH_URL=http://search:9200/ networks : - archivy depends_on : - elasticsearch deploy : replicas : 1 restart_policy : condition : on-failure delay : 10s max_attempts : 3 elasticsearch : image : elasticsearch:7.9.0 ports : - target : 9200 published : 9200 protocol : tcp volumes : - searchData:/usr/share/elasticsearch/data environment : - \"discovery.type=single-node\" networks : archivy : aliases : - search deploy : replicas : 1 restart_policy : condition : on-failure delay : 10s max_attempts : 3 networks : archivy : volumes : archivyData : searchData : For a detailed description of the declarations used in the compose file, refer to the official documentation . Given below is a simplified version of the same compose file which should be fine for testing purposes: version : '3.8' services : archivy : image : harshavardhanj/archivy ports : - \"5000:5000\" volumes : - ./archivyData:/archivy/data environment : - FLASK_DEBUG=0 - ELASTICSEARCH_ENABLED=1 - ELASTICSEARCH_URL=http://elasticsearch:9200/ depends_on : - elasticsearch elasticsearch : image : elasticsearch:7.9.0 ports : - \"9200:9200\" volumes : - ./searchData:/usr/share/elasticsearch/data environment : - \"discovery.type=single-node\" The declarations are described below: For the archivy service Pulls the harshavardhanj/archivy image. ( image: ) Connects port 5000 on the host to port 5000 on the container. ( ports: ) Creates the archivyData directory in the current working directory and bind-mounts it to the /archivy/data directory in the archivy container. ( volumes: ) Sets the following environment variables. ( environment: ) FLASK_DEBUG=0 ELASTICSEARCH_ENABLED=1 ( required to enable Elasticsearch support ) ELASTICSEARCH_URL=http://elasticsearch:9200/ ( required by Archivy to connect to Elasticsearch ) Sets a condition that the archivy container will start only after the elasticsearch container starts. ( depends_on: ) For the elasticsearch service Pulls the elasticsearch:7.9.0 image. ( image: ) Connects port 9200 on the host to port 9200 on the container Creates the searchData directory in the current working directory and bind-mounts it to the /usr/share/elasticsearch/data directory in the elasticsearch container.( volumes: ) Sets the following environment variable discovery.type=single-node Using Docker Swarm To be added","title":"Installing with Docker"},{"location":"docker/#guide-to-using-archivy-with-docker","text":"This document contains enough information to help you get started with using Archivy as a container, in this case, with Docker(although you can use any other container runtime). This document will cover the following: Building a container image of Archivy Running Archivy as a container Quick start Running Archivy along with persistent data storage Running Archivy with environment variable injection Demo on \u2018 Play With Docker \u2019 Running Archivy using Docker Compose Running Archivy with Elasticsearch for full text search capabilities(with Docker Compose) Planned for the future: Running Archivy using Docker Swarm(container orchestrator) Running Archivy on Kubernetes as a production-ready setup Running Archivy on OpenStack as a production-ready setup(will be done only after the aforementioned points are completed) NOTE : Parts of the document may be incomplete as it is a work in progress. In time, more information will be added to each section/topic. If some part of the documentation is ambiguous, feel free to ask questions or make suggestions on the Issues page of the project. If necessary, additional revisions to the documentation can be made based on user feedback.","title":"Guide to using Archivy with Docker"},{"location":"docker/#prerequisites","text":"Docker needs to be installed. You can check if Docker is installed by running $ docker --version Docker version 19 .03.12, build 48a66213fe If you don't have Docker installed, take a look at the official installation guide for your device.","title":"Prerequisites"},{"location":"docker/#building-archivy","text":"","title":"Building Archivy"},{"location":"docker/#building-using-dockerfile","text":"The file Dockerfile is much more portable than the local-build.Dockerfile as you do not need any other additional files to build the image. The Dockerfile automatically downloads the source code during the build stage. Just download Dockerfile and run the following command to build the image: $ docker build -t archivy:1.0 -f /path/to/Dockerfile . This tags the image with the name archivy:1.0 . Technically, you do not need to mention the path to Dockerfile as long as the Dockerfile is titled Dockerfile . If the name is anything but that, you will need to mention the path using the -f flag. So the above command is the same as $ docker build -t archivy:1.0 . There's an easier way to build the image. You can pass the URL to the GitHub repository directly to docker , and as long as there\u2019s a file named Dockerfile in the root of the repository, Docker will build it. $ docker build -t archivy:1.0 https://github.com/Uzay-G/archivy.git#docker This will clone the GitHub repository and use the cloned repository as context. The Dockerfile at the root of the repository is used as Dockerfile . NOTE : The master branch of the Archivy repository does not contain any Dockerfiles. All files pertaining to Docker are available in the docker branch of this repository.","title":"Building using Dockerfile"},{"location":"docker/#running-archivy","text":"","title":"Running Archivy"},{"location":"docker/#quick-start","text":"You can get an instance of Archivy up and running for testing purposes with the following one-liner: $ docker run -d --name archivy-test -p 5000 :5000 harshavardhanj/archivy --name \u2014\u2014 Name of the container(can be anything) -p/--publish host:container \u2014\u2014 Bind port on host to port on container The above command runs a container with the name archivy-test and binds port 5000 on the host to port 5000 on the container. That way, you can access the server at http://localhost:5000/ on the device. If you wish to access the server at http://localhost:9000/ instead, you would change the argument to -p 9000:5000 . This container runs in detached mode, meaning that no output is printed to the terminal. To view the logs, run $ docker logs -f [ name-of-container ] which will continuously stream the logs to the terminal. To exit, type ctrl+c . To just print the logs and exit, run the same command without the -f flag. NOTE : There\u2019s an image available on DockerHub for testing purposes. It is titled archivy and is available in the harshavardhanj repository on DockerHub. You can pull the image using $ docker pull harshavardhanj/archivy","title":"Quick Start"},{"location":"docker/#running-container-in-interactive-mode","text":"You can also pass commands to the container by appending it to the docker run command. Keep in mind that the container will execute your commands and not run Archivy. This is useful if you want to find out what is going on inside the container. For example, $ docker run -it --name archivy-test -p 5000 :5000 harshavardhanj/archivy sh will start an interactive shell inside the container. Remember to pass the -it flags when you want access to a terminal inside the container. -i/--interactive \u2014 Keeps standard input open -t/--tty \u2014 Allocates a pseudo TTY","title":"Running container in interactive mode"},{"location":"docker/#with-data-persistence","text":"You can bind-mount any directory on your host to the data directory on the container in order to ensure that if and when the container is stopped/terminated, the data saved to the container isn\u2019t lost. This can be done as follows: $ docker run -d --name archivy -p 5000 :5000 -v /path/to/host/dir:/archivy/data harshavardhanj/archivy -v/--volume host:container \u2014\u2014 Bind-mount host path to container path The argument -v /path/to/host/dir:/archivy/data bind-mounts the directory /path/to/host/dir on the host to /archivy/data on the container. If you wish to mount, say /home/bob/data , you would first need to create the data directory at /home/bob , and then change the argument to -v /home/bob/data:/archivy/data .","title":"With Data Persistence"},{"location":"docker/#environment-variable-injection","text":"You can inject environment variables while starting the container as follows: $ docker run -d --name archivy -p 5000 :5000 -v /path/to/host/dir:/archivy/data -e FLASK_DEBUG = 1 -e ELASTICSEARCH_ENABLED = 0 -e ELASTICSEARCH_URL = \"http://localhost:9200/\" harshavardhanj/archivy -e/--env KEY=value \u2014\u2014 Set the environment variable( key=value ) Multiple such environment variables can be specified during run time. For now, Archivy supports the following environment variables FLASK_DEBUG - Runs Flask in debug mode. More verbose output to console ELASTICSEARCH_ENABLED - Enables Elasticsearch support ELASTICSEARCH_URL - Sets the URL at which Elasticsearch listens If the values are not set by the user, they are assigned default values during run time. NOTE : If you wish to use Archivy with Elasticsearch, read on. The setup is explained in the Docker Compose section below.","title":"Environment Variable Injection"},{"location":"docker/#try-demo-on-play-with-docker","text":"For those with DockerHub accounts, you can try a version of Archivy on Play With Docker by clicking on the badge below. This will require you to login to your DockerHub account. This version of archivy is based on the following Docker Compose file: version : '3.8' services : archivy : image : harshavardhanj/archivy ports : - \"5000:5000\" volumes : - archivyData:/archivy/data environment : - FLASK_DEBUG=0 - ELASTICSEARCH_ENABLED=0 volumes : archivyData : As is visible from the above compose file, this is a version of Archivy running without Elasticsearch enabled. Therefore, the search function will not work.","title":"Try Demo on \u2018Play With Docker\u2019"},{"location":"docker/#using-docker-compose","text":"Docker Compose is an easier way to bring up containers when compared to running lengthy docker run commands. The docker-compose.yaml file contains all the necessary information that you would normally provide to the docker run command, declared in a YAML format. Once this file is written, it as simple as running the following command in the same directory as the docker-compose.yml file: $ docker-compose up -d archivy This works as long as the compose file is named docker-compose.yml . If it has a different name, you will need to specify the path to the file as an argument to the -f flag as shown below: $ docker-compose -f ./compose.yml archivy up -d This repository contains two compose files(for now). A version of the simpler one is given below: version : '3.8' services : archivy : image : harshavardhanj/archivy ports : - \"5000:5000\" volumes : - archivyData:/archivy/data environment : - FLASK_DEBUG=1 - ELASTICSEARCH_ENABLED=0 volumes : archivyData : This file pulls the container image harshavardhanj/archivy from DockerHub. binds port 5000 on the host to port 5000 on the container. creates a named volume archivyData on the host and mounts it to the /archivy/data directory on the container(which is where all persistent data will be stored). Sets the following environment variables so that they can be used by Archivy during run time FLASK_DEBUG=1 ELASTICSEARCH_ENABLED=0 This would be the same as running the following commands: $ docker volume create archivyData $ docker run -d -p 5000 :5000 -v archivy:/archivy/data -e FLASK_DEBUG = 1 -e ELASTICSEARCH_ENABLED = 0 harshavardhanj/archivy When multiple container get involved, it becomes a lot easier to deal with compose files.","title":"Using Docker Compose"},{"location":"docker/#which-compose-file-to-use","text":"There are currently two compose files in the repository: docker-compose.yml docker-compose-with-elasticsearch.yml If you would like to test Archivy, just download the docker-compose.yml and run $ docker-compose -f ./docker-compose.yml up -d The contents of the file are shown below: version : '3.8' services : archivy : image : harshavardhanj/archivy ports : - \"5000:5000\" volumes : - archivyData:/archivy/data environment : - FLASK_DEBUG=0 - ELASTICSEARCH_ENABLED=0 volumes : archivyData : This file pulls the archivy image. binds port 5000 on the host to port 5000 on the container. creates a named volume archivyData on the host and mounts it to the /archivy/data directory on the container(which is where all persistent data will be stored). NOTE : If you wish to bind-mount a folder to Archivy, modify the volumes as shown below: services : archivy : ... volumes : - ./archivyData:/archivy/data This will create a folder named archivyData in your current working directory. This is the folder in which all user-generated notes/bookmarks will be stored. If you wish to test Archivy\u2019s full capabilities with Elasticsearch, use the docker-compose-with-elasticsearch.yml . The usage of this file is explained in the next section.","title":"Which compose file to use?"},{"location":"docker/#running-archivy-with-elasticsearch","text":"The compose file given below will do the following: Set up an instance/container of Elasticsearch that listens on port 9200. Set up an instance/container of Archivy that listens on port 5000. Ensure that the two can communicate with each by setting the approprate environment variables( ELASTICSEARCH_ENABLED and ELASTICSEARCH_URL ). Both containers have volumes attached to them to ensure data persistence. The containers will restart if the process running in the container fails and exits. version : '3.8' services : archivy : image : harshavardhanj/archivy ports : - target : 5000 published : 5000 protocol : tcp volumes : - archivyData:/archivy/data environment : - FLASK_DEBUG=0 - ELASTICSEARCH_ENABLED=1 - ELASTICSEARCH_URL=http://search:9200/ networks : - archivy depends_on : - elasticsearch deploy : replicas : 1 restart_policy : condition : on-failure delay : 10s max_attempts : 3 elasticsearch : image : elasticsearch:7.9.0 ports : - target : 9200 published : 9200 protocol : tcp volumes : - searchData:/usr/share/elasticsearch/data environment : - \"discovery.type=single-node\" networks : archivy : aliases : - search deploy : replicas : 1 restart_policy : condition : on-failure delay : 10s max_attempts : 3 networks : archivy : volumes : archivyData : searchData : For a detailed description of the declarations used in the compose file, refer to the official documentation . Given below is a simplified version of the same compose file which should be fine for testing purposes: version : '3.8' services : archivy : image : harshavardhanj/archivy ports : - \"5000:5000\" volumes : - ./archivyData:/archivy/data environment : - FLASK_DEBUG=0 - ELASTICSEARCH_ENABLED=1 - ELASTICSEARCH_URL=http://elasticsearch:9200/ depends_on : - elasticsearch elasticsearch : image : elasticsearch:7.9.0 ports : - \"9200:9200\" volumes : - ./searchData:/usr/share/elasticsearch/data environment : - \"discovery.type=single-node\" The declarations are described below: For the archivy service Pulls the harshavardhanj/archivy image. ( image: ) Connects port 5000 on the host to port 5000 on the container. ( ports: ) Creates the archivyData directory in the current working directory and bind-mounts it to the /archivy/data directory in the archivy container. ( volumes: ) Sets the following environment variables. ( environment: ) FLASK_DEBUG=0 ELASTICSEARCH_ENABLED=1 ( required to enable Elasticsearch support ) ELASTICSEARCH_URL=http://elasticsearch:9200/ ( required by Archivy to connect to Elasticsearch ) Sets a condition that the archivy container will start only after the elasticsearch container starts. ( depends_on: ) For the elasticsearch service Pulls the elasticsearch:7.9.0 image. ( image: ) Connects port 9200 on the host to port 9200 on the container Creates the searchData directory in the current working directory and bind-mounts it to the /usr/share/elasticsearch/data directory in the elasticsearch container.( volumes: ) Sets the following environment variable discovery.type=single-node","title":"Running Archivy With Elasticsearch"},{"location":"docker/#using-docker-swarm","text":"To be added","title":"Using Docker Swarm"},{"location":"editing/","text":"Format Archivy files are in the markdown format following the commonmark spec . We've also included a few powerful extensions: LaTeX : you can render mathematical expressions like this: $$ \\pi = 3.14 $$ Footnotes : What does this describe? [^1] [ ^1 ]: test foot note. Tables : | Column 1 | Column 2 | | -------- | -------- | | ... | ... | Code blocks with syntax highlighting : ```python print ( \"this will be highlighted\" ) x = 1337 ``` There are several ways you can edit content in Archivy. Whenever you open a note or bookmark, at the bottom of the page you'll find a few buttons that allow you to edit it. Ways you can edit Through the web interface You can still do edits through the web app, by clicking \"Toggle web editor\" at the bottom. Locally You can do a local edit . This option is only viable if running archivy on your own computer. This will open the concerned file with the default app set to edit markdown. For example like this:","title":"Editing"},{"location":"editing/#format","text":"Archivy files are in the markdown format following the commonmark spec . We've also included a few powerful extensions: LaTeX : you can render mathematical expressions like this: $$ \\pi = 3.14 $$ Footnotes : What does this describe? [^1] [ ^1 ]: test foot note. Tables : | Column 1 | Column 2 | | -------- | -------- | | ... | ... | Code blocks with syntax highlighting : ```python print ( \"this will be highlighted\" ) x = 1337 ``` There are several ways you can edit content in Archivy. Whenever you open a note or bookmark, at the bottom of the page you'll find a few buttons that allow you to edit it.","title":"Format"},{"location":"editing/#ways-you-can-edit","text":"","title":"Ways you can edit"},{"location":"editing/#through-the-web-interface","text":"You can still do edits through the web app, by clicking \"Toggle web editor\" at the bottom.","title":"Through the web interface"},{"location":"editing/#locally","text":"You can do a local edit . This option is only viable if running archivy on your own computer. This will open the concerned file with the default app set to edit markdown. For example like this:","title":"Locally"},{"location":"install/","text":"With pip You can easily install archivy with pip . (pip is the default package installer for Python, you can use pip to install many packages and apps, see this link for more information if needed) Make sure your system has Python and pip installed. The Python programming language can also be downloaded from here . Install the python package with pip install archivy If you'd like to use search, follow these docs first and then do this part. Either way, run archivy init to create a new user and use the setup wizard. There you go! You should be able to start the app by running archivy run in your terminal and then just login. With Nix $ nix-env -i archivy With docker You can also use archivy with Docker. See Installing with docker for instructions on this. We might implement an AppImage install. Comment here if you'd like to see that happen.","title":"Installing Archivy"},{"location":"install/#with-pip","text":"You can easily install archivy with pip . (pip is the default package installer for Python, you can use pip to install many packages and apps, see this link for more information if needed) Make sure your system has Python and pip installed. The Python programming language can also be downloaded from here . Install the python package with pip install archivy If you'd like to use search, follow these docs first and then do this part. Either way, run archivy init to create a new user and use the setup wizard. There you go! You should be able to start the app by running archivy run in your terminal and then just login.","title":"With pip"},{"location":"install/#with-nix","text":"$ nix-env -i archivy","title":"With Nix"},{"location":"install/#with-docker","text":"You can also use archivy with Docker. See Installing with docker for instructions on this. We might implement an AppImage install. Comment here if you'd like to see that happen.","title":"With docker"},{"location":"plugins/","text":"Plugins Plugins are a newly introduced method to add extensions to the archivy cli and web interface. It relies on the extremely useful click-plugins package that is loaded through pip and the click-web which has been modified and whose can be found in archivy/click_web/ , some of the tests, templates and static files. To help you understand the way the plugin system works, we're going to build our own example plugin. We'll even deploy it to Pypi so that other people can install it. Note: The source code for this plugin is available here . We also recommend you read the overview of the reference beforehand so you can better use the wrapper methods archivy exposes. Prerequisites: A python and pip installation with archivy. Step 1: Defining what our archivy extension does Let's build a simple plugin that will automatically add some metadata (author name, location...) at the end of each note. Step 2: Setting up the project Make a new directory named archivy_extra_metadata that will be our plugin directory and create a new setup.py file that will define the characteristics of our package. Note: Using frontmatter is better suited for this functionality, but here we'll just want something simple that adds text directly at the end of the content, like: Made by John Doe in London. This is what our setup.py looks like: from setuptools import setup , find_packages setup ( name = 'archivy_extra_metadata' , version = '0.1.0' , author = \"Uzay-G\" , description = ( \"Archivy extension to add some metadata at the end of your notes / bookmarks.\" ), classifiers = [ \"Programming Language :: Python :: 3\" , ], packages = find_packages (), entry_points = ''' [archivy.plugins] extra-metadata=archivy_extra_metadata:extra_metadata ''' ) Let's walk through what this is doing: We specify some metadata you can adapt to your own package like name , author and description . We then load our package and source code by using the find_packages function. The entry_points is the most important: the [archivy.plugins] part tells archivy that this package's commands will directly extend the archivy CLI so we can call archivy extra-metadata in the command line. We will actually be creating a group of commands so users will call subcommands like this: archivy extra-metadata <subcommand> . You can do things either way. Step 3: Writing the core code of our plugin Create an archivy_extra_metadata directory inside the current directory where setup.py is stored. Create an __init__.py file in that directory where we'll store our main project code. For larger projects, it's better to separate concerns but that'll be good for now. This will be the skeleton structure of our __init__.py : import click # the package that manages the cli @click . group () def extra_metadata (): pass @extra_metadata . command () def command1 (): ... @extra_metadata . command () def command2 (): ... With this example structure, you'd be able to run commands like archivy extra_metadata command1 and archivy extra_metadata command2 . Read the click docs to learn about how to build more intricate commands / options. We also provide some custom option types like email and password Let's get into actually writing a command that interacts with the archivy codebase. We'll make a command called setup to allow users to setup their metadata by specifying their name and location: import click from archivy.helpers import get_db from archivy import app @click . group () def extra_metadata (): \"\"\"`archivy_extra_metadata` plugin to add metadata to your notes.\"\"\" pass @extra_metadata . command () @click . option ( \"--author\" , required = True ) @click . option ( \"--location\" , required = True ) def setup ( author , location ): \"\"\"Save metadata values.\"\"\" with app . app_context (): # save data in db get_db () . insert ({ \"type\" : \"metadata\" , \"author\" : author , \"location\" : location }) click . echo ( \"Metadata saved!\" ) The code above does a few things: It imports the archivy app that is basically the interface for the webserver and many essential Flask features (flask is the web framework archivy uses). It imports the get_db function that allows us to access and modify the database. We define our extra_metadata group of commands that will be the parent of our subcommands. We create a new command from that group with two parameters: author and location . An important part of the code is the with app.app_context() part, we need to run our code inside the archivy app_context to be able to call some of the archivy methods. If you call archivy methods in your plugins, it might fail if you don't include this part. Then we just have our command code that takes the arguments and saves them into the database . Now you just need to do pip install . in the main directory and you'll have access to the commands. Check it out by running archivy extra_metadata --help and then you can access the commands: $ archivy extra-metadata --help Usage: archivy extra-metadata [ OPTIONS ] COMMAND [ ARGS ] ... ` archivy_extra_metadata ` plugin to add metadata to your notes. Options: --help Show this message and exit. Commands: setup Save metadata values. $ archivy extra-metadata setup --help Usage: archivy extra-metadata setup [ OPTIONS ] Save metadata values. Options: --name TEXT [ required ] --location TEXT [ required ] --help Show this message and exit. $ archivy extra-metadata setup --author Uzay --location Europe Metadata saved! That's nice and all, but now we want to actually use this metadata! This wouldn't work with a cli command, because we want it to add metadata whenever a new note is created, not just when a command is executed. To do this, we'll use hooks . These allow the end user to configure what happens whenever an event is fired off, like the creation of a DataObj . Archivy saves a link to a user-specified directory where it stores all your data for Archivy. This directory is usually set during the execution of the archivy init command. It has this structure: data/ # content Creating a hooks.py file in the root of this directory is how archivy calls these events. For example, this could be a potential hook file: # import base hooks that our `Hooks` class inherits from archivy.config import BaseHooks class Hooks(BaseHooks): def on_dataobj_create(self, dataobj): print(\"New dataobj created!\") What we'll be doing is we'll use the before_dataobj_create event to add our metadata before the text is saved. Let's define a function at the end of the __init__.py file of our package that takes a DataObj as an argument and modifies it: from tinydb import Query # this is the db handler that we use and we need to make a Query def add_metadata(dataobj): with app.app_context(): metadata = get_db().search(Query().type == \"metadata\") dataobj.content += f\"Made by {metadata['author']} in {metadata['location']}\" Combined with our previous code: import click from tinydb import Query from archivy.helpers import get_db from archivy import app @click . group () def extra_metadata (): \"\"\"`archivy_extra_metadata` plugin to add metadata to your notes.\"\"\" pass @extra_metadata . command () @click . option ( \"--author\" , required = True ) @click . option ( \"--location\" , required = True ) def setup ( author , location ): \"\"\"Save metadata values.\"\"\" with app . app_context (): # save data in db get_db () . insert ({ \"type\" : \"metadata\" , \"author\" : author , \"location\" : location }) click . echo ( \"Metadata saved!\" ) def add_metadata ( dataobj ): with app . app_context (): metadata = get_db () . search ( Query () . type == \"metadata\" )[ 0 ] dataobj . content += f \" \\n Made by { metadata [ 'author' ] } in { metadata [ 'location' ] } .\" Now to enable our plugin, all people have to do is modify their hooks.py file like this for example: from archivy_extra_metadata import add_metadata from archivy.config import BaseHooks class Hooks : def before_dataobj_create ( self , dataobj ): add_metadata ( dataobj ) Users of your plugin will add the add_metadata(dataobj) to the before_dataobj_create call, and the metadata will be added automatically when the event is called. We now have a working setup: This is how our plugin will be used: Users install the package. They run archivy extra-metadata setup --author xxx --location xxx to set metadata. They modify their hooks.py to call add_metadata on the before_dataobj_create event. Voil\u00e0! However, we need a way for them to install this package. That brings us to Step 4. Step 4: Publishing our package to Pypi Pypi is the Python package repository. Publishing our package to it will allow other users to easily install our code onto their own archivy instance. This is a short overview of how you can upload your package. Check out this website for more info. This section is inspired by this useful tutorial . Make sure the required utilities are installed: python3 - m pip install -- user -- upgrade setuptools wheel Now run this command in the main directory to build the source: python3 setup . py sdist bdist_wheel Create an account on Pypi . Then go here and create a new API token; set its scope to all projects. Once you've saved your token, install twine , the program that will take care of the upload: python3 - m pip install -- user -- upgrade twine And you can finally upload your code! The username you should enter is __token__ and then the password is your API token. python3 - m twine upload dist /* You're done! Now that you've finished your package, you can share it if you'd like, publish it on a public git repository so other people can collaborate, and you can add it to the awesome_archivy github repo which is an official list of plugins built around Archivy. We'd also love to hear about it on our discord server !","title":"Index"},{"location":"plugins/#plugins","text":"Plugins are a newly introduced method to add extensions to the archivy cli and web interface. It relies on the extremely useful click-plugins package that is loaded through pip and the click-web which has been modified and whose can be found in archivy/click_web/ , some of the tests, templates and static files. To help you understand the way the plugin system works, we're going to build our own example plugin. We'll even deploy it to Pypi so that other people can install it. Note: The source code for this plugin is available here . We also recommend you read the overview of the reference beforehand so you can better use the wrapper methods archivy exposes. Prerequisites: A python and pip installation with archivy.","title":"Plugins"},{"location":"plugins/#step-1-defining-what-our-archivy-extension-does","text":"Let's build a simple plugin that will automatically add some metadata (author name, location...) at the end of each note.","title":"Step 1: Defining what our archivy extension does"},{"location":"plugins/#step-2-setting-up-the-project","text":"Make a new directory named archivy_extra_metadata that will be our plugin directory and create a new setup.py file that will define the characteristics of our package. Note: Using frontmatter is better suited for this functionality, but here we'll just want something simple that adds text directly at the end of the content, like: Made by John Doe in London. This is what our setup.py looks like: from setuptools import setup , find_packages setup ( name = 'archivy_extra_metadata' , version = '0.1.0' , author = \"Uzay-G\" , description = ( \"Archivy extension to add some metadata at the end of your notes / bookmarks.\" ), classifiers = [ \"Programming Language :: Python :: 3\" , ], packages = find_packages (), entry_points = ''' [archivy.plugins] extra-metadata=archivy_extra_metadata:extra_metadata ''' ) Let's walk through what this is doing: We specify some metadata you can adapt to your own package like name , author and description . We then load our package and source code by using the find_packages function. The entry_points is the most important: the [archivy.plugins] part tells archivy that this package's commands will directly extend the archivy CLI so we can call archivy extra-metadata in the command line. We will actually be creating a group of commands so users will call subcommands like this: archivy extra-metadata <subcommand> . You can do things either way.","title":"Step 2: Setting up the project"},{"location":"plugins/#step-3-writing-the-core-code-of-our-plugin","text":"Create an archivy_extra_metadata directory inside the current directory where setup.py is stored. Create an __init__.py file in that directory where we'll store our main project code. For larger projects, it's better to separate concerns but that'll be good for now. This will be the skeleton structure of our __init__.py : import click # the package that manages the cli @click . group () def extra_metadata (): pass @extra_metadata . command () def command1 (): ... @extra_metadata . command () def command2 (): ... With this example structure, you'd be able to run commands like archivy extra_metadata command1 and archivy extra_metadata command2 . Read the click docs to learn about how to build more intricate commands / options. We also provide some custom option types like email and password Let's get into actually writing a command that interacts with the archivy codebase. We'll make a command called setup to allow users to setup their metadata by specifying their name and location: import click from archivy.helpers import get_db from archivy import app @click . group () def extra_metadata (): \"\"\"`archivy_extra_metadata` plugin to add metadata to your notes.\"\"\" pass @extra_metadata . command () @click . option ( \"--author\" , required = True ) @click . option ( \"--location\" , required = True ) def setup ( author , location ): \"\"\"Save metadata values.\"\"\" with app . app_context (): # save data in db get_db () . insert ({ \"type\" : \"metadata\" , \"author\" : author , \"location\" : location }) click . echo ( \"Metadata saved!\" ) The code above does a few things: It imports the archivy app that is basically the interface for the webserver and many essential Flask features (flask is the web framework archivy uses). It imports the get_db function that allows us to access and modify the database. We define our extra_metadata group of commands that will be the parent of our subcommands. We create a new command from that group with two parameters: author and location . An important part of the code is the with app.app_context() part, we need to run our code inside the archivy app_context to be able to call some of the archivy methods. If you call archivy methods in your plugins, it might fail if you don't include this part. Then we just have our command code that takes the arguments and saves them into the database . Now you just need to do pip install . in the main directory and you'll have access to the commands. Check it out by running archivy extra_metadata --help and then you can access the commands: $ archivy extra-metadata --help Usage: archivy extra-metadata [ OPTIONS ] COMMAND [ ARGS ] ... ` archivy_extra_metadata ` plugin to add metadata to your notes. Options: --help Show this message and exit. Commands: setup Save metadata values. $ archivy extra-metadata setup --help Usage: archivy extra-metadata setup [ OPTIONS ] Save metadata values. Options: --name TEXT [ required ] --location TEXT [ required ] --help Show this message and exit. $ archivy extra-metadata setup --author Uzay --location Europe Metadata saved! That's nice and all, but now we want to actually use this metadata! This wouldn't work with a cli command, because we want it to add metadata whenever a new note is created, not just when a command is executed. To do this, we'll use hooks . These allow the end user to configure what happens whenever an event is fired off, like the creation of a DataObj . Archivy saves a link to a user-specified directory where it stores all your data for Archivy. This directory is usually set during the execution of the archivy init command. It has this structure: data/ # content Creating a hooks.py file in the root of this directory is how archivy calls these events. For example, this could be a potential hook file: # import base hooks that our `Hooks` class inherits from archivy.config import BaseHooks class Hooks(BaseHooks): def on_dataobj_create(self, dataobj): print(\"New dataobj created!\") What we'll be doing is we'll use the before_dataobj_create event to add our metadata before the text is saved. Let's define a function at the end of the __init__.py file of our package that takes a DataObj as an argument and modifies it: from tinydb import Query # this is the db handler that we use and we need to make a Query def add_metadata(dataobj): with app.app_context(): metadata = get_db().search(Query().type == \"metadata\") dataobj.content += f\"Made by {metadata['author']} in {metadata['location']}\" Combined with our previous code: import click from tinydb import Query from archivy.helpers import get_db from archivy import app @click . group () def extra_metadata (): \"\"\"`archivy_extra_metadata` plugin to add metadata to your notes.\"\"\" pass @extra_metadata . command () @click . option ( \"--author\" , required = True ) @click . option ( \"--location\" , required = True ) def setup ( author , location ): \"\"\"Save metadata values.\"\"\" with app . app_context (): # save data in db get_db () . insert ({ \"type\" : \"metadata\" , \"author\" : author , \"location\" : location }) click . echo ( \"Metadata saved!\" ) def add_metadata ( dataobj ): with app . app_context (): metadata = get_db () . search ( Query () . type == \"metadata\" )[ 0 ] dataobj . content += f \" \\n Made by { metadata [ 'author' ] } in { metadata [ 'location' ] } .\" Now to enable our plugin, all people have to do is modify their hooks.py file like this for example: from archivy_extra_metadata import add_metadata from archivy.config import BaseHooks class Hooks : def before_dataobj_create ( self , dataobj ): add_metadata ( dataobj ) Users of your plugin will add the add_metadata(dataobj) to the before_dataobj_create call, and the metadata will be added automatically when the event is called. We now have a working setup: This is how our plugin will be used: Users install the package. They run archivy extra-metadata setup --author xxx --location xxx to set metadata. They modify their hooks.py to call add_metadata on the before_dataobj_create event. Voil\u00e0! However, we need a way for them to install this package. That brings us to Step 4.","title":"Step 3: Writing the core code of our plugin"},{"location":"plugins/#step-4-publishing-our-package-to-pypi","text":"Pypi is the Python package repository. Publishing our package to it will allow other users to easily install our code onto their own archivy instance. This is a short overview of how you can upload your package. Check out this website for more info. This section is inspired by this useful tutorial . Make sure the required utilities are installed: python3 - m pip install -- user -- upgrade setuptools wheel Now run this command in the main directory to build the source: python3 setup . py sdist bdist_wheel Create an account on Pypi . Then go here and create a new API token; set its scope to all projects. Once you've saved your token, install twine , the program that will take care of the upload: python3 - m pip install -- user -- upgrade twine And you can finally upload your code! The username you should enter is __token__ and then the password is your API token. python3 - m twine upload dist /*","title":"Step 4: Publishing our package to Pypi"},{"location":"plugins/#youre-done","text":"Now that you've finished your package, you can share it if you'd like, publish it on a public git repository so other people can collaborate, and you can add it to the awesome_archivy github repo which is an official list of plugins built around Archivy. We'd also love to hear about it on our discord server !","title":"You're done!"},{"location":"setup-search/","text":"Archivy uses ElasticSearch to provide efficient full-text search. Instructions to install and run the service are provided here . Append these two lines to your elasticsearch.yml config file : http.cors.enabled : true http.cors.allow-origin : \"http://localhost:5000\" Then, when you run archivy init simply specify you have enabled ES to integrate it with archivy. You will now have full-text search on your knowledge base! Whenever you edit an item locally, it is better to have archivy running that way your changes will be automatically synced to Elasticsearch. Elasticsearch can be a hefty dependency, so if you have any ideas for something more light-weight that could be used as an alternative, please share on this thread .","title":"Search"},{"location":"usage/","text":"Archivy comes with a simple command line interface that you use on the backend to run archivy: Usage: archivy [OPTIONS] COMMAND [ARGS]... Options: --version Show the flask version --help Show this message and exit. Commands: config Open archivy config. create-admin Creates a new admin user format Format normal markdown files for archivy. index Sync content to Elasticsearch init Initialise your archivy application run Runs archivy web application shell Run a shell in the app context. unformat Convert archivy-formatted files back to normal markdown. Make sure you've configured Archivy by running archivy init , as outlined in install . If you'd like to add users, you can simply create new admin users with the create-admin command. Only give credentials to trusted people. If you have normal md files you'd like to migrate to archivy, move your files into your archivy data directory and then run archivy format <filenames> to make them conform to archivy's formatting . Run archivy unformat to convert the other way around. You can sync changes to files to the Elasticsearch index by running archivy index or by simply using the web editor which updates ES when you push a change. The config command allows you to play around with configuration and use shell if you'd like to play around with the archivy python API. You can then use archivy to create notes, bookmarks and to organize and store information. The web api is also useful to extend archivy, or plugins . These have been recently introduced, but you can check the existing plugins that you can install onto your instance here .","title":"Usage"},{"location":"whats_next/","text":"Now that you have a working installation and you know how to use, where can you go from here? If you'd like to write a plugin for archivy with standalone functionality you'd like to see, check out the plugin tutorial . You can also use our Web API ! If you notice any bugs or problems, or if you have any features you'd like to see, please open up an issue on GitHub . You can also directly talk to us on the archivy discord server !","title":"What's Next"},{"location":"example-plugin/","text":"","title":"Index"},{"location":"reference/architecture/","text":"This document is a general overview of how the different pieces of archivy interact and what technologies it uses. Reading this will be useful for people looking to access the inner archivy API to write plugins. Read this post to understand what the function of an architecture.md file is. Archivy is: A Flask web application. A click backend command line interface. You use the cli to run the app, and you'll probably be using the web application for direct usage of archivy. Data Storage DataObjs is the term used to denote a note or bookmark that is stored in your knowledge base (abbreviation for Data Object). These are stored in a directory on your filesystem of which you can configure the location . They are organized in markdown files with yaml front matter like this: --- date : 08-31-20 desc : '' id : 100 path : '' tags : [] title : ... type : note --- ... Archivy uses the python-frontmatter package to handle the parsing of these files. They can be organized into user-specified sub-directories. Check out the reference to see the methods archivy uses for this. Another storage method Archivy uses is TinyDB . This is a small, simple document-oriented database archivy gives you access to for persistent data you might want to store in archivy plugins. Use helpers.get_db to call the database. Search Archivy uses Elasticsearch to index and allow users to have full-text search on their knowledge bases. Elasticsearch requires configuration to have higher quality search results. You can check out the top-notch config archivy already uses by default here . Check out the helper methods archivy exposes for ES. Auth Archivy uses flask-login for auth. All endpoints require to be authenticated. When you run archivy for the first time, it creates an admin user for you and outputs a temporary password. In our roadmap we plan to extend our permission framework to have a multi-user system, and define configuration for the permissions of non-logged in users. In general we want to make things more flexible on the auth side. How bookmarks work One of the core features of archivy is being able to save webpages locally. The way this works is the conversion of the html of the page you specify to a simple, markdown file. We might want to extend this to also be able to save PDF, EPUB and other formats. You can find the reference for this part here . Further down the road, it'd be nice to add background processing and not only download the webpage, but also save the essential assets it loads for a more complete process. This feature of preserving web content aligns with the mission against link rot 1 . Plugins Plugins in archivy function as standalone python packages. The phenomenal click-plugins package allows us to do this by basically adding commands to the cli. So you create a python package where you specify commands to extend your pre-existing archivy cli. Then these added commands will be able to be used through the cli. But what makes plugins interesting is that you can actually also use the plugins through the web interface, without having access to the system running archivy. We use an adaptation of the click-web to convert your cli commands to interactive web forms. See this manifesto to learn more about this phenomenon. \u21a9","title":"Architecture"},{"location":"reference/architecture/#data-storage","text":"DataObjs is the term used to denote a note or bookmark that is stored in your knowledge base (abbreviation for Data Object). These are stored in a directory on your filesystem of which you can configure the location . They are organized in markdown files with yaml front matter like this: --- date : 08-31-20 desc : '' id : 100 path : '' tags : [] title : ... type : note --- ... Archivy uses the python-frontmatter package to handle the parsing of these files. They can be organized into user-specified sub-directories. Check out the reference to see the methods archivy uses for this. Another storage method Archivy uses is TinyDB . This is a small, simple document-oriented database archivy gives you access to for persistent data you might want to store in archivy plugins. Use helpers.get_db to call the database.","title":"Data Storage"},{"location":"reference/architecture/#search","text":"Archivy uses Elasticsearch to index and allow users to have full-text search on their knowledge bases. Elasticsearch requires configuration to have higher quality search results. You can check out the top-notch config archivy already uses by default here . Check out the helper methods archivy exposes for ES.","title":"Search"},{"location":"reference/architecture/#auth","text":"Archivy uses flask-login for auth. All endpoints require to be authenticated. When you run archivy for the first time, it creates an admin user for you and outputs a temporary password. In our roadmap we plan to extend our permission framework to have a multi-user system, and define configuration for the permissions of non-logged in users. In general we want to make things more flexible on the auth side.","title":"Auth"},{"location":"reference/architecture/#how-bookmarks-work","text":"One of the core features of archivy is being able to save webpages locally. The way this works is the conversion of the html of the page you specify to a simple, markdown file. We might want to extend this to also be able to save PDF, EPUB and other formats. You can find the reference for this part here . Further down the road, it'd be nice to add background processing and not only download the webpage, but also save the essential assets it loads for a more complete process. This feature of preserving web content aligns with the mission against link rot 1 .","title":"How bookmarks work"},{"location":"reference/architecture/#plugins","text":"Plugins in archivy function as standalone python packages. The phenomenal click-plugins package allows us to do this by basically adding commands to the cli. So you create a python package where you specify commands to extend your pre-existing archivy cli. Then these added commands will be able to be used through the cli. But what makes plugins interesting is that you can actually also use the plugins through the web interface, without having access to the system running archivy. We use an adaptation of the click-web to convert your cli commands to interactive web forms. See this manifesto to learn more about this phenomenon. \u21a9","title":"Plugins"},{"location":"reference/filesystem_layer/","text":"This module holds the methods used to access, modify, and delete components of the filesystem where Dataobjs are stored in Archivy. Directory Tree like file-structure used to build file navigation in Archiv create ( contents , title , path = '' ) Helper method to save a new dataobj onto the filesystem. contents : md file contents title - title used for filename path Source code in archivy/data.py def create ( contents , title , path = \"\" ): \"\"\" Helper method to save a new dataobj onto the filesystem. Parameters: - **contents**: md file contents - **title** - title used for filename - **path** \"\"\" path_to_md_file = get_data_dir () / path / f \" { secure_filename ( title ) } .md\" with open ( path_to_md_file , \"w\" , encoding = \"utf-8\" ) as file : file . write ( contents ) return path_to_md_file create_dir ( name ) Create dir of given name Source code in archivy/data.py def create_dir ( name ): \"\"\"Create dir of given name\"\"\" home_dir = get_data_dir () new_path = home_dir / name new_path . mkdir ( parents = True , exist_ok = True ) return str ( new_path . relative_to ( home_dir )) delete_dir ( name ) Deletes dir of given name Source code in archivy/data.py def delete_dir ( name ): \"\"\"Deletes dir of given name\"\"\" try : rmtree ( get_data_dir () / name ) return True except FileNotFoundError : return False delete_item ( dataobj_id ) Delete dataobj of given id Source code in archivy/data.py def delete_item ( dataobj_id ): \"\"\"Delete dataobj of given id\"\"\" file = get_by_id ( dataobj_id ) remove_from_index ( dataobj_id ) if file : Path ( file ) . unlink () format_file ( path ) Converts normal md of file at path to formatted archivy markdown file, with yaml front matter and a filename of format \"{id}-{old_filename}.md\" Source code in archivy/data.py def format_file ( path : str ): \"\"\" Converts normal md of file at `path` to formatted archivy markdown file, with yaml front matter and a filename of format \"{id}-{old_filename}.md\" \"\"\" from archivy.models import DataObj data_dir = get_data_dir () path = Path ( path ) if not path . exists (): return if path . is_dir (): for filename in path . iterdir (): format_file ( filename ) else : new_file = path . open () file_contents = new_file . read () new_file . close () try : # get relative path of object in `data` dir datapath = path . parent . resolve () . relative_to ( data_dir ) except ValueError : datapath = Path () note_dataobj = { \"title\" : path . name . replace ( \".md\" , \"\" ), \"content\" : file_contents , \"type\" : \"note\" , \"path\" : str ( datapath ) } dataobj = DataObj ( ** note_dataobj ) dataobj . insert () path . unlink () current_app . logger . info ( f \"Formatted and moved { str ( datapath / path . name ) } to { dataobj . fullpath } \" ) get_by_id ( dataobj_id ) Returns filename of dataobj of given id Source code in archivy/data.py def get_by_id ( dataobj_id ): \"\"\"Returns filename of dataobj of given id\"\"\" results = list ( get_data_dir () . rglob ( f \" { dataobj_id }{ FILE_GLOB } \" )) return results [ 0 ] if results else None get_data_dir () Returns the directory where dataobjs are stored Source code in archivy/data.py def get_data_dir (): \"\"\"Returns the directory where dataobjs are stored\"\"\" return Path ( current_app . config [ 'USER_DIR' ]) / \"data\" get_dirs () Gets all dir names where dataobjs are stored Source code in archivy/data.py def get_dirs (): \"\"\"Gets all dir names where dataobjs are stored\"\"\" # join glob matchers dirnames = [ str ( dir_path . relative_to ( get_data_dir ())) for dir_path in get_data_dir () . rglob ( \"*\" ) if dir_path . is_dir ()] # append name for root dir dirnames . append ( \"not classified\" ) return dirnames get_item ( dataobj_id ) Returns a Post object with the given dataobjs' attributes Source code in archivy/data.py def get_item ( dataobj_id ): \"\"\"Returns a Post object with the given dataobjs' attributes\"\"\" file = get_by_id ( dataobj_id ) if file : data = frontmatter . load ( file ) data [ \"fullpath\" ] = str ( file ) return data return None get_items ( collections = [], path = '' , structured = True , json_format = False ) Gets all dataobjs. collections - filter dataobj by type, eg. bookmark / note path - filter by path **structured: if set to True, will return a Directory object, otherwise data will just be returned as a list of dataobjs json_format : boolean value used internally to pre-process dataobjs to send back a json response. Source code in archivy/data.py def get_items ( collections = [], path = \"\" , structured = True , json_format = False ): \"\"\" Gets all dataobjs. Parameters: - **collections** - filter dataobj by type, eg. bookmark / note - **path** - filter by path - **structured: if set to True, will return a Directory object, otherwise data will just be returned as a list of dataobjs - **json_format**: boolean value used internally to pre-process dataobjs to send back a json response. \"\"\" datacont = Directory ( \"root\" ) if structured else [] home_dir = get_data_dir () for filename in home_dir . rglob ( path + \"*\" ): if structured : paths = filename . relative_to ( home_dir ) current_dir = datacont # iterate through paths for segment in paths . parts : if segment . endswith ( \".md\" ): data = frontmatter . load ( filename ) current_dir . child_files . append ( data ) else : # directory has not been saved in tree yet if segment not in current_dir . child_dirs : current_dir . child_dirs [ segment ] = Directory ( segment ) current_dir = current_dir . child_dirs [ segment ] else : if filename . parts [ - 1 ] . endswith ( \".md\" ): data = frontmatter . load ( filename ) if len ( collections ) == 0 or \\ any ([ collection == data [ \"type\" ] for collection in collections ]): if json_format : dict_dataobj = data . __dict__ # remove unnecessary yaml handler dict_dataobj . pop ( \"handler\" ) datacont . append ( dict_dataobj ) else : datacont . append ( data ) return datacont open_file ( path ) Cross platform way of opening file on user's computer Source code in archivy/data.py def open_file ( path ): \"\"\"Cross platform way of opening file on user's computer\"\"\" if platform . system () == \"Windows\" : os . startfile ( path ) elif platform . system () == \"Darwin\" : subprocess . Popen ([ \"open\" , path ]) else : subprocess . Popen ([ \"xdg-open\" , path ]) unformat_file ( path , out_dir ) Converts normal md of file at path to formatted archivy markdown file, with yaml front matter and a filename of format \"{id}-{old_filename}.md\" Source code in archivy/data.py def unformat_file ( path : str , out_dir : str ): \"\"\" Converts normal md of file at `path` to formatted archivy markdown file, with yaml front matter and a filename of format \"{id}-{old_filename}.md\" \"\"\" data_dir = get_data_dir () path = Path ( path ) out_dir = Path ( out_dir ) if not path . exists () and out_dir . exists () and out_dir . is_dir (): return if path . is_dir (): path . mkdir ( exist_ok = True ) for filename in path . iterdir (): unformat_file ( filename , str ( out_dir )) else : dataobj = frontmatter . load ( str ( path )) try : # get relative path of object in `data` dir datapath = path . parent . resolve () . relative_to ( data_dir ) except ValueError : datapath = Path () # create subdir if doesn't exist ( out_dir / datapath ) . mkdir ( exist_ok = True ) new_path = out_dir / datapath / f \" { dataobj . metadata [ 'title' ] } .md\" with new_path . open ( \"w\" ) as f : f . write ( dataobj . content ) current_app . logger . info ( f \"Unformatted and moved { str ( path ) } to { str ( new_path . resolve ()) } \" ) path . unlink () update_item ( dataobj_id , new_content ) Given an object id, this method overwrites the inner content of the post with new_content . This means that it won't change the frontmatter (eg tags, id, title) but it can change the file content. For example: If we have a dataobj like this: --- id: 1 title: Note --- # This is random Calling update_item(1, \"# This is specific\") will turn it into: --- id: 1 # unchanged title: Note --- # This is specific Source code in archivy/data.py def update_item ( dataobj_id , new_content ): \"\"\" Given an object id, this method overwrites the inner content of the post with `new_content`. This means that it won't change the frontmatter (eg tags, id, title) but it can change the file content. For example: If we have a dataobj like this: ```md --- id: 1 title: Note --- # This is random ``` Calling `update_item(1, \"# This is specific\")` will turn it into: ```md --- id: 1 # unchanged title: Note --- # This is specific ``` \"\"\" from archivy.models import DataObj filename = get_by_id ( dataobj_id ) dataobj = frontmatter . load ( filename ) dataobj . content = new_content md = frontmatter . dumps ( dataobj ) with open ( filename , \"w\" , encoding = \"utf-8\" ) as f : f . write ( md ) converted_dataobj = DataObj . from_md ( md ) converted_dataobj . fullpath = str ( filename . relative_to ( current_app . config [ \"USER_DIR\" ])) converted_dataobj . index () load_hooks () . on_edit ( converted_dataobj )","title":"Dataobj Filesystem Layer"},{"location":"reference/filesystem_layer/#archivy.data","text":"","title":"archivy.data"},{"location":"reference/filesystem_layer/#archivy.data.Directory","text":"Tree like file-structure used to build file navigation in Archiv","title":"Directory"},{"location":"reference/filesystem_layer/#archivy.data.create","text":"Helper method to save a new dataobj onto the filesystem. contents : md file contents title - title used for filename path Source code in archivy/data.py def create ( contents , title , path = \"\" ): \"\"\" Helper method to save a new dataobj onto the filesystem. Parameters: - **contents**: md file contents - **title** - title used for filename - **path** \"\"\" path_to_md_file = get_data_dir () / path / f \" { secure_filename ( title ) } .md\" with open ( path_to_md_file , \"w\" , encoding = \"utf-8\" ) as file : file . write ( contents ) return path_to_md_file","title":"create()"},{"location":"reference/filesystem_layer/#archivy.data.create_dir","text":"Create dir of given name Source code in archivy/data.py def create_dir ( name ): \"\"\"Create dir of given name\"\"\" home_dir = get_data_dir () new_path = home_dir / name new_path . mkdir ( parents = True , exist_ok = True ) return str ( new_path . relative_to ( home_dir ))","title":"create_dir()"},{"location":"reference/filesystem_layer/#archivy.data.delete_dir","text":"Deletes dir of given name Source code in archivy/data.py def delete_dir ( name ): \"\"\"Deletes dir of given name\"\"\" try : rmtree ( get_data_dir () / name ) return True except FileNotFoundError : return False","title":"delete_dir()"},{"location":"reference/filesystem_layer/#archivy.data.delete_item","text":"Delete dataobj of given id Source code in archivy/data.py def delete_item ( dataobj_id ): \"\"\"Delete dataobj of given id\"\"\" file = get_by_id ( dataobj_id ) remove_from_index ( dataobj_id ) if file : Path ( file ) . unlink ()","title":"delete_item()"},{"location":"reference/filesystem_layer/#archivy.data.format_file","text":"Converts normal md of file at path to formatted archivy markdown file, with yaml front matter and a filename of format \"{id}-{old_filename}.md\" Source code in archivy/data.py def format_file ( path : str ): \"\"\" Converts normal md of file at `path` to formatted archivy markdown file, with yaml front matter and a filename of format \"{id}-{old_filename}.md\" \"\"\" from archivy.models import DataObj data_dir = get_data_dir () path = Path ( path ) if not path . exists (): return if path . is_dir (): for filename in path . iterdir (): format_file ( filename ) else : new_file = path . open () file_contents = new_file . read () new_file . close () try : # get relative path of object in `data` dir datapath = path . parent . resolve () . relative_to ( data_dir ) except ValueError : datapath = Path () note_dataobj = { \"title\" : path . name . replace ( \".md\" , \"\" ), \"content\" : file_contents , \"type\" : \"note\" , \"path\" : str ( datapath ) } dataobj = DataObj ( ** note_dataobj ) dataobj . insert () path . unlink () current_app . logger . info ( f \"Formatted and moved { str ( datapath / path . name ) } to { dataobj . fullpath } \" )","title":"format_file()"},{"location":"reference/filesystem_layer/#archivy.data.get_by_id","text":"Returns filename of dataobj of given id Source code in archivy/data.py def get_by_id ( dataobj_id ): \"\"\"Returns filename of dataobj of given id\"\"\" results = list ( get_data_dir () . rglob ( f \" { dataobj_id }{ FILE_GLOB } \" )) return results [ 0 ] if results else None","title":"get_by_id()"},{"location":"reference/filesystem_layer/#archivy.data.get_data_dir","text":"Returns the directory where dataobjs are stored Source code in archivy/data.py def get_data_dir (): \"\"\"Returns the directory where dataobjs are stored\"\"\" return Path ( current_app . config [ 'USER_DIR' ]) / \"data\"","title":"get_data_dir()"},{"location":"reference/filesystem_layer/#archivy.data.get_dirs","text":"Gets all dir names where dataobjs are stored Source code in archivy/data.py def get_dirs (): \"\"\"Gets all dir names where dataobjs are stored\"\"\" # join glob matchers dirnames = [ str ( dir_path . relative_to ( get_data_dir ())) for dir_path in get_data_dir () . rglob ( \"*\" ) if dir_path . is_dir ()] # append name for root dir dirnames . append ( \"not classified\" ) return dirnames","title":"get_dirs()"},{"location":"reference/filesystem_layer/#archivy.data.get_item","text":"Returns a Post object with the given dataobjs' attributes Source code in archivy/data.py def get_item ( dataobj_id ): \"\"\"Returns a Post object with the given dataobjs' attributes\"\"\" file = get_by_id ( dataobj_id ) if file : data = frontmatter . load ( file ) data [ \"fullpath\" ] = str ( file ) return data return None","title":"get_item()"},{"location":"reference/filesystem_layer/#archivy.data.get_items","text":"Gets all dataobjs. collections - filter dataobj by type, eg. bookmark / note path - filter by path **structured: if set to True, will return a Directory object, otherwise data will just be returned as a list of dataobjs json_format : boolean value used internally to pre-process dataobjs to send back a json response. Source code in archivy/data.py def get_items ( collections = [], path = \"\" , structured = True , json_format = False ): \"\"\" Gets all dataobjs. Parameters: - **collections** - filter dataobj by type, eg. bookmark / note - **path** - filter by path - **structured: if set to True, will return a Directory object, otherwise data will just be returned as a list of dataobjs - **json_format**: boolean value used internally to pre-process dataobjs to send back a json response. \"\"\" datacont = Directory ( \"root\" ) if structured else [] home_dir = get_data_dir () for filename in home_dir . rglob ( path + \"*\" ): if structured : paths = filename . relative_to ( home_dir ) current_dir = datacont # iterate through paths for segment in paths . parts : if segment . endswith ( \".md\" ): data = frontmatter . load ( filename ) current_dir . child_files . append ( data ) else : # directory has not been saved in tree yet if segment not in current_dir . child_dirs : current_dir . child_dirs [ segment ] = Directory ( segment ) current_dir = current_dir . child_dirs [ segment ] else : if filename . parts [ - 1 ] . endswith ( \".md\" ): data = frontmatter . load ( filename ) if len ( collections ) == 0 or \\ any ([ collection == data [ \"type\" ] for collection in collections ]): if json_format : dict_dataobj = data . __dict__ # remove unnecessary yaml handler dict_dataobj . pop ( \"handler\" ) datacont . append ( dict_dataobj ) else : datacont . append ( data ) return datacont","title":"get_items()"},{"location":"reference/filesystem_layer/#archivy.data.open_file","text":"Cross platform way of opening file on user's computer Source code in archivy/data.py def open_file ( path ): \"\"\"Cross platform way of opening file on user's computer\"\"\" if platform . system () == \"Windows\" : os . startfile ( path ) elif platform . system () == \"Darwin\" : subprocess . Popen ([ \"open\" , path ]) else : subprocess . Popen ([ \"xdg-open\" , path ])","title":"open_file()"},{"location":"reference/filesystem_layer/#archivy.data.unformat_file","text":"Converts normal md of file at path to formatted archivy markdown file, with yaml front matter and a filename of format \"{id}-{old_filename}.md\" Source code in archivy/data.py def unformat_file ( path : str , out_dir : str ): \"\"\" Converts normal md of file at `path` to formatted archivy markdown file, with yaml front matter and a filename of format \"{id}-{old_filename}.md\" \"\"\" data_dir = get_data_dir () path = Path ( path ) out_dir = Path ( out_dir ) if not path . exists () and out_dir . exists () and out_dir . is_dir (): return if path . is_dir (): path . mkdir ( exist_ok = True ) for filename in path . iterdir (): unformat_file ( filename , str ( out_dir )) else : dataobj = frontmatter . load ( str ( path )) try : # get relative path of object in `data` dir datapath = path . parent . resolve () . relative_to ( data_dir ) except ValueError : datapath = Path () # create subdir if doesn't exist ( out_dir / datapath ) . mkdir ( exist_ok = True ) new_path = out_dir / datapath / f \" { dataobj . metadata [ 'title' ] } .md\" with new_path . open ( \"w\" ) as f : f . write ( dataobj . content ) current_app . logger . info ( f \"Unformatted and moved { str ( path ) } to { str ( new_path . resolve ()) } \" ) path . unlink ()","title":"unformat_file()"},{"location":"reference/filesystem_layer/#archivy.data.update_item","text":"Given an object id, this method overwrites the inner content of the post with new_content . This means that it won't change the frontmatter (eg tags, id, title) but it can change the file content. For example: If we have a dataobj like this: --- id: 1 title: Note --- # This is random Calling update_item(1, \"# This is specific\") will turn it into: --- id: 1 # unchanged title: Note --- # This is specific Source code in archivy/data.py def update_item ( dataobj_id , new_content ): \"\"\" Given an object id, this method overwrites the inner content of the post with `new_content`. This means that it won't change the frontmatter (eg tags, id, title) but it can change the file content. For example: If we have a dataobj like this: ```md --- id: 1 title: Note --- # This is random ``` Calling `update_item(1, \"# This is specific\")` will turn it into: ```md --- id: 1 # unchanged title: Note --- # This is specific ``` \"\"\" from archivy.models import DataObj filename = get_by_id ( dataobj_id ) dataobj = frontmatter . load ( filename ) dataobj . content = new_content md = frontmatter . dumps ( dataobj ) with open ( filename , \"w\" , encoding = \"utf-8\" ) as f : f . write ( md ) converted_dataobj = DataObj . from_md ( md ) converted_dataobj . fullpath = str ( filename . relative_to ( current_app . config [ \"USER_DIR\" ])) converted_dataobj . index () load_hooks () . on_edit ( converted_dataobj )","title":"update_item()"},{"location":"reference/helpers/","text":"This is a series of helper functions that could be useful for you. Notably, the get_db and get_elastic_client could help with writing an archivy plugin. get_db ( force_reconnect = False ) Returns the database object that you can use to store data persistently Source code in archivy/helpers.py def get_db ( force_reconnect = False ): \"\"\" Returns the database object that you can use to store data persistently \"\"\" if 'db' not in g or force_reconnect : g . db = TinyDB ( str ( Path ( current_app . config [ \"INTERNAL_DIR\" ]) / \"db.json\" )) return g . db get_elastic_client () Returns the elasticsearch client you can use to search and insert / delete data Source code in archivy/helpers.py def get_elastic_client (): \"\"\"Returns the elasticsearch client you can use to search and insert / delete data\"\"\" if not current_app . config [ \"SEARCH_CONF\" ][ \"enabled\" ]: return None es = Elasticsearch ( current_app . config [ \"SEARCH_CONF\" ][ \"url\" ]) try : health = es . cluster . health () except elasticsearch . exceptions . ConnectionError : current_app . logger . error ( \"Elasticsearch does not seem to be running on \" f \" { current_app . config [ 'SEARCH_CONF' ][ 'url' ] } . Please start \" \"it, for example with: sudo service elasticsearch restart\" ) current_app . logger . error ( \"You can disable Elasticsearch by modifying the `enabled` variable \" f \"in { str ( Path ( current_app . config [ 'INTERNAL_DIR' ]) / 'config.yml' ) } \" ) sys . exit ( 1 ) if health [ \"status\" ] not in ( \"yellow\" , \"green\" ): current_app . logger . warning ( \"Elasticsearch reports that it is not working \" \"properly. Search might not work. You can disable \" \"Elasticsearch by setting ELASTICSEARCH_ENABLED to 0.\" ) return es get_max_id () Returns the current maximum id of dataobjs in the database. Source code in archivy/helpers.py def get_max_id (): \"\"\"Returns the current maximum id of dataobjs in the database.\"\"\" db = get_db () max_id = db . search ( Query () . name == \"max_id\" ) if not max_id : db . insert ({ \"name\" : \"max_id\" , \"val\" : 0 }) return 0 return max_id [ 0 ][ \"val\" ] load_config ( path = '' ) Loads config.yml file and deserializes it to a python dict. Source code in archivy/helpers.py def load_config ( path = \"\" ): \"\"\"Loads `config.yml` file and deserializes it to a python dict.\"\"\" path = path or current_app . config [ \"INTERNAL_DIR\" ] with ( Path ( path ) / \"config.yml\" ) . open () as f : return yaml . load ( f . read (), Loader = yaml . FullLoader ) set_max_id ( val ) Sets a new max_id Source code in archivy/helpers.py def set_max_id ( val ): \"\"\"Sets a new max_id\"\"\" db = get_db () db . update ( operations . set ( \"val\" , val ), Query () . name == \"max_id\" ) write_config ( config ) Writes a new config dict to a config.yml file that will override defaults Source code in archivy/helpers.py def write_config ( config : dict ): \"\"\"Writes a new config dict to a `config.yml` file that will override defaults\"\"\" with ( Path ( current_app . config [ \"INTERNAL_DIR\" ]) / \"config.yml\" ) . open ( \"w\" ) as f : yaml . dump ( config , f )","title":"Helpers"},{"location":"reference/helpers/#archivy.helpers","text":"","title":"archivy.helpers"},{"location":"reference/helpers/#archivy.helpers.get_db","text":"Returns the database object that you can use to store data persistently Source code in archivy/helpers.py def get_db ( force_reconnect = False ): \"\"\" Returns the database object that you can use to store data persistently \"\"\" if 'db' not in g or force_reconnect : g . db = TinyDB ( str ( Path ( current_app . config [ \"INTERNAL_DIR\" ]) / \"db.json\" )) return g . db","title":"get_db()"},{"location":"reference/helpers/#archivy.helpers.get_elastic_client","text":"Returns the elasticsearch client you can use to search and insert / delete data Source code in archivy/helpers.py def get_elastic_client (): \"\"\"Returns the elasticsearch client you can use to search and insert / delete data\"\"\" if not current_app . config [ \"SEARCH_CONF\" ][ \"enabled\" ]: return None es = Elasticsearch ( current_app . config [ \"SEARCH_CONF\" ][ \"url\" ]) try : health = es . cluster . health () except elasticsearch . exceptions . ConnectionError : current_app . logger . error ( \"Elasticsearch does not seem to be running on \" f \" { current_app . config [ 'SEARCH_CONF' ][ 'url' ] } . Please start \" \"it, for example with: sudo service elasticsearch restart\" ) current_app . logger . error ( \"You can disable Elasticsearch by modifying the `enabled` variable \" f \"in { str ( Path ( current_app . config [ 'INTERNAL_DIR' ]) / 'config.yml' ) } \" ) sys . exit ( 1 ) if health [ \"status\" ] not in ( \"yellow\" , \"green\" ): current_app . logger . warning ( \"Elasticsearch reports that it is not working \" \"properly. Search might not work. You can disable \" \"Elasticsearch by setting ELASTICSEARCH_ENABLED to 0.\" ) return es","title":"get_elastic_client()"},{"location":"reference/helpers/#archivy.helpers.get_max_id","text":"Returns the current maximum id of dataobjs in the database. Source code in archivy/helpers.py def get_max_id (): \"\"\"Returns the current maximum id of dataobjs in the database.\"\"\" db = get_db () max_id = db . search ( Query () . name == \"max_id\" ) if not max_id : db . insert ({ \"name\" : \"max_id\" , \"val\" : 0 }) return 0 return max_id [ 0 ][ \"val\" ]","title":"get_max_id()"},{"location":"reference/helpers/#archivy.helpers.load_config","text":"Loads config.yml file and deserializes it to a python dict. Source code in archivy/helpers.py def load_config ( path = \"\" ): \"\"\"Loads `config.yml` file and deserializes it to a python dict.\"\"\" path = path or current_app . config [ \"INTERNAL_DIR\" ] with ( Path ( path ) / \"config.yml\" ) . open () as f : return yaml . load ( f . read (), Loader = yaml . FullLoader )","title":"load_config()"},{"location":"reference/helpers/#archivy.helpers.set_max_id","text":"Sets a new max_id Source code in archivy/helpers.py def set_max_id ( val ): \"\"\"Sets a new max_id\"\"\" db = get_db () db . update ( operations . set ( \"val\" , val ), Query () . name == \"max_id\" )","title":"set_max_id()"},{"location":"reference/helpers/#archivy.helpers.write_config","text":"Writes a new config dict to a config.yml file that will override defaults Source code in archivy/helpers.py def write_config ( config : dict ): \"\"\"Writes a new config dict to a `config.yml` file that will override defaults\"\"\" with ( Path ( current_app . config [ \"INTERNAL_DIR\" ]) / \"config.yml\" ) . open ( \"w\" ) as f : yaml . dump ( config , f )","title":"write_config()"},{"location":"reference/hooks/","text":"Class of methods users can inherit to configure and extend archivy with hooks. Usage: Archivy checks for the presence of a hooks.py file in the user directory that stores the data/ directory with your notes and bookmarks. This location is usually set during archivy init . Example hooks.py file: from archivy.config import BaseHooks class Hooks ( BaseHooks ): def on_edit ( self , dataobj ): print ( f \"Edit made to { dataobj . title } \" ) def before_dataobj_create ( self , dataobj ): from random import randint dataobj . content += f \" \\n This note's random number is { randint ( 1 , 10 ) } \" # ... If you have ideas for any other hooks you'd find useful if they were supported, please open an issue . before_dataobj_create ( self , dataobj ) Hook called immediately before dataobj creation. Source code in archivy/config.py def before_dataobj_create ( self , dataobj ): \"\"\"Hook called immediately before dataobj creation.\"\"\" on_dataobj_create ( self , dataobj ) Hook for dataobj creation. Source code in archivy/config.py def on_dataobj_create ( self , dataobj ): \"\"\"Hook for dataobj creation.\"\"\" on_edit ( self , dataobj ) Hook called whenever a user edits through the web interface or the API. Source code in archivy/config.py def on_edit ( self , dataobj ): \"\"\"Hook called whenever a user edits through the web interface or the API.\"\"\" on_user_create ( self , user ) Hook called after a new user is created. Source code in archivy/config.py def on_user_create ( self , user ): \"\"\"Hook called after a new user is created.\"\"\"","title":"Hooks"},{"location":"reference/hooks/#archivy.config.BaseHooks","text":"Class of methods users can inherit to configure and extend archivy with hooks.","title":"archivy.config.BaseHooks"},{"location":"reference/hooks/#usage","text":"Archivy checks for the presence of a hooks.py file in the user directory that stores the data/ directory with your notes and bookmarks. This location is usually set during archivy init . Example hooks.py file: from archivy.config import BaseHooks class Hooks ( BaseHooks ): def on_edit ( self , dataobj ): print ( f \"Edit made to { dataobj . title } \" ) def before_dataobj_create ( self , dataobj ): from random import randint dataobj . content += f \" \\n This note's random number is { randint ( 1 , 10 ) } \" # ... If you have ideas for any other hooks you'd find useful if they were supported, please open an issue .","title":"Usage:"},{"location":"reference/hooks/#archivy.config.BaseHooks.before_dataobj_create","text":"Hook called immediately before dataobj creation. Source code in archivy/config.py def before_dataobj_create ( self , dataobj ): \"\"\"Hook called immediately before dataobj creation.\"\"\"","title":"before_dataobj_create()"},{"location":"reference/hooks/#archivy.config.BaseHooks.on_dataobj_create","text":"Hook for dataobj creation. Source code in archivy/config.py def on_dataobj_create ( self , dataobj ): \"\"\"Hook for dataobj creation.\"\"\"","title":"on_dataobj_create()"},{"location":"reference/hooks/#archivy.config.BaseHooks.on_edit","text":"Hook called whenever a user edits through the web interface or the API. Source code in archivy/config.py def on_edit ( self , dataobj ): \"\"\"Hook called whenever a user edits through the web interface or the API.\"\"\"","title":"on_edit()"},{"location":"reference/hooks/#archivy.config.BaseHooks.on_user_create","text":"Hook called after a new user is created. Source code in archivy/config.py def on_user_create ( self , user ): \"\"\"Hook called after a new user is created.\"\"\"","title":"on_user_create()"},{"location":"reference/models/","text":"Internal API for the models Archivy uses in the backend that could be useful for writing plugins. DataObj Class that holds a data object (either a note or a bookmark). [Required to pass when creating a new object] type -> \"note\" or \"bookmark\" Note : - title Bookmark : url [Optional attrs that if passed, will be set by the class] desc tags content path [Handled by the code] id date For bookmarks, Run process_bookmark_url() once you've created it. For both types, run insert() if you want to create a new file in the db with their contents. extract_content ( self , beautsoup ) converts html bookmark url to optimized markdown Source code in archivy/models.py def extract_content ( self , beautsoup ): \"\"\"converts html bookmark url to optimized markdown\"\"\" stripped_tags = [ \"footer\" , \"nav\" ] url = self . url . rstrip ( \"/\" ) for tag in stripped_tags : if getattr ( beautsoup , tag ): getattr ( beautsoup , tag ) . extract () resources = beautsoup . find_all ([ \"a\" , \"img\" ]) for tag in resources : if tag . name == \"a\" : if tag . has_attr ( \"href\" ) and ( tag [ \"href\" ] . startswith ( \"/\" )): tag [ \"href\" ] = urljoin ( url , tag [ \"href\" ]) # check it's a normal link and not some sort of image # string returns the text content of the tag if not tag . string : # delete tag tag . decompose () elif tag . name == \"img\" and tag . has_attr ( \"src\" ) and ( tag [ \"src\" ] . startswith ( \"/\" ) or tag [ \"src\" ] . startswith ( \"./\" )): tag [ \"src\" ] = urljoin ( url , tag [ \"src\" ]) res = html2text ( str ( beautsoup ), bodywidth = 0 ) return res from_md ( md_content ) classmethod Class method to generate new dataobj from a well formatted markdown string Call like this: Dataobj . from_md ( content ) Source code in archivy/models.py @classmethod def from_md ( cls , md_content : str ): \"\"\" Class method to generate new dataobj from a well formatted markdown string Call like this: ```python Dataobj.from_md(content) ``` \"\"\" data = frontmatter . loads ( md_content ) dataobj = {} dataobj [ \"content\" ] = data . content for pair in [ \"tags\" , \"desc\" , \"id\" , \"title\" , \"path\" ]: try : dataobj [ pair ] = data [ pair ] except KeyError : # files sometimes get moved temporarily by applications while you edit # this can create bugs where the data is not loaded correctly # this handles that scenario as validation will simply fail and the event will # be ignored break dataobj [ \"type\" ] = \"processed-dataobj\" return cls ( ** dataobj ) insert ( self ) Creates a new file with the object's attributes Source code in archivy/models.py def insert ( self ): \"\"\"Creates a new file with the object's attributes\"\"\" if self . validate (): helpers . set_max_id ( helpers . get_max_id () + 1 ) self . id = helpers . get_max_id () self . date = datetime . now () hooks = helpers . load_hooks () hooks . before_dataobj_create ( self ) data = { \"type\" : self . type , \"desc\" : self . desc , \"title\" : str ( self . title ), \"date\" : self . date . strftime ( \" %x \" ) . replace ( \"/\" , \"-\" ), \"tags\" : self . tags , \"id\" : self . id , \"path\" : self . path } if self . type == \"bookmark\" or self . type == \"pocket_bookmark\" : data [ \"url\" ] = self . url # convert to markdown file dataobj = frontmatter . Post ( self . content ) dataobj . metadata = data self . fullpath = create ( frontmatter . dumps ( dataobj ), str ( self . id ) + \"-\" + dataobj [ \"date\" ] + \"-\" + dataobj [ \"title\" ], path = self . path , ) hooks . on_dataobj_create ( self ) self . index () return self . id return False process_bookmark_url ( self ) Process url to get content for bookmark Source code in archivy/models.py def process_bookmark_url ( self ): \"\"\"Process url to get content for bookmark\"\"\" if self . type not in ( \"bookmark\" , \"pocket_bookmark\" ) or not validators . url ( self . url ): return None try : url_request = requests . get ( self . url ) except Exception : flash ( f \"Could not retrieve { self . url } \\n \" ) self . wipe () return try : parsed_html = BeautifulSoup ( url_request . text , features = \"html.parser\" ) except Exception : flash ( f \"Could not parse { self . url } \\n \" ) self . wipe () return try : self . content = self . extract_content ( parsed_html ) except Exception : flash ( f \"Could not extract content from { self . url } \\n \" ) return parsed_title = parsed_html . title self . title = ( parsed_title . string if parsed_title is not None else self . url ) validate ( self ) Verifies that the content matches required validation constraints Source code in archivy/models.py def validate ( self ): \"\"\"Verifies that the content matches required validation constraints\"\"\" valid_url = ( self . type != \"bookmark\" or self . type != \"pocket_bookmark\" ) or ( isinstance ( self . url , str ) and validators . url ( self . url )) valid_title = isinstance ( self . title , str ) and self . title != \"\" valid_content = ( self . type not in ( \"bookmark\" , \"pocket_bookmark\" ) or isinstance ( self . content , str )) return valid_url and valid_title and valid_content wipe ( self ) Resets and invalidates dataobj Source code in archivy/models.py def wipe ( self ): \"\"\"Resets and invalidates dataobj\"\"\" self . title = \"\" self . desc = None self . content = \"\" User Model we use for User that inherits from flask login's UserMixin username password is_admin from_db ( db_object ) classmethod Takes a database object and turns it into a user Source code in archivy/models.py @classmethod def from_db ( cls , db_object ): \"\"\"Takes a database object and turns it into a user\"\"\" username = db_object [ \"username\" ] id = db_object . doc_id return cls ( username = username , id = id ) insert ( self ) Inserts the model from the database Source code in archivy/models.py def insert ( self ): \"\"\"Inserts the model from the database\"\"\" if not self . password : return False hashed_password = generate_password_hash ( self . password ) db = helpers . get_db () if db . search (( Query () . type == \"user\" ) & ( Query () . username == self . username )): return False db_user = { \"username\" : self . username , \"hashed_password\" : hashed_password , \"is_admin\" : self . is_admin , \"type\" : \"user\" } helpers . load_hooks () . on_user_create ( self ) return db . insert ( db_user )","title":"Models for User and DataObj"},{"location":"reference/models/#archivy.models","text":"","title":"archivy.models"},{"location":"reference/models/#archivy.models.DataObj","text":"Class that holds a data object (either a note or a bookmark). [Required to pass when creating a new object] type -> \"note\" or \"bookmark\" Note : - title Bookmark : url [Optional attrs that if passed, will be set by the class] desc tags content path [Handled by the code] id date For bookmarks, Run process_bookmark_url() once you've created it. For both types, run insert() if you want to create a new file in the db with their contents.","title":"DataObj"},{"location":"reference/models/#archivy.models.DataObj.extract_content","text":"converts html bookmark url to optimized markdown Source code in archivy/models.py def extract_content ( self , beautsoup ): \"\"\"converts html bookmark url to optimized markdown\"\"\" stripped_tags = [ \"footer\" , \"nav\" ] url = self . url . rstrip ( \"/\" ) for tag in stripped_tags : if getattr ( beautsoup , tag ): getattr ( beautsoup , tag ) . extract () resources = beautsoup . find_all ([ \"a\" , \"img\" ]) for tag in resources : if tag . name == \"a\" : if tag . has_attr ( \"href\" ) and ( tag [ \"href\" ] . startswith ( \"/\" )): tag [ \"href\" ] = urljoin ( url , tag [ \"href\" ]) # check it's a normal link and not some sort of image # string returns the text content of the tag if not tag . string : # delete tag tag . decompose () elif tag . name == \"img\" and tag . has_attr ( \"src\" ) and ( tag [ \"src\" ] . startswith ( \"/\" ) or tag [ \"src\" ] . startswith ( \"./\" )): tag [ \"src\" ] = urljoin ( url , tag [ \"src\" ]) res = html2text ( str ( beautsoup ), bodywidth = 0 ) return res","title":"extract_content()"},{"location":"reference/models/#archivy.models.DataObj.from_md","text":"Class method to generate new dataobj from a well formatted markdown string Call like this: Dataobj . from_md ( content ) Source code in archivy/models.py @classmethod def from_md ( cls , md_content : str ): \"\"\" Class method to generate new dataobj from a well formatted markdown string Call like this: ```python Dataobj.from_md(content) ``` \"\"\" data = frontmatter . loads ( md_content ) dataobj = {} dataobj [ \"content\" ] = data . content for pair in [ \"tags\" , \"desc\" , \"id\" , \"title\" , \"path\" ]: try : dataobj [ pair ] = data [ pair ] except KeyError : # files sometimes get moved temporarily by applications while you edit # this can create bugs where the data is not loaded correctly # this handles that scenario as validation will simply fail and the event will # be ignored break dataobj [ \"type\" ] = \"processed-dataobj\" return cls ( ** dataobj )","title":"from_md()"},{"location":"reference/models/#archivy.models.DataObj.insert","text":"Creates a new file with the object's attributes Source code in archivy/models.py def insert ( self ): \"\"\"Creates a new file with the object's attributes\"\"\" if self . validate (): helpers . set_max_id ( helpers . get_max_id () + 1 ) self . id = helpers . get_max_id () self . date = datetime . now () hooks = helpers . load_hooks () hooks . before_dataobj_create ( self ) data = { \"type\" : self . type , \"desc\" : self . desc , \"title\" : str ( self . title ), \"date\" : self . date . strftime ( \" %x \" ) . replace ( \"/\" , \"-\" ), \"tags\" : self . tags , \"id\" : self . id , \"path\" : self . path } if self . type == \"bookmark\" or self . type == \"pocket_bookmark\" : data [ \"url\" ] = self . url # convert to markdown file dataobj = frontmatter . Post ( self . content ) dataobj . metadata = data self . fullpath = create ( frontmatter . dumps ( dataobj ), str ( self . id ) + \"-\" + dataobj [ \"date\" ] + \"-\" + dataobj [ \"title\" ], path = self . path , ) hooks . on_dataobj_create ( self ) self . index () return self . id return False","title":"insert()"},{"location":"reference/models/#archivy.models.DataObj.process_bookmark_url","text":"Process url to get content for bookmark Source code in archivy/models.py def process_bookmark_url ( self ): \"\"\"Process url to get content for bookmark\"\"\" if self . type not in ( \"bookmark\" , \"pocket_bookmark\" ) or not validators . url ( self . url ): return None try : url_request = requests . get ( self . url ) except Exception : flash ( f \"Could not retrieve { self . url } \\n \" ) self . wipe () return try : parsed_html = BeautifulSoup ( url_request . text , features = \"html.parser\" ) except Exception : flash ( f \"Could not parse { self . url } \\n \" ) self . wipe () return try : self . content = self . extract_content ( parsed_html ) except Exception : flash ( f \"Could not extract content from { self . url } \\n \" ) return parsed_title = parsed_html . title self . title = ( parsed_title . string if parsed_title is not None else self . url )","title":"process_bookmark_url()"},{"location":"reference/models/#archivy.models.DataObj.validate","text":"Verifies that the content matches required validation constraints Source code in archivy/models.py def validate ( self ): \"\"\"Verifies that the content matches required validation constraints\"\"\" valid_url = ( self . type != \"bookmark\" or self . type != \"pocket_bookmark\" ) or ( isinstance ( self . url , str ) and validators . url ( self . url )) valid_title = isinstance ( self . title , str ) and self . title != \"\" valid_content = ( self . type not in ( \"bookmark\" , \"pocket_bookmark\" ) or isinstance ( self . content , str )) return valid_url and valid_title and valid_content","title":"validate()"},{"location":"reference/models/#archivy.models.DataObj.wipe","text":"Resets and invalidates dataobj Source code in archivy/models.py def wipe ( self ): \"\"\"Resets and invalidates dataobj\"\"\" self . title = \"\" self . desc = None self . content = \"\"","title":"wipe()"},{"location":"reference/models/#archivy.models.User","text":"Model we use for User that inherits from flask login's UserMixin username password is_admin","title":"User"},{"location":"reference/models/#archivy.models.User.from_db","text":"Takes a database object and turns it into a user Source code in archivy/models.py @classmethod def from_db ( cls , db_object ): \"\"\"Takes a database object and turns it into a user\"\"\" username = db_object [ \"username\" ] id = db_object . doc_id return cls ( username = username , id = id )","title":"from_db()"},{"location":"reference/models/#archivy.models.User.insert","text":"Inserts the model from the database Source code in archivy/models.py def insert ( self ): \"\"\"Inserts the model from the database\"\"\" if not self . password : return False hashed_password = generate_password_hash ( self . password ) db = helpers . get_db () if db . search (( Query () . type == \"user\" ) & ( Query () . username == self . username )): return False db_user = { \"username\" : self . username , \"hashed_password\" : hashed_password , \"is_admin\" : self . is_admin , \"type\" : \"user\" } helpers . load_hooks () . on_user_create ( self ) return db . insert ( db_user )","title":"insert()"},{"location":"reference/search/","text":"These are a few methods to interface between archivy and the elasticsearch instance. add_to_index ( model ) Adds dataobj to given index. If object of given id already exists, it will be updated. index - String of the ES Index. Archivy uses dataobj by default. model - Instance of archivy.models.Dataobj , the object you want to index. Source code in archivy/search.py def add_to_index ( model ): \"\"\" Adds dataobj to given index. If object of given id already exists, it will be updated. Params: - **index** - String of the ES Index. Archivy uses `dataobj` by default. - **model** - Instance of `archivy.models.Dataobj`, the object you want to index. \"\"\" es = get_elastic_client () if not es : return payload = {} for field in model . __searchable__ : payload [ field ] = getattr ( model , field ) es . index ( index = current_app . config [ \"SEARCH_CONF\" ][ \"index_name\" ], id = model . id , body = payload ) return True query_index ( query ) Returns search results for your given query Source code in archivy/search.py def query_index ( query ): \"\"\"Returns search results for your given query\"\"\" es = get_elastic_client () if not es : return [] search = es . search ( index = current_app . config [ \"SEARCH_CONF\" ][ \"index_name\" ], body = { \"query\" : { \"multi_match\" : { \"query\" : query , \"fields\" : [ \"*\" ], \"analyzer\" : \"rebuilt_standard\" } }, \"highlight\" : { \"fragment_size\" : 0 , \"fields\" : { \"content\" : { \"pre_tags\" : \"==\" , \"post_tags\" : \"==\" , } } } } ) hits = [] for hit in search [ \"hits\" ][ \"hits\" ]: formatted_hit = { \"id\" : hit [ \"_id\" ], \"title\" : hit [ \"_source\" ][ \"title\" ]} if \"highlight\" in hit : formatted_hit [ \"highlight\" ] = hit [ \"highlight\" ][ \"content\" ] hits . append ( formatted_hit ) return hits remove_from_index ( dataobj_id ) Removes object of given id Source code in archivy/search.py def remove_from_index ( dataobj_id ): \"\"\"Removes object of given id\"\"\" es = get_elastic_client () if not es : return es . delete ( index = current_app . config [ \"SEARCH_CONF\" ][ \"index_name\" ], id = dataobj_id )","title":"Search"},{"location":"reference/search/#archivy.search","text":"","title":"archivy.search"},{"location":"reference/search/#archivy.search.add_to_index","text":"Adds dataobj to given index. If object of given id already exists, it will be updated. index - String of the ES Index. Archivy uses dataobj by default. model - Instance of archivy.models.Dataobj , the object you want to index. Source code in archivy/search.py def add_to_index ( model ): \"\"\" Adds dataobj to given index. If object of given id already exists, it will be updated. Params: - **index** - String of the ES Index. Archivy uses `dataobj` by default. - **model** - Instance of `archivy.models.Dataobj`, the object you want to index. \"\"\" es = get_elastic_client () if not es : return payload = {} for field in model . __searchable__ : payload [ field ] = getattr ( model , field ) es . index ( index = current_app . config [ \"SEARCH_CONF\" ][ \"index_name\" ], id = model . id , body = payload ) return True","title":"add_to_index()"},{"location":"reference/search/#archivy.search.query_index","text":"Returns search results for your given query Source code in archivy/search.py def query_index ( query ): \"\"\"Returns search results for your given query\"\"\" es = get_elastic_client () if not es : return [] search = es . search ( index = current_app . config [ \"SEARCH_CONF\" ][ \"index_name\" ], body = { \"query\" : { \"multi_match\" : { \"query\" : query , \"fields\" : [ \"*\" ], \"analyzer\" : \"rebuilt_standard\" } }, \"highlight\" : { \"fragment_size\" : 0 , \"fields\" : { \"content\" : { \"pre_tags\" : \"==\" , \"post_tags\" : \"==\" , } } } } ) hits = [] for hit in search [ \"hits\" ][ \"hits\" ]: formatted_hit = { \"id\" : hit [ \"_id\" ], \"title\" : hit [ \"_source\" ][ \"title\" ]} if \"highlight\" in hit : formatted_hit [ \"highlight\" ] = hit [ \"highlight\" ][ \"content\" ] hits . append ( formatted_hit ) return hits","title":"query_index()"},{"location":"reference/search/#archivy.search.remove_from_index","text":"Removes object of given id Source code in archivy/search.py def remove_from_index ( dataobj_id ): \"\"\"Removes object of given id\"\"\" es = get_elastic_client () if not es : return es . delete ( index = current_app . config [ \"SEARCH_CONF\" ][ \"index_name\" ], id = dataobj_id )","title":"remove_from_index()"},{"location":"reference/web_api/","text":"The Archivy HTTP API allows you to run small scripts in any language that will interact with your archivy instance through HTTP. All calls must be first logged in with the login endpoint. Small example in Python This code uses the requests module to interact with the API: import requests # we create a new session that will allow us to login once s = requests . session () INSTANCE_URL = < your instance url > s . post ( f \" { INSTANCE_URL } /api/login\" , auth = ( < username > , < password > )) # once you've logged in - you can make authenticated requests to the api, like: resp = s . get ( f \" { INSTANCE_URL } /api/dataobjs\" ) . content ) Reference create_bookmark () Creates a new bookmark Parameters: All parameters are sent through the JSON body. - url (required) - desc - tags - path Source code in archivy/api.py @api_bp . route ( \"/bookmarks\" , methods = [ \"POST\" ]) def create_bookmark (): \"\"\" Creates a new bookmark **Parameters:** All parameters are sent through the JSON body. - **url** (required) - **desc** - **tags** - **path** \"\"\" json_data = request . get_json () bookmark = DataObj ( url = json_data [ 'url' ], desc = json_data . get ( 'desc' ), tags = json_data . get ( 'tags' ), path = json_data . get ( \"path\" , \"\" ), type = \"bookmark\" , ) bookmark . process_bookmark_url () bookmark_id = bookmark . insert () if bookmark_id : return jsonify ( bookmark_id = bookmark_id , ) return Response ( status = 400 ) create_folder () Creates new directory Parameter in JSON body: - path (required) - path of newdir Source code in archivy/api.py @api_bp . route ( \"/folders/new\" , methods = [ \"POST\" ]) def create_folder (): \"\"\" Creates new directory Parameter in JSON body: - **path** (required) - path of newdir \"\"\" directory = request . json . get ( \"path\" ) try : sanitized_name = data . create_dir ( directory ) except FileExistsError : return Response ( \"Directory already exists\" , status = 401 ) return Response ( sanitized_name , status = 200 ) create_note () Creates a new note. Parameters: All parameters are sent through the JSON body. - title (required) - content (required) - desc - tags - path Source code in archivy/api.py @api_bp . route ( \"/notes\" , methods = [ \"POST\" ]) def create_note (): \"\"\" Creates a new note. **Parameters:** All parameters are sent through the JSON body. - **title** (required) - **content** (required) - **desc** - **tags** - **path** \"\"\" json_data = request . get_json () note = DataObj ( title = json_data [ \"title\" ], content = json_data [ \"content\" ], desc = json_data . get ( \"desc\" ), tags = json_data . get ( \"tags\" ), path = json_data . get ( \"path\" , \"\" ), type = \"note\" ) note_id = note . insert () if note_id : return jsonify ( note_id = note_id ) return Response ( status = 400 ) delete_dataobj ( dataobj_id ) Deletes object of given id Source code in archivy/api.py @api_bp . route ( \"/dataobjs/<int:dataobj_id>\" , methods = [ \"DELETE\" ]) def delete_dataobj ( dataobj_id ): \"\"\"Deletes object of given id\"\"\" if not data . get_item ( dataobj_id ): return Response ( status = 404 ) data . delete_item ( dataobj_id ) return Response ( status = 204 ) delete_folder () Deletes directory. Parameter in JSON body: - path of dir to delete Source code in archivy/api.py @api_bp . route ( \"/folders/delete\" , methods = [ \"DELETE\" ]) def delete_folder (): \"\"\" Deletes directory. Parameter in JSON body: - **path** of dir to delete \"\"\" directory = request . json . get ( \"path\" ) if directory == \"\" : return Response ( \"Cannot delete root dir\" , status = 401 ) if data . delete_dir ( directory ): return Response ( \"Successfully deleted\" , status = 200 ) return Response ( \"Not found\" , status = 404 ) get_dataobj ( dataobj_id ) Returns dataobj of given id Source code in archivy/api.py @api_bp . route ( \"/dataobjs/<int:dataobj_id>\" ) def get_dataobj ( dataobj_id ): \"\"\"Returns dataobj of given id\"\"\" dataobj = data . get_item ( dataobj_id ) return jsonify ( dataobj_id = dataobj_id , title = dataobj [ \"title\" ], content = dataobj . content , md_path = dataobj [ \"fullpath\" ], ) if dataobj else Response ( status = 404 ) get_dataobjs () Gets all dataobjs Source code in archivy/api.py @api_bp . route ( \"/dataobjs\" , methods = [ \"GET\" ]) def get_dataobjs (): \"\"\"Gets all dataobjs\"\"\" cur_dir = data . get_items ( structured = False , json_format = True ) return jsonify ( cur_dir ) login () Logs in the API client using HTTP Basic Auth . Pass in the username and password of your account. Source code in archivy/api.py @api_bp . route ( \"/login\" , methods = [ \"POST\" ]) def login (): \"\"\" Logs in the API client using [HTTP Basic Auth](https://en.wikipedia.org/wiki/Basic_access_authentication). Pass in the username and password of your account. \"\"\" db = get_db () user = db . search ( Query () . username == request . authorization [ \"username\" ]) if ( user and check_password_hash ( user [ 0 ][ \"hashed_password\" ], request . authorization [ \"password\" ])): # user is verified so we can log him in from the db user = User . from_db ( user [ 0 ]) login_user ( user , remember = True ) return Response ( status = 200 ) return Response ( status = 401 ) search_elastic () Searches the instance. Request URL Parameter: - query Source code in archivy/api.py @api_bp . route ( \"/search\" , methods = [ \"GET\" ]) def search_elastic (): \"\"\" Searches the instance. Request URL Parameter: - **query** \"\"\" query = request . args . get ( \"query\" ) search_results = query_index ( query ) return jsonify ( search_results ) update_dataobj ( dataobj_id ) Updates object of given id. Paramter in JSON body: content : markdown text of new dataobj. Source code in archivy/api.py @api_bp . route ( \"/dataobjs/<int:dataobj_id>\" , methods = [ \"PUT\" ]) def update_dataobj ( dataobj_id ): \"\"\" Updates object of given id. Paramter in JSON body: - **content**: markdown text of new dataobj. \"\"\" if request . json . get ( \"content\" ): try : data . update_item ( dataobj_id , request . json . get ( \"content\" )) return Response ( status = 200 ) except BaseException : return Response ( status = 404 ) return Response ( \"Must provide content parameter\" , status = 401 )","title":"Web API"},{"location":"reference/web_api/#small-example-in-python","text":"This code uses the requests module to interact with the API: import requests # we create a new session that will allow us to login once s = requests . session () INSTANCE_URL = < your instance url > s . post ( f \" { INSTANCE_URL } /api/login\" , auth = ( < username > , < password > )) # once you've logged in - you can make authenticated requests to the api, like: resp = s . get ( f \" { INSTANCE_URL } /api/dataobjs\" ) . content )","title":"Small example in Python"},{"location":"reference/web_api/#reference","text":"","title":"Reference"},{"location":"reference/web_api/#archivy.api","text":"","title":"archivy.api"},{"location":"reference/web_api/#archivy.api.create_bookmark","text":"Creates a new bookmark Parameters: All parameters are sent through the JSON body. - url (required) - desc - tags - path Source code in archivy/api.py @api_bp . route ( \"/bookmarks\" , methods = [ \"POST\" ]) def create_bookmark (): \"\"\" Creates a new bookmark **Parameters:** All parameters are sent through the JSON body. - **url** (required) - **desc** - **tags** - **path** \"\"\" json_data = request . get_json () bookmark = DataObj ( url = json_data [ 'url' ], desc = json_data . get ( 'desc' ), tags = json_data . get ( 'tags' ), path = json_data . get ( \"path\" , \"\" ), type = \"bookmark\" , ) bookmark . process_bookmark_url () bookmark_id = bookmark . insert () if bookmark_id : return jsonify ( bookmark_id = bookmark_id , ) return Response ( status = 400 )","title":"create_bookmark()"},{"location":"reference/web_api/#archivy.api.create_folder","text":"Creates new directory Parameter in JSON body: - path (required) - path of newdir Source code in archivy/api.py @api_bp . route ( \"/folders/new\" , methods = [ \"POST\" ]) def create_folder (): \"\"\" Creates new directory Parameter in JSON body: - **path** (required) - path of newdir \"\"\" directory = request . json . get ( \"path\" ) try : sanitized_name = data . create_dir ( directory ) except FileExistsError : return Response ( \"Directory already exists\" , status = 401 ) return Response ( sanitized_name , status = 200 )","title":"create_folder()"},{"location":"reference/web_api/#archivy.api.create_note","text":"Creates a new note. Parameters: All parameters are sent through the JSON body. - title (required) - content (required) - desc - tags - path Source code in archivy/api.py @api_bp . route ( \"/notes\" , methods = [ \"POST\" ]) def create_note (): \"\"\" Creates a new note. **Parameters:** All parameters are sent through the JSON body. - **title** (required) - **content** (required) - **desc** - **tags** - **path** \"\"\" json_data = request . get_json () note = DataObj ( title = json_data [ \"title\" ], content = json_data [ \"content\" ], desc = json_data . get ( \"desc\" ), tags = json_data . get ( \"tags\" ), path = json_data . get ( \"path\" , \"\" ), type = \"note\" ) note_id = note . insert () if note_id : return jsonify ( note_id = note_id ) return Response ( status = 400 )","title":"create_note()"},{"location":"reference/web_api/#archivy.api.delete_dataobj","text":"Deletes object of given id Source code in archivy/api.py @api_bp . route ( \"/dataobjs/<int:dataobj_id>\" , methods = [ \"DELETE\" ]) def delete_dataobj ( dataobj_id ): \"\"\"Deletes object of given id\"\"\" if not data . get_item ( dataobj_id ): return Response ( status = 404 ) data . delete_item ( dataobj_id ) return Response ( status = 204 )","title":"delete_dataobj()"},{"location":"reference/web_api/#archivy.api.delete_folder","text":"Deletes directory. Parameter in JSON body: - path of dir to delete Source code in archivy/api.py @api_bp . route ( \"/folders/delete\" , methods = [ \"DELETE\" ]) def delete_folder (): \"\"\" Deletes directory. Parameter in JSON body: - **path** of dir to delete \"\"\" directory = request . json . get ( \"path\" ) if directory == \"\" : return Response ( \"Cannot delete root dir\" , status = 401 ) if data . delete_dir ( directory ): return Response ( \"Successfully deleted\" , status = 200 ) return Response ( \"Not found\" , status = 404 )","title":"delete_folder()"},{"location":"reference/web_api/#archivy.api.get_dataobj","text":"Returns dataobj of given id Source code in archivy/api.py @api_bp . route ( \"/dataobjs/<int:dataobj_id>\" ) def get_dataobj ( dataobj_id ): \"\"\"Returns dataobj of given id\"\"\" dataobj = data . get_item ( dataobj_id ) return jsonify ( dataobj_id = dataobj_id , title = dataobj [ \"title\" ], content = dataobj . content , md_path = dataobj [ \"fullpath\" ], ) if dataobj else Response ( status = 404 )","title":"get_dataobj()"},{"location":"reference/web_api/#archivy.api.get_dataobjs","text":"Gets all dataobjs Source code in archivy/api.py @api_bp . route ( \"/dataobjs\" , methods = [ \"GET\" ]) def get_dataobjs (): \"\"\"Gets all dataobjs\"\"\" cur_dir = data . get_items ( structured = False , json_format = True ) return jsonify ( cur_dir )","title":"get_dataobjs()"},{"location":"reference/web_api/#archivy.api.login","text":"Logs in the API client using HTTP Basic Auth . Pass in the username and password of your account. Source code in archivy/api.py @api_bp . route ( \"/login\" , methods = [ \"POST\" ]) def login (): \"\"\" Logs in the API client using [HTTP Basic Auth](https://en.wikipedia.org/wiki/Basic_access_authentication). Pass in the username and password of your account. \"\"\" db = get_db () user = db . search ( Query () . username == request . authorization [ \"username\" ]) if ( user and check_password_hash ( user [ 0 ][ \"hashed_password\" ], request . authorization [ \"password\" ])): # user is verified so we can log him in from the db user = User . from_db ( user [ 0 ]) login_user ( user , remember = True ) return Response ( status = 200 ) return Response ( status = 401 )","title":"login()"},{"location":"reference/web_api/#archivy.api.search_elastic","text":"Searches the instance. Request URL Parameter: - query Source code in archivy/api.py @api_bp . route ( \"/search\" , methods = [ \"GET\" ]) def search_elastic (): \"\"\" Searches the instance. Request URL Parameter: - **query** \"\"\" query = request . args . get ( \"query\" ) search_results = query_index ( query ) return jsonify ( search_results )","title":"search_elastic()"},{"location":"reference/web_api/#archivy.api.update_dataobj","text":"Updates object of given id. Paramter in JSON body: content : markdown text of new dataobj. Source code in archivy/api.py @api_bp . route ( \"/dataobjs/<int:dataobj_id>\" , methods = [ \"PUT\" ]) def update_dataobj ( dataobj_id ): \"\"\" Updates object of given id. Paramter in JSON body: - **content**: markdown text of new dataobj. \"\"\" if request . json . get ( \"content\" ): try : data . update_item ( dataobj_id , request . json . get ( \"content\" )) return Response ( status = 200 ) except BaseException : return Response ( status = 404 ) return Response ( \"Must provide content parameter\" , status = 401 )","title":"update_dataobj()"},{"location":"reference/web_inputs/","text":"When developing plugins, you may want to use custom HTML input types on the frontend, like email or password . Archivy currently allows you use these two types in your click options. For example: from archivy.click_web.web_click_types import EMAIL_TYPE , PASSWORD_TYPE @cli . command () @click . option ( \"--the_email\" , type = EMAIL_TYPE ) # this will validate the email format on the frontend and backend @click . option ( \"--password\" , type = PASSWORD_TYPE ) # type='password' on the HTML frontend. def login ( the_email , password ): ...","title":"Web Inputs For Plugins"}]}