{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Archivy Archivy is a self-hostable knowledge repository that allows you to learn and retain information in your own personal and extensible wiki. Features: If you add bookmarks, their web-pages contents' will be saved to ensure that you will always have access to it, following the idea of digital preservation . Archivy is also easily integrated with other services and your online accounts. Knowledge base organization with bidirectional links between notes, and embedded tags. Everything is a file! For ease of access and editing, all the content is stored in extended markdown files with yaml front matter. This format supports footnotes, LaTeX math rendering, syntax highlighting and more. Extensible plugin system and API for power users to take control of their knowledge process syncing options Powerful and advanced search. Image upload Roadmap Upcoming: Annotations Multi User System with permission setup. Quickstart Install archivy with pip install archivy . Other installations methods are listed here Then run this and enter a password to create a new user: $ archivy create-admin <username> Finally, execute archivy run to serve the app. You can open it at https://localhost:5000 and login with the credentials you entered before. You can then use archivy to create notes, bookmarks and then organize and store information. See the docs for information on other installation methods. Community Archivy is dedicated at building open and quality knowledge base software through collaboration and community discussion. To get news and updates on Archivy and its development, you can watch the archivy repository or follow @uzpg_ on Twitter . You can interact with us through the issue board and the more casual discord server . If you'd like to support the project and its development, you can also sponsor the Archivy maintainer. Note: If you're interested in the applications of AI to knowledge management, we're also working on this with Espial . License This project is licensed under the MIT License. See LICENSE for more information. The Archivy Logo is designed by Roy Quilor , licensed under CC BY-NC 4.0 Changelog","title":"Index"},{"location":"#archivy","text":"Archivy is a self-hostable knowledge repository that allows you to learn and retain information in your own personal and extensible wiki. Features: If you add bookmarks, their web-pages contents' will be saved to ensure that you will always have access to it, following the idea of digital preservation . Archivy is also easily integrated with other services and your online accounts. Knowledge base organization with bidirectional links between notes, and embedded tags. Everything is a file! For ease of access and editing, all the content is stored in extended markdown files with yaml front matter. This format supports footnotes, LaTeX math rendering, syntax highlighting and more. Extensible plugin system and API for power users to take control of their knowledge process syncing options Powerful and advanced search. Image upload Roadmap Upcoming: Annotations Multi User System with permission setup.","title":"Archivy"},{"location":"#quickstart","text":"Install archivy with pip install archivy . Other installations methods are listed here Then run this and enter a password to create a new user: $ archivy create-admin <username> Finally, execute archivy run to serve the app. You can open it at https://localhost:5000 and login with the credentials you entered before. You can then use archivy to create notes, bookmarks and then organize and store information. See the docs for information on other installation methods.","title":"Quickstart"},{"location":"#community","text":"Archivy is dedicated at building open and quality knowledge base software through collaboration and community discussion. To get news and updates on Archivy and its development, you can watch the archivy repository or follow @uzpg_ on Twitter . You can interact with us through the issue board and the more casual discord server . If you'd like to support the project and its development, you can also sponsor the Archivy maintainer. Note: If you're interested in the applications of AI to knowledge management, we're also working on this with Espial .","title":"Community"},{"location":"#license","text":"This project is licensed under the MIT License. See LICENSE for more information. The Archivy Logo is designed by Roy Quilor , licensed under CC BY-NC 4.0 Changelog","title":"License"},{"location":"CONTRIBUTING/","text":"This is a short guide on the things you should know if you'd like to contribute to Archivy. Setting up a dev environment. Fork the archivy repo and then clone the fork on your local machine. Create a virtual environment by running python -m venv venv/ . This will hold all archivy dependencies. Run source venv/bin/activate to activate this new environment. Run pip install -r requirements.txt to download all dependencies. Running the dev server. # after sourcing the virtualenv $ export FLASK_APP=archivy/__init__.py $ export FLASK_ENV=development $ flask run Running cli commands # after sourcing the virtualenv $ python -m archivy.cli --help If you'd like to work on an existing issue , please comment on the github thread for the issue to notify that you're working on it, and then create a new branch with a suitable name. For example, if you'd like to work on something about \"Improving the UI\", you'd call it improve_ui . Once you're done with your changes, you can open a pull request and we'll review them. Do not begin working on a new feature without first discussing it and opening an issue, as we might not agree with your vision. If your feature is more isolated and specific, it can also be interesting to develop a plugin for it, in which case we can help you with any questions related to plugin development, and would be happy to list your plugin on awesome-archivy . Thanks for contributing!","title":"Contributing"},{"location":"CONTRIBUTING/#setting-up-a-dev-environment","text":"Fork the archivy repo and then clone the fork on your local machine. Create a virtual environment by running python -m venv venv/ . This will hold all archivy dependencies. Run source venv/bin/activate to activate this new environment. Run pip install -r requirements.txt to download all dependencies.","title":"Setting up a dev environment."},{"location":"CONTRIBUTING/#running-the-dev-server","text":"# after sourcing the virtualenv $ export FLASK_APP=archivy/__init__.py $ export FLASK_ENV=development $ flask run","title":"Running the dev server."},{"location":"CONTRIBUTING/#running-cli-commands","text":"# after sourcing the virtualenv $ python -m archivy.cli --help If you'd like to work on an existing issue , please comment on the github thread for the issue to notify that you're working on it, and then create a new branch with a suitable name. For example, if you'd like to work on something about \"Improving the UI\", you'd call it improve_ui . Once you're done with your changes, you can open a pull request and we'll review them. Do not begin working on a new feature without first discussing it and opening an issue, as we might not agree with your vision. If your feature is more isolated and specific, it can also be interesting to develop a plugin for it, in which case we can help you with any questions related to plugin development, and would be happy to list your plugin on awesome-archivy . Thanks for contributing!","title":"Running cli commands"},{"location":"config/","text":"Once you've initialized your archivy install, an archivy config is automatically generated. You can edit it through the archivy interface by clicking on the gear icon on the top right of the navbar, or by running archivy config in your terminal. Here's an overview of the different values you can set and modify. General Variable Default Description USER_DIR System-dependent, see below. It is recommended to set this through archivy init Directory in which markdown data will be saved INTERNAL_DIR System-dependent, see below Directory where archivy internals will be stored (config, db...) PORT 5000 Port on which archivy will run HOST 127.0.0.1 Host on which the app will run. DEFAULT_BOOKMARKS_DIR empty string (represents the root directory) any subdirectory of the data/ directory with your notes. SITE_TITLE Archivy String value to be displayed in page title and headings. Scraping An in-progress configuration object to customize how you'd like bookmarking / scraping to work. The options are children of the SCRAPING_CONF object, like so: SCRAPING_CONF : save_images : ... Variable Default Description save_images False If true, whenever you save a bookmark, every linked image will also be downloaded locally. If you want to configure the scraping progress more, you can also create a scraping.py file in the root of your user directory. This file allows you to override the default bookmarking behavior for certain websites / links, which you can match with regex. Once it matches a link, you can either pass your own custom function for parsing, or simply pass a string, which corresponds to the CSS selector for the part of the page you want archivy to scrape. If there are several matches, only the first will be treated. Example that processes youtube videos: def process_videos ( data ): url = data . url # modify whatever you want / set the metadata data . title = \"Video - \" + url data . tags = [ \"video\" ] data . content = \"...\" # declare your patterns in this PATTERNS variable PATTERNS = { \"*youtube.com/*\" : process_videos } With this example, whenever you create a bookmark of a youtube video, instead of going through the default archival, your function will be called on the data. Example that tells archivy only to scrape the main body of Wikipedia pages: PATTERNS = { \"https://*.wikipedia.org/wiki/*\" : \"#bodyContent\" } Example patterns: *wikipedia* ( * matches everything) https://duckduckg?.com* (? matches a single character) https://www.[nl][ya]times.com ([] matches any character inside the brackets. Here it'll match nytimes or latimes, for example. Use ![] to match any character not inside the brackets) Theming Configure the way your Archivy install looks. These configuration options are children of the THEME_CONF object, like this: THEME_CONF : use_theme_dark : use_custom_css : custom_css_file : Variable Default Description use_theme_dark false Whether or not to load the dark version of the default theme CSS. use_custom_css false Whether or not to load custom css from custom_css_file custom_css_file \"\" Name of file to load in the css/ subdirectory of your user directory (the one with your data or hooks). Create css/ if it doesn't exist. Search See Setup Search for more information. All of these options are children of the SEARCH_CONF object, like this in the config.yml : SEARCH_CONF : enabled : url : # ... To use search, you first have to enable it either through the archivy init setup script or by modifying the enabled variable (see below). Variables marked ES only in their description are only relevant when using the Elasticsearch engine. Variable Default Description enabled 1 engine empty string search engine you'd like to use. One of [\"ripgrep\", [\"elasticsearch\"] url http://localhost:9200 [ES only] Url to the elasticsearch server es_user and es_password None If you're using authentication, for example with a cloud-hosted ES install, you can specify a user and password es_processing_conf Long dict of ES config options [ES only] Configuration of Elasticsearch analyzer , mappings and general settings. INTERNAL_DIR and USER_DIR by default will be set by the appdirs python library: On Linux systems, it follows the XDG specification : ~/.local/share/archivy Editor configuration Archivy uses the markdown-it parser for its editor. This parser can be configured to change the output according to your needs. The default values of EDITOR_CONF are given below. Refer to the markdown-it docs for a full list of possible options. EDITOR_CONF : settings : linkify : true html : false xhtmlOut : false breaks : true typographer : false plugins : ... toolbar_icons : [ \"bold\" , \"italic\" , \"link\" , \"upload-image\" , \"heading\" , \"code\" , \"strikethrough\" , \"quote\" , \"table\" ] # see https://github.com/Ionaru/easy-markdown-editor#toolbar-icons for more options Archivy uses several markdown plugins to enhance its functionality: markdown-it-anchor markdown-it-toc-done-right markdown-it-mark markdown-it-footnote markdown-it-texmath Some of these plugins (see below) can be configured and modified. Refer to their homepages above to see what you can change. They are set up with the following configuration by default: EDITOR_CONF : plugins : markdownitFootnote : {} markdownitMark : {} markdownItAnchor : permalink : True permalinkSymbol : '\u00b6' markdownItTocDoneRight : {}","title":"Config"},{"location":"config/#general","text":"Variable Default Description USER_DIR System-dependent, see below. It is recommended to set this through archivy init Directory in which markdown data will be saved INTERNAL_DIR System-dependent, see below Directory where archivy internals will be stored (config, db...) PORT 5000 Port on which archivy will run HOST 127.0.0.1 Host on which the app will run. DEFAULT_BOOKMARKS_DIR empty string (represents the root directory) any subdirectory of the data/ directory with your notes. SITE_TITLE Archivy String value to be displayed in page title and headings.","title":"General"},{"location":"config/#scraping","text":"An in-progress configuration object to customize how you'd like bookmarking / scraping to work. The options are children of the SCRAPING_CONF object, like so: SCRAPING_CONF : save_images : ... Variable Default Description save_images False If true, whenever you save a bookmark, every linked image will also be downloaded locally. If you want to configure the scraping progress more, you can also create a scraping.py file in the root of your user directory. This file allows you to override the default bookmarking behavior for certain websites / links, which you can match with regex. Once it matches a link, you can either pass your own custom function for parsing, or simply pass a string, which corresponds to the CSS selector for the part of the page you want archivy to scrape. If there are several matches, only the first will be treated. Example that processes youtube videos: def process_videos ( data ): url = data . url # modify whatever you want / set the metadata data . title = \"Video - \" + url data . tags = [ \"video\" ] data . content = \"...\" # declare your patterns in this PATTERNS variable PATTERNS = { \"*youtube.com/*\" : process_videos } With this example, whenever you create a bookmark of a youtube video, instead of going through the default archival, your function will be called on the data. Example that tells archivy only to scrape the main body of Wikipedia pages: PATTERNS = { \"https://*.wikipedia.org/wiki/*\" : \"#bodyContent\" } Example patterns: *wikipedia* ( * matches everything) https://duckduckg?.com* (? matches a single character) https://www.[nl][ya]times.com ([] matches any character inside the brackets. Here it'll match nytimes or latimes, for example. Use ![] to match any character not inside the brackets)","title":"Scraping"},{"location":"config/#theming","text":"Configure the way your Archivy install looks. These configuration options are children of the THEME_CONF object, like this: THEME_CONF : use_theme_dark : use_custom_css : custom_css_file : Variable Default Description use_theme_dark false Whether or not to load the dark version of the default theme CSS. use_custom_css false Whether or not to load custom css from custom_css_file custom_css_file \"\" Name of file to load in the css/ subdirectory of your user directory (the one with your data or hooks). Create css/ if it doesn't exist.","title":"Theming"},{"location":"config/#search","text":"See Setup Search for more information. All of these options are children of the SEARCH_CONF object, like this in the config.yml : SEARCH_CONF : enabled : url : # ... To use search, you first have to enable it either through the archivy init setup script or by modifying the enabled variable (see below). Variables marked ES only in their description are only relevant when using the Elasticsearch engine. Variable Default Description enabled 1 engine empty string search engine you'd like to use. One of [\"ripgrep\", [\"elasticsearch\"] url http://localhost:9200 [ES only] Url to the elasticsearch server es_user and es_password None If you're using authentication, for example with a cloud-hosted ES install, you can specify a user and password es_processing_conf Long dict of ES config options [ES only] Configuration of Elasticsearch analyzer , mappings and general settings. INTERNAL_DIR and USER_DIR by default will be set by the appdirs python library: On Linux systems, it follows the XDG specification : ~/.local/share/archivy","title":"Search"},{"location":"config/#editor-configuration","text":"Archivy uses the markdown-it parser for its editor. This parser can be configured to change the output according to your needs. The default values of EDITOR_CONF are given below. Refer to the markdown-it docs for a full list of possible options. EDITOR_CONF : settings : linkify : true html : false xhtmlOut : false breaks : true typographer : false plugins : ... toolbar_icons : [ \"bold\" , \"italic\" , \"link\" , \"upload-image\" , \"heading\" , \"code\" , \"strikethrough\" , \"quote\" , \"table\" ] # see https://github.com/Ionaru/easy-markdown-editor#toolbar-icons for more options Archivy uses several markdown plugins to enhance its functionality: markdown-it-anchor markdown-it-toc-done-right markdown-it-mark markdown-it-footnote markdown-it-texmath Some of these plugins (see below) can be configured and modified. Refer to their homepages above to see what you can change. They are set up with the following configuration by default: EDITOR_CONF : plugins : markdownitFootnote : {} markdownitMark : {} markdownItAnchor : permalink : True permalinkSymbol : '\u00b6' markdownItTocDoneRight : {}","title":"Editor configuration"},{"location":"difference/","text":"There are many great tools out there to create your knowledge base. So why should you use Archivy? Here are the ingredients that make Archivy stand out (of course many tools have other interesting components / focuses, and you should pick the one that resonates with what you want): Focus on scripting and extensibility : When I began developing archivy, I had plans for developing in the app's core additional features that would allow you to sync up to your digital presence, for exemple a Reddit extension that would download your upvoted posts, etc... I quickly realised that this would be a bad way to go, as many people often want maybe one feature, but not a ton of useless included extensions. That's when I decided to instead build a flexible framework for people to build installable plugins , as this allows a) users only download what they want and b) Archivy can focus on its core and users can build their own extensions for themselves and others. Importance of Digital Preservation : ^ I mentioned above the idea of a plugin for saving all your upvoted reddit posts. This is just an example of how Archivy is intended to be used not only as a knowledge base, but also a resilient stronghold for the data that used to be solely held by third-party services. This idea of automatically syncing and saving content you've found valuable could be expanded to HN upvoted posts, browser bookmarks, etc... 1 deployable OR you can just run it on your computer : The way archivy was engineered makes it possible for you to just run it on your laptop or pc, and still use it without problems. On the other hand, it is also very much a possibility to self-host it and expose it publicly for use from anywhere, which is why archivy has auth. Archivy supports several options for its search , including Elasticsearch. This tool might be considered overkill for one's knowledge base, but as your knowledge base grows also with content from other data sources and automation, it can become a large amount of data. Elasticsearch provides very high quality search on this information at a swift speed and the other alternative ripgrep is much lighter while still reacting well to large amounts of data. https://beepb00p.xyz/hpi.html is an intriguing tool on this topic. \u21a9 See this thread if you have any tools in mind for this. \u21a9","title":"What makes Archivy different"},{"location":"editing/","text":"Format Archivy files are in the markdown format following the commonmark spec . We've also included a few powerful extensions: Bidirectional links : You can easily link to a new note in the web editor by typing [[ : an input box will appear where you can enter the title of the note you want to link to. Otherwise, links between notes are in the format [[linked note title|linked note id]] . If the title you wrote doesn't refer to an existing note, you can click enter and Archivy will create a new note. Embedded tags : You can directly add tags inside your notes with this #tag# syntax (see below). These tags and their groupings can then be viewed by clicking Tags on the navigation bar. Starting to type # in the web editor will display an input where you can search existing tags. I was going to a #python # conference, when I saw a #lion #. In-editor bookmarking : if you'd like to store a local copy of a webpage you're referring to inside an archivy note, simply select the url and click the \"bookmark\" icon. LaTeX : you can render mathematical expressions like this: $$ \\pi = 3.14 $$ Footnotes : What does this describe? [^1] [ ^1 ]: test foot note. Tables : | Column 1 | Column 2 | | -------- | -------- | | ... | ... | Code blocks with syntax highlighting : ```python print ( \"this will be highlighted\" ) x = 1337 ``` There are several ways you can edit content in Archivy. Whenever you open a note or bookmark, at the bottom of the page you'll find a few buttons that allow you to edit it. Editing through the web interface You can edit through the web app, by clicking \"Toggle web editor\" at the bottom. This is the recommended way because the Archivy web editor provides useful functionality. You can save your work, link to other notes with the \"Link to a note button\", and archive webpages referenced in your note, all inside the editor! Locally You can do a local edit . This option is only viable if running archivy on your own computer. This will open the concerned file with the default app set to edit markdown. For example like this:","title":"Editing"},{"location":"editing/#format","text":"Archivy files are in the markdown format following the commonmark spec . We've also included a few powerful extensions: Bidirectional links : You can easily link to a new note in the web editor by typing [[ : an input box will appear where you can enter the title of the note you want to link to. Otherwise, links between notes are in the format [[linked note title|linked note id]] . If the title you wrote doesn't refer to an existing note, you can click enter and Archivy will create a new note. Embedded tags : You can directly add tags inside your notes with this #tag# syntax (see below). These tags and their groupings can then be viewed by clicking Tags on the navigation bar. Starting to type # in the web editor will display an input where you can search existing tags. I was going to a #python # conference, when I saw a #lion #. In-editor bookmarking : if you'd like to store a local copy of a webpage you're referring to inside an archivy note, simply select the url and click the \"bookmark\" icon. LaTeX : you can render mathematical expressions like this: $$ \\pi = 3.14 $$ Footnotes : What does this describe? [^1] [ ^1 ]: test foot note. Tables : | Column 1 | Column 2 | | -------- | -------- | | ... | ... | Code blocks with syntax highlighting : ```python print ( \"this will be highlighted\" ) x = 1337 ``` There are several ways you can edit content in Archivy. Whenever you open a note or bookmark, at the bottom of the page you'll find a few buttons that allow you to edit it.","title":"Format"},{"location":"editing/#editing-through-the-web-interface","text":"You can edit through the web app, by clicking \"Toggle web editor\" at the bottom. This is the recommended way because the Archivy web editor provides useful functionality. You can save your work, link to other notes with the \"Link to a note button\", and archive webpages referenced in your note, all inside the editor!","title":"Editing through the web interface"},{"location":"editing/#locally","text":"You can do a local edit . This option is only viable if running archivy on your own computer. This will open the concerned file with the default app set to edit markdown. For example like this:","title":"Locally"},{"location":"install/","text":"With pip You can easily install archivy with pip . (pip is the default package installer for Python, you can use pip to install many packages and apps, see this link for more information if needed) Make sure your system has Python and pip installed. The Python programming language can also be downloaded from here . Install the python package with pip install archivy It's highly recommended to install ripgrep , which Archivy uses for some of it's organization features (note links & tags inside notes). If you'd like to use search, follow these docs first and then do this part. Run archivy init to create a new user and use the setup wizard. There you go! You should be able to start the app by running archivy run in your terminal and then just login. With docker You can also use archivy with Docker. See the docker documentation for instructions on this. We might implement an AppImage install. Comment here if you'd like to see that happen. With Nix $ nix-env -i archivy","title":"Installing Archivy"},{"location":"install/#with-pip","text":"You can easily install archivy with pip . (pip is the default package installer for Python, you can use pip to install many packages and apps, see this link for more information if needed) Make sure your system has Python and pip installed. The Python programming language can also be downloaded from here . Install the python package with pip install archivy It's highly recommended to install ripgrep , which Archivy uses for some of it's organization features (note links & tags inside notes). If you'd like to use search, follow these docs first and then do this part. Run archivy init to create a new user and use the setup wizard. There you go! You should be able to start the app by running archivy run in your terminal and then just login.","title":"With pip"},{"location":"install/#with-docker","text":"You can also use archivy with Docker. See the docker documentation for instructions on this. We might implement an AppImage install. Comment here if you'd like to see that happen.","title":"With docker"},{"location":"install/#with-nix","text":"$ nix-env -i archivy","title":"With Nix"},{"location":"plugins/","text":"Plugins Plugins are a newly introduced method to add extensions to the archivy cli and web interface. It relies on the extremely useful click-plugins package that is loaded through pip and the click-web which has been modified and whose can be found in archivy/click_web/ , some of the tests, templates and static files. To help you understand the way the plugin system works, we're going to build our own example plugin. We'll even deploy it to Pypi so that other people can install it. Note: The source code for this plugin is available here . We also recommend you read the overview of the reference beforehand so you can better use the wrapper methods archivy exposes. Prerequisites: A python and pip installation with archivy. Step 1: Defining what our archivy extension does Let's build a simple plugin that will automatically add some metadata (author name, location...) at the end of each note. Step 2: Setting up the project Make a new directory named archivy_extra_metadata that will be our plugin directory and create a new setup.py file that will define the characteristics of our package. Note: Using frontmatter is better suited for this functionality, but here we'll just want something simple that adds text directly at the end of the content, like: Made by John Doe in London. This is what our setup.py looks like: from setuptools import setup , find_packages setup ( name = 'archivy_extra_metadata' , version = '0.1.0' , author = \"Uzay-G\" , description = ( \"Archivy extension to add some metadata at the end of your notes / bookmarks.\" ), classifiers = [ \"Programming Language :: Python :: 3\" , ], packages = find_packages (), entry_points = ''' [archivy.plugins] extra-metadata=archivy_extra_metadata:extra_metadata ''' ) Let's walk through what this is doing: We specify some metadata you can adapt to your own package like name , author and description . We then load our package and source code by using the find_packages function. The entry_points is the most important: the [archivy.plugins] part tells archivy that this package's commands will directly extend the archivy CLI so we can call archivy extra-metadata in the command line. We will actually be creating a group of commands so users will call subcommands like this: archivy extra-metadata <subcommand> . You can do things either way. Step 3: Writing the core code of our plugin Create an archivy_extra_metadata directory inside the current directory where setup.py is stored. Create an __init__.py file in that directory where we'll store our main project code. For larger projects, it's better to separate concerns but that'll be good for now. This will be the skeleton structure of our __init__.py : import click # the package that manages the cli @click . group () def extra_metadata (): pass @extra_metadata . command () def command1 (): ... @extra_metadata . command () def command2 (): ... With this example structure, you'd be able to run commands like archivy extra_metadata command1 and archivy extra_metadata command2 . Read the click docs to learn about how to build more intricate commands / options. We also provide some custom option types like email and password Let's get into actually writing a command that interacts with the archivy codebase. We'll make a command called setup to allow users to setup their metadata by specifying their name and location: import click from archivy.helpers import get_db from archivy import app @click . group () def extra_metadata (): \"\"\"`archivy_extra_metadata` plugin to add metadata to your notes.\"\"\" pass @extra_metadata . command () @click . option ( \"--author\" , required = True ) @click . option ( \"--location\" , required = True ) def setup ( author , location ): \"\"\"Save metadata values.\"\"\" with app . app_context (): # save data in db get_db () . insert ({ \"type\" : \"metadata\" , \"author\" : author , \"location\" : location }) click . echo ( \"Metadata saved!\" ) The code above does a few things: It imports the archivy app that is basically the interface for the webserver and many essential Flask features (flask is the web framework archivy uses). It imports the get_db function that allows us to access and modify the database. We define our extra_metadata group of commands that will be the parent of our subcommands. We create a new command from that group with two parameters: author and location . An important part of the code is the with app.app_context() part, we need to run our code inside the archivy app_context to be able to call some of the archivy methods. If you call archivy methods in your plugins, it might fail if you don't include this part. Then we just have our command code that takes the arguments and saves them into the database . Now you just need to do pip install . in the main directory and you'll have access to the commands. Check it out by running archivy extra_metadata --help and then you can access the commands: $ archivy extra-metadata --help Usage: archivy extra-metadata [ OPTIONS ] COMMAND [ ARGS ] ... ` archivy_extra_metadata ` plugin to add metadata to your notes. Options: --help Show this message and exit. Commands: setup Save metadata values. $ archivy extra-metadata setup --help Usage: archivy extra-metadata setup [ OPTIONS ] Save metadata values. Options: --name TEXT [ required ] --location TEXT [ required ] --help Show this message and exit. $ archivy extra-metadata setup --author Uzay --location Europe Metadata saved! That's nice and all, but now we want to actually use this metadata! This wouldn't work with a cli command, because we want it to add metadata whenever a new note is created, not just when a command is executed. To do this, we'll use hooks . These allow the end user to configure what happens whenever an event is fired off, like the creation of a DataObj . Archivy saves a link to a user-specified directory where it stores all your data for Archivy. This directory is usually set during the execution of the archivy init command. It has this structure: data/ # content Creating a hooks.py file in the root of this directory is how archivy calls these events. For example, this could be a potential hook file: # import base hooks that our `Hooks` class inherits from archivy.config import BaseHooks class Hooks(BaseHooks): def on_dataobj_create(self, dataobj): print(\"New dataobj created!\") What we'll be doing is we'll use the before_dataobj_create event to add our metadata before the text is saved. Let's define a function at the end of the __init__.py file of our package that takes a DataObj as an argument and modifies it: from tinydb import Query # this is the db handler that we use and we need to make a Query def add_metadata(dataobj): with app.app_context(): metadata = get_db().search(Query().type == \"metadata\") dataobj.content += f\"Made by {metadata['author']} in {metadata['location']}\" Combined with our previous code: import click from tinydb import Query from archivy.helpers import get_db from archivy import app @click . group () def extra_metadata (): \"\"\"`archivy_extra_metadata` plugin to add metadata to your notes.\"\"\" pass @extra_metadata . command () @click . option ( \"--author\" , required = True ) @click . option ( \"--location\" , required = True ) def setup ( author , location ): \"\"\"Save metadata values.\"\"\" with app . app_context (): # save data in db get_db () . insert ({ \"type\" : \"metadata\" , \"author\" : author , \"location\" : location }) click . echo ( \"Metadata saved!\" ) def add_metadata ( dataobj ): with app . app_context (): metadata = get_db () . search ( Query () . type == \"metadata\" )[ 0 ] dataobj . content += f \" \\n Made by { metadata [ 'author' ] } in { metadata [ 'location' ] } .\" Now to enable our plugin, all people have to do is modify their hooks.py file like this for example: from archivy_extra_metadata import add_metadata from archivy.config import BaseHooks class Hooks : def before_dataobj_create ( self , dataobj ): add_metadata ( dataobj ) Users of your plugin will add the add_metadata(dataobj) to the before_dataobj_create call, and the metadata will be added automatically when the event is called. We now have a working setup: This is how our plugin will be used: Users install the package. They run archivy extra-metadata setup --author xxx --location xxx to set metadata. They modify their hooks.py to call add_metadata on the before_dataobj_create event. Voil\u00e0! However, we need a way for them to install this package. That brings us to Step 4. Step 4: Publishing our package to Pypi Pypi is the Python package repository. Publishing our package to it will allow other users to easily install our code onto their own archivy instance. This is a short overview of how you can upload your package. Check out this website for more info. This section is inspired by this useful tutorial . Make sure the required utilities are installed: python3 - m pip install -- user -- upgrade setuptools wheel Now run this command in the main directory to build the source: python3 setup . py sdist bdist_wheel Create an account on Pypi . Then go here and create a new API token; set its scope to all projects. Once you've saved your token, install twine , the program that will take care of the upload: python3 - m pip install -- user -- upgrade twine And you can finally upload your code! The username you should enter is __token__ and then the password is your API token. python3 - m twine upload dist /* You're done! Now that you've finished your package, you can share it if you'd like, publish it on a public git repository so other people can collaborate, and you can add it to the awesome_archivy github repo which is an official list of plugins built around Archivy. We'd also love to hear about it on our discord server !","title":"Index"},{"location":"plugins/#plugins","text":"Plugins are a newly introduced method to add extensions to the archivy cli and web interface. It relies on the extremely useful click-plugins package that is loaded through pip and the click-web which has been modified and whose can be found in archivy/click_web/ , some of the tests, templates and static files. To help you understand the way the plugin system works, we're going to build our own example plugin. We'll even deploy it to Pypi so that other people can install it. Note: The source code for this plugin is available here . We also recommend you read the overview of the reference beforehand so you can better use the wrapper methods archivy exposes. Prerequisites: A python and pip installation with archivy.","title":"Plugins"},{"location":"plugins/#step-1-defining-what-our-archivy-extension-does","text":"Let's build a simple plugin that will automatically add some metadata (author name, location...) at the end of each note.","title":"Step 1: Defining what our archivy extension does"},{"location":"plugins/#step-2-setting-up-the-project","text":"Make a new directory named archivy_extra_metadata that will be our plugin directory and create a new setup.py file that will define the characteristics of our package. Note: Using frontmatter is better suited for this functionality, but here we'll just want something simple that adds text directly at the end of the content, like: Made by John Doe in London. This is what our setup.py looks like: from setuptools import setup , find_packages setup ( name = 'archivy_extra_metadata' , version = '0.1.0' , author = \"Uzay-G\" , description = ( \"Archivy extension to add some metadata at the end of your notes / bookmarks.\" ), classifiers = [ \"Programming Language :: Python :: 3\" , ], packages = find_packages (), entry_points = ''' [archivy.plugins] extra-metadata=archivy_extra_metadata:extra_metadata ''' ) Let's walk through what this is doing: We specify some metadata you can adapt to your own package like name , author and description . We then load our package and source code by using the find_packages function. The entry_points is the most important: the [archivy.plugins] part tells archivy that this package's commands will directly extend the archivy CLI so we can call archivy extra-metadata in the command line. We will actually be creating a group of commands so users will call subcommands like this: archivy extra-metadata <subcommand> . You can do things either way.","title":"Step 2: Setting up the project"},{"location":"plugins/#step-3-writing-the-core-code-of-our-plugin","text":"Create an archivy_extra_metadata directory inside the current directory where setup.py is stored. Create an __init__.py file in that directory where we'll store our main project code. For larger projects, it's better to separate concerns but that'll be good for now. This will be the skeleton structure of our __init__.py : import click # the package that manages the cli @click . group () def extra_metadata (): pass @extra_metadata . command () def command1 (): ... @extra_metadata . command () def command2 (): ... With this example structure, you'd be able to run commands like archivy extra_metadata command1 and archivy extra_metadata command2 . Read the click docs to learn about how to build more intricate commands / options. We also provide some custom option types like email and password Let's get into actually writing a command that interacts with the archivy codebase. We'll make a command called setup to allow users to setup their metadata by specifying their name and location: import click from archivy.helpers import get_db from archivy import app @click . group () def extra_metadata (): \"\"\"`archivy_extra_metadata` plugin to add metadata to your notes.\"\"\" pass @extra_metadata . command () @click . option ( \"--author\" , required = True ) @click . option ( \"--location\" , required = True ) def setup ( author , location ): \"\"\"Save metadata values.\"\"\" with app . app_context (): # save data in db get_db () . insert ({ \"type\" : \"metadata\" , \"author\" : author , \"location\" : location }) click . echo ( \"Metadata saved!\" ) The code above does a few things: It imports the archivy app that is basically the interface for the webserver and many essential Flask features (flask is the web framework archivy uses). It imports the get_db function that allows us to access and modify the database. We define our extra_metadata group of commands that will be the parent of our subcommands. We create a new command from that group with two parameters: author and location . An important part of the code is the with app.app_context() part, we need to run our code inside the archivy app_context to be able to call some of the archivy methods. If you call archivy methods in your plugins, it might fail if you don't include this part. Then we just have our command code that takes the arguments and saves them into the database . Now you just need to do pip install . in the main directory and you'll have access to the commands. Check it out by running archivy extra_metadata --help and then you can access the commands: $ archivy extra-metadata --help Usage: archivy extra-metadata [ OPTIONS ] COMMAND [ ARGS ] ... ` archivy_extra_metadata ` plugin to add metadata to your notes. Options: --help Show this message and exit. Commands: setup Save metadata values. $ archivy extra-metadata setup --help Usage: archivy extra-metadata setup [ OPTIONS ] Save metadata values. Options: --name TEXT [ required ] --location TEXT [ required ] --help Show this message and exit. $ archivy extra-metadata setup --author Uzay --location Europe Metadata saved! That's nice and all, but now we want to actually use this metadata! This wouldn't work with a cli command, because we want it to add metadata whenever a new note is created, not just when a command is executed. To do this, we'll use hooks . These allow the end user to configure what happens whenever an event is fired off, like the creation of a DataObj . Archivy saves a link to a user-specified directory where it stores all your data for Archivy. This directory is usually set during the execution of the archivy init command. It has this structure: data/ # content Creating a hooks.py file in the root of this directory is how archivy calls these events. For example, this could be a potential hook file: # import base hooks that our `Hooks` class inherits from archivy.config import BaseHooks class Hooks(BaseHooks): def on_dataobj_create(self, dataobj): print(\"New dataobj created!\") What we'll be doing is we'll use the before_dataobj_create event to add our metadata before the text is saved. Let's define a function at the end of the __init__.py file of our package that takes a DataObj as an argument and modifies it: from tinydb import Query # this is the db handler that we use and we need to make a Query def add_metadata(dataobj): with app.app_context(): metadata = get_db().search(Query().type == \"metadata\") dataobj.content += f\"Made by {metadata['author']} in {metadata['location']}\" Combined with our previous code: import click from tinydb import Query from archivy.helpers import get_db from archivy import app @click . group () def extra_metadata (): \"\"\"`archivy_extra_metadata` plugin to add metadata to your notes.\"\"\" pass @extra_metadata . command () @click . option ( \"--author\" , required = True ) @click . option ( \"--location\" , required = True ) def setup ( author , location ): \"\"\"Save metadata values.\"\"\" with app . app_context (): # save data in db get_db () . insert ({ \"type\" : \"metadata\" , \"author\" : author , \"location\" : location }) click . echo ( \"Metadata saved!\" ) def add_metadata ( dataobj ): with app . app_context (): metadata = get_db () . search ( Query () . type == \"metadata\" )[ 0 ] dataobj . content += f \" \\n Made by { metadata [ 'author' ] } in { metadata [ 'location' ] } .\" Now to enable our plugin, all people have to do is modify their hooks.py file like this for example: from archivy_extra_metadata import add_metadata from archivy.config import BaseHooks class Hooks : def before_dataobj_create ( self , dataobj ): add_metadata ( dataobj ) Users of your plugin will add the add_metadata(dataobj) to the before_dataobj_create call, and the metadata will be added automatically when the event is called. We now have a working setup: This is how our plugin will be used: Users install the package. They run archivy extra-metadata setup --author xxx --location xxx to set metadata. They modify their hooks.py to call add_metadata on the before_dataobj_create event. Voil\u00e0! However, we need a way for them to install this package. That brings us to Step 4.","title":"Step 3: Writing the core code of our plugin"},{"location":"plugins/#step-4-publishing-our-package-to-pypi","text":"Pypi is the Python package repository. Publishing our package to it will allow other users to easily install our code onto their own archivy instance. This is a short overview of how you can upload your package. Check out this website for more info. This section is inspired by this useful tutorial . Make sure the required utilities are installed: python3 - m pip install -- user -- upgrade setuptools wheel Now run this command in the main directory to build the source: python3 setup . py sdist bdist_wheel Create an account on Pypi . Then go here and create a new API token; set its scope to all projects. Once you've saved your token, install twine , the program that will take care of the upload: python3 - m pip install -- user -- upgrade twine And you can finally upload your code! The username you should enter is __token__ and then the password is your API token. python3 - m twine upload dist /*","title":"Step 4: Publishing our package to Pypi"},{"location":"plugins/#youre-done","text":"Now that you've finished your package, you can share it if you'd like, publish it on a public git repository so other people can collaborate, and you can add it to the awesome_archivy github repo which is an official list of plugins built around Archivy. We'd also love to hear about it on our discord server !","title":"You're done!"},{"location":"setup-search/","text":"Archivy supports two search engines: Elasticsearch - an incredibly powerful solution that is however harder to install. ripgrep , much more lightweight but also less powerful. These allow archivy to index and provide full-text search on their knowledge bases. You can select these in the archivy init script, and the the config docs also has more information on this. Elasticsearch Elasticsearch, is a complex and extendable search engine that returns high-quality results. Instructions to install and run the service which needs to be running when you use Elasticsearch can be found here . Append these two lines to your elasticsearch.yml config file : http.cors.enabled : true http.cors.allow-origin : \"http://localhost:5000\" Then, when you run archivy init simply specify you have enabled ES to integrate it with archivy. You will now have full-text search on your knowledge base! If you're adding ES to an existing knowledge base, use archivy index to sync any changes. Elasticsearch can be a hefty dependency, so if you have any ideas for something more light-weight that could be used as an alternative, please share on this thread . Ripgrep Ripgrep , on the other hand, is much more lightweight and small, but is also a bit limited in its functionality. Follow these instructions to install ripgrep. Then simply specify you want to use it during the archivy init script (or edit the config to add it).","title":"Search"},{"location":"setup-search/#elasticsearch","text":"Elasticsearch, is a complex and extendable search engine that returns high-quality results. Instructions to install and run the service which needs to be running when you use Elasticsearch can be found here . Append these two lines to your elasticsearch.yml config file : http.cors.enabled : true http.cors.allow-origin : \"http://localhost:5000\" Then, when you run archivy init simply specify you have enabled ES to integrate it with archivy. You will now have full-text search on your knowledge base! If you're adding ES to an existing knowledge base, use archivy index to sync any changes. Elasticsearch can be a hefty dependency, so if you have any ideas for something more light-weight that could be used as an alternative, please share on this thread .","title":"Elasticsearch"},{"location":"setup-search/#ripgrep","text":"Ripgrep , on the other hand, is much more lightweight and small, but is also a bit limited in its functionality. Follow these instructions to install ripgrep. Then simply specify you want to use it during the archivy init script (or edit the config to add it).","title":"Ripgrep"},{"location":"usage/","text":"Archivy comes with a simple command line interface that you use on the backend to run archivy: Usage: archivy [OPTIONS] COMMAND [ARGS]... Options: --version Show the flask version --help Show this message and exit. Commands: config Open archivy config. create-admin Creates a new admin user format Format normal markdown files for archivy. index Sync content to Elasticsearch init Initialise your archivy application run Runs archivy web application shell Run a shell in the app context. unformat Convert archivy-formatted files back to normal markdown. Make sure you've configured Archivy by running archivy init , as outlined in install . If you'd like to add users, you can simply create new admin users with the create-admin command. Only give credentials to trusted people. If you have normal md files you'd like to migrate to archivy, move your files into your archivy data directory and then run archivy format <filenames> to make them conform to archivy's formatting . Run archivy unformat to convert the other way around. You can sync changes to files to the Elasticsearch index by running archivy index or by simply using the web editor which updates ES when you push a change. The config command allows you to play around with configuration and use shell if you'd like to play around with the archivy python API. You can then use archivy to create notes, bookmarks and to organize and store information. The web api is also useful to extend archivy, or plugins . These have been recently introduced, but you can check the existing plugins that you can install onto your instance here .","title":"Usage"},{"location":"whats_next/","text":"Now that you have a working installation and you know how to use, where can you go from here? If you'd like to write a plugin for archivy with standalone functionality you'd like to see, check out the plugin tutorial . You can also use our Web API ! If you notice any bugs or problems, or if you have any features you'd like to see, please open up an issue on GitHub . You can also directly talk to us on the archivy discord server !","title":"What's Next"},{"location":"example-plugin/","text":"","title":"Index"},{"location":"reference/architecture/","text":"This document is a general overview of how the different pieces of archivy interact and what technologies it uses. Reading this will be useful for people looking to access the inner archivy API to write plugins. Read this post to understand what the function of an architecture.md file is. Archivy is: A Flask web application. A click backend command line interface. You use the cli to run the app, and you'll probably be using the web application for direct usage of archivy. Data Storage DataObjs is the term used to denote a note or bookmark that is stored in your knowledge base (abbreviation for Data Object). These are stored in a directory on your filesystem of which you can configure the location . They are organized in markdown files with yaml front matter like this: --- date : 08-31-20 desc : '' id : 100 path : '' tags : [] title : ... type : note --- ... Archivy uses the python-frontmatter package to handle the parsing of these files. They can be organized into user-specified sub-directories. Check out the reference to see the methods archivy uses for this. Another storage method Archivy uses is TinyDB . This is a small, simple document-oriented database archivy gives you access to for persistent data you might want to store in archivy plugins. Use helpers.get_db to call the database. Search Archivy supports two search engines: Elasticsearch - an incredibly powerful solution that is however harder to install. ripgrep , much more lightweight but also less powerful. These allow archivy to to index and provide full-text search on their knowledge bases. Elasticsearch requires configuration to have higher quality search results. You can check out the top-notch config archivy already uses by default here . Check out the helper methods archivy exposes for ES. Auth Archivy uses flask-login for auth. All endpoints require to be authenticated. You can create an admin user with the create-admin command. In our roadmap we plan to extend our permission framework to have a multi-user system, and define configuration for the permissions of non-logged in users. In general we want to make things more flexible on the auth side. How bookmarks work One of the core features of archivy is being able to save webpages locally. The way this works is the conversion of the html of the page you specify to a simple, markdown file. We might want to extend this to also be able to save PDF, EPUB and other formats. You can find the reference for this part here . Further down the road, it'd be nice to add background processing and not only download the webpage, but also save the essential assets it loads for a more complete process. This feature of preserving web content aligns with the mission against link rot 1 . Plugins Plugins in archivy function as standalone python packages. The phenomenal click-plugins package allows us to do this by basically adding commands to the cli. So you create a python package where you specify commands to extend your pre-existing archivy cli. Then these added commands will be able to be used through the cli. But what makes plugins interesting is that you can actually also use the plugins through the web interface, without having access to the system running archivy. We use an adaptation of the click-web to convert your cli commands to interactive web forms. See this manifesto to learn more about this phenomenon. \u21a9","title":"Architecture"},{"location":"reference/architecture/#data-storage","text":"DataObjs is the term used to denote a note or bookmark that is stored in your knowledge base (abbreviation for Data Object). These are stored in a directory on your filesystem of which you can configure the location . They are organized in markdown files with yaml front matter like this: --- date : 08-31-20 desc : '' id : 100 path : '' tags : [] title : ... type : note --- ... Archivy uses the python-frontmatter package to handle the parsing of these files. They can be organized into user-specified sub-directories. Check out the reference to see the methods archivy uses for this. Another storage method Archivy uses is TinyDB . This is a small, simple document-oriented database archivy gives you access to for persistent data you might want to store in archivy plugins. Use helpers.get_db to call the database.","title":"Data Storage"},{"location":"reference/architecture/#search","text":"Archivy supports two search engines: Elasticsearch - an incredibly powerful solution that is however harder to install. ripgrep , much more lightweight but also less powerful. These allow archivy to to index and provide full-text search on their knowledge bases. Elasticsearch requires configuration to have higher quality search results. You can check out the top-notch config archivy already uses by default here . Check out the helper methods archivy exposes for ES.","title":"Search"},{"location":"reference/architecture/#auth","text":"Archivy uses flask-login for auth. All endpoints require to be authenticated. You can create an admin user with the create-admin command. In our roadmap we plan to extend our permission framework to have a multi-user system, and define configuration for the permissions of non-logged in users. In general we want to make things more flexible on the auth side.","title":"Auth"},{"location":"reference/architecture/#how-bookmarks-work","text":"One of the core features of archivy is being able to save webpages locally. The way this works is the conversion of the html of the page you specify to a simple, markdown file. We might want to extend this to also be able to save PDF, EPUB and other formats. You can find the reference for this part here . Further down the road, it'd be nice to add background processing and not only download the webpage, but also save the essential assets it loads for a more complete process. This feature of preserving web content aligns with the mission against link rot 1 .","title":"How bookmarks work"},{"location":"reference/architecture/#plugins","text":"Plugins in archivy function as standalone python packages. The phenomenal click-plugins package allows us to do this by basically adding commands to the cli. So you create a python package where you specify commands to extend your pre-existing archivy cli. Then these added commands will be able to be used through the cli. But what makes plugins interesting is that you can actually also use the plugins through the web interface, without having access to the system running archivy. We use an adaptation of the click-web to convert your cli commands to interactive web forms. See this manifesto to learn more about this phenomenon. \u21a9","title":"Plugins"},{"location":"reference/filesystem_layer/","text":"This module holds the methods used to access, modify, and delete components of the filesystem where Dataobjs are stored in Archivy. Directory Tree like file-structure used to build file navigation in Archiv Source code in archivy/data.py class Directory : \"\"\"Tree like file-structure used to build file navigation in Archiv\"\"\" def __init__ ( self , name ): self . name = name self . child_files = [] self . child_dirs = {} build_dir_tree ( path , query_dir , load_content = True ) Builds a structured tree of directories and data objects. path : name of the directory relative to the root directory. query_dir : absolute path of the directory we're building the tree of. load_content : internal option to not save post contents in memory if they're not going to be accessed. Source code in archivy/data.py def build_dir_tree ( path , query_dir , load_content = True ): \"\"\" Builds a structured tree of directories and data objects. - **path**: name of the directory relative to the root directory. - **query_dir**: absolute path of the directory we're building the tree of. - **load_content**: internal option to not save post contents in memory if they're not going to be accessed. \"\"\" datacont = Directory ( path or \"root\" ) for filepath in query_dir . rglob ( \"*\" ): current_path = filepath . relative_to ( query_dir ) current_dir = datacont # iterate through parent directories for segment in current_path . parts [: - 1 ]: # directory has not been saved in tree yet if segment not in current_dir . child_dirs : current_dir . child_dirs [ segment ] = Directory ( segment ) current_dir = current_dir . child_dirs [ segment ] # handle last part of current_path last_seg = current_path . parts [ - 1 ] if filepath . is_dir (): if last_seg not in current_dir . child_dirs : current_dir . child_dirs [ last_seg ] = Directory ( last_seg ) current_dir = current_dir . child_dirs [ last_seg ] elif last_seg . endswith ( \".md\" ): data = frontmatter . load ( filepath ) if not load_content : data . content = \"\" current_dir . child_files . append ( data ) return datacont create ( contents , title , path = '' ) Helper method to save a new dataobj onto the filesystem. contents : md file contents title - title used for filename path Source code in archivy/data.py def create ( contents , title , path = \"\" ): \"\"\" Helper method to save a new dataobj onto the filesystem. Parameters: - **contents**: md file contents - **title** - title used for filename - **path** \"\"\" filename = secure_filename ( title ) data_dir = get_data_dir () max_filename_length = 255 if len ( filename + \".md\" ) > max_filename_length : filename = filename [ 0 : max_filename_length - 3 ] if not is_relative_to ( data_dir / path , data_dir ): path = \"\" path_to_md_file = data_dir / path / f \" { filename } .md\" with open ( path_to_md_file , \"w\" , encoding = \"utf-8\" ) as file : file . write ( contents ) return path_to_md_file create_dir ( name ) Create dir of given name Source code in archivy/data.py def create_dir ( name ): \"\"\"Create dir of given name\"\"\" root_dir = get_data_dir () new_path = root_dir / name . strip ( \"/\" ) if is_relative_to ( new_path , root_dir ): new_path . mkdir ( parents = True , exist_ok = True ) return str ( new_path . relative_to ( root_dir )) return False delete_dir ( name ) Deletes dir of given name Source code in archivy/data.py def delete_dir ( name ): \"\"\"Deletes dir of given name\"\"\" root_dir = get_data_dir () target_dir = root_dir / name if not is_relative_to ( target_dir , root_dir ) or target_dir == root_dir : return False try : shutil . rmtree ( target_dir ) return True except FileNotFoundError : return False delete_item ( dataobj_id ) Delete dataobj of given id Source code in archivy/data.py def delete_item ( dataobj_id ): \"\"\"Delete dataobj of given id\"\"\" file = get_by_id ( dataobj_id ) remove_from_index ( dataobj_id ) if file : Path ( file ) . unlink () format_file ( path ) Converts normal md of file at path to formatted archivy markdown file, with yaml front matter and a filename of format \"{id}-{old_filename}.md\" Source code in archivy/data.py def format_file ( path : str ): \"\"\" Converts normal md of file at `path` to formatted archivy markdown file, with yaml front matter and a filename of format \"{id}-{old_filename}.md\" \"\"\" from archivy.models import DataObj data_dir = get_data_dir () path = Path ( path ) if not path . exists (): return if path . is_dir (): for filename in path . iterdir (): format_file ( filename ) else : new_file = path . open ( \"r\" , encoding = \"utf-8\" ) file_contents = new_file . read () new_file . close () try : # get relative path of object in `data` dir datapath = path . parent . resolve () . relative_to ( data_dir ) except ValueError : datapath = Path () note_dataobj = { \"title\" : path . name . replace ( \".md\" , \"\" ), \"content\" : file_contents , \"type\" : \"note\" , \"path\" : str ( datapath ), } dataobj = DataObj ( ** note_dataobj ) dataobj . insert () path . unlink () current_app . logger . info ( f \"Formatted and moved { str ( datapath / path . name ) } to { dataobj . fullpath } \" ) get_by_id ( dataobj_id ) Returns filename of dataobj of given id Source code in archivy/data.py def get_by_id ( dataobj_id ): \"\"\"Returns filename of dataobj of given id\"\"\" results = list ( get_data_dir () . rglob ( f \" { dataobj_id } -*.md\" )) return results [ 0 ] if results else None get_data_dir () Returns the directory where dataobjs are stored Source code in archivy/data.py def get_data_dir (): \"\"\"Returns the directory where dataobjs are stored\"\"\" return Path ( current_app . config [ \"USER_DIR\" ]) / \"data\" get_dirs () Gets all dir names where dataobjs are stored Source code in archivy/data.py def get_dirs (): \"\"\"Gets all dir names where dataobjs are stored\"\"\" # join glob matchers dirnames = [ str ( dir_path . relative_to ( get_data_dir ())) for dir_path in get_data_dir () . rglob ( \"*\" ) if dir_path . is_dir () ] return dirnames get_item ( dataobj_id ) Returns a Post object with the given dataobjs' attributes Source code in archivy/data.py def get_item ( dataobj_id ): \"\"\"Returns a Post object with the given dataobjs' attributes\"\"\" file = get_by_id ( dataobj_id ) if file : data = frontmatter . load ( file ) data [ \"fullpath\" ] = str ( file ) data [ \"dir\" ] = str ( file . parent . relative_to ( get_data_dir ())) # replace . for root items to '' if data [ \"dir\" ] == \".\" : data [ \"dir\" ] = \"\" return data return None get_items ( collections = [], path = '' , structured = True , json_format = False , load_content = True ) Gets all dataobjs. collections - filter dataobj by type, eg. bookmark / note path - filter by path **structured: if set to True, will return a Directory object, otherwise data will just be returned as a list of dataobjs json_format : boolean value used internally to pre-process dataobjs to send back a json response. load_content : internal value to disregard post content and not save them in memory if they won't be accessed. Source code in archivy/data.py def get_items ( collections = [], path = \"\" , structured = True , json_format = False , load_content = True ): \"\"\" Gets all dataobjs. Parameters: - **collections** - filter dataobj by type, eg. bookmark / note - **path** - filter by path - **structured: if set to True, will return a Directory object, otherwise data will just be returned as a list of dataobjs - **json_format**: boolean value used internally to pre-process dataobjs to send back a json response. - **load_content**: internal value to disregard post content and not save them in memory if they won't be accessed. \"\"\" data_dir = get_data_dir () query_dir = data_dir / path if not is_relative_to ( query_dir , data_dir ) or not query_dir . exists (): raise FileNotFoundError if structured : return build_dir_tree ( path , query_dir ) else : datacont = [] for filepath in query_dir . rglob ( \"*.md\" ): data = frontmatter . load ( filepath ) if not load_content : data . content = \"\" data [ \"fullpath\" ] = str ( filepath . parent . relative_to ( query_dir )) if len ( collections ) == 0 or any ( [ collection == data [ \"type\" ] for collection in collections ] ): if json_format : dict_dataobj = data . __dict__ # remove unnecessary yaml handler dict_dataobj . pop ( \"handler\" ) datacont . append ( dict_dataobj ) else : datacont . append ( data ) return datacont is_relative_to ( sub_path , parent ) Implement pathlib is_relative_to only available in python 3.9 Source code in archivy/data.py def is_relative_to ( sub_path , parent ): \"\"\"Implement pathlib `is_relative_to` only available in python 3.9\"\"\" try : parent_path = Path ( parent ) . resolve () sub_path . resolve () . relative_to ( parent_path ) return True except ValueError : return False move_item ( dataobj_id , new_path ) Move dataobj of given id to new_path Source code in archivy/data.py def move_item ( dataobj_id , new_path ): \"\"\"Move dataobj of given id to new_path\"\"\" file = get_by_id ( dataobj_id ) data_dir = get_data_dir () out_dir = ( data_dir / new_path ) . resolve () if not file : raise FileNotFoundError if ( out_dir / file . parts [ - 1 ]) . exists (): raise FileExistsError elif is_relative_to ( out_dir , data_dir ) and out_dir . exists (): # check file isn't return shutil . move ( str ( file ), f \" { get_data_dir () } / { new_path } /\" ) return False open_file ( path ) Cross platform way of opening file on user's computer Source code in archivy/data.py def open_file ( path ): \"\"\"Cross platform way of opening file on user's computer\"\"\" if platform . system () == \"Windows\" : os . startfile ( path ) elif platform . system () == \"Darwin\" : subprocess . Popen ([ \"open\" , path ]) else : subprocess . Popen ([ \"xdg-open\" , path ]) save_image ( image ) Saves image to USER_DATA_DIR Returns: filename where image has been saved. Source code in archivy/data.py def save_image ( image : FileStorage ): \"\"\" Saves image to USER_DATA_DIR Returns: filename where image has been saved. \"\"\" base_path = Path ( current_app . config [ \"USER_DIR\" ]) / \"images\" fileparts = image . filename . rsplit ( \".\" , 1 ) sanitized_filename = secure_filename ( fileparts [ 0 ]) dest_path = base_path / f \" { sanitized_filename } . { fileparts [ 1 ] } \" i = 1 while dest_path . exists (): dest_path = base_path / f \" { sanitized_filename } - { i } . { fileparts [ 1 ] } \" i += 1 image . save ( str ( dest_path )) return dest_path . parts [ - 1 ] unformat_file ( path , out_dir ) Converts normal md of file at path to formatted archivy markdown file, with yaml front matter and a filename of format \"{id}-{old_filename}.md\" Source code in archivy/data.py def unformat_file ( path : str , out_dir : str ): \"\"\" Converts normal md of file at `path` to formatted archivy markdown file, with yaml front matter and a filename of format \"{id}-{old_filename}.md\" \"\"\" data_dir = get_data_dir () path = Path ( path ) out_dir = Path ( out_dir ) if not path . exists () and out_dir . exists () and out_dir . is_dir (): return if path . is_dir (): path . mkdir ( exist_ok = True ) for filename in path . iterdir (): unformat_file ( filename , str ( out_dir )) else : dataobj = frontmatter . load ( str ( path )) try : # get relative path of object in `data` dir datapath = path . parent . resolve () . relative_to ( data_dir ) except ValueError : datapath = Path () # create subdir if doesn't exist ( out_dir / datapath ) . mkdir ( exist_ok = True ) new_path = out_dir / datapath / f \" { dataobj . metadata [ 'title' ] } .md\" with new_path . open ( \"w\" ) as f : f . write ( dataobj . content ) current_app . logger . info ( f \"Unformatted and moved { str ( path ) } to { str ( new_path . resolve ()) } \" ) path . unlink () update_item_frontmatter ( dataobj_id , new_frontmatter ) Given an object id, this method overwrites the front matter of the post with new_frontmatter . date: Str id: Str path: Str tags: List[Str] title: Str type: note/bookmark Source code in archivy/data.py def update_item_frontmatter ( dataobj_id , new_frontmatter ): \"\"\" Given an object id, this method overwrites the front matter of the post with `new_frontmatter`. --- date: Str id: Str path: Str tags: List[Str] title: Str type: note/bookmark --- \"\"\" from archivy.models import DataObj filename = get_by_id ( dataobj_id ) dataobj = frontmatter . load ( filename ) for key in list ( new_frontmatter ): dataobj [ key ] = new_frontmatter [ key ] md = frontmatter . dumps ( dataobj ) with open ( filename , \"w\" , encoding = \"utf-8\" ) as f : f . write ( md ) converted_dataobj = DataObj . from_md ( md ) converted_dataobj . fullpath = str ( filename . relative_to ( current_app . config [ \"USER_DIR\" ]) ) converted_dataobj . index () current_app . config [ \"HOOKS\" ] . on_edit ( converted_dataobj ) update_item_md ( dataobj_id , new_content ) Given an object id, this method overwrites the inner content of the post with new_content . This means that it won't change the frontmatter (eg tags, id, title) but it can change the file content. For example: If we have a dataobj like this: --- id: 1 title: Note --- # This is random Calling update_item(1, \"# This is specific\") will turn it into: --- id: 1 # unchanged title: Note --- # This is specific Source code in archivy/data.py def update_item_md ( dataobj_id , new_content ): \"\"\" Given an object id, this method overwrites the inner content of the post with `new_content`. This means that it won't change the frontmatter (eg tags, id, title) but it can change the file content. For example: If we have a dataobj like this: ```md --- id: 1 title: Note --- # This is random ``` Calling `update_item(1, \"# This is specific\")` will turn it into: ```md --- id: 1 # unchanged title: Note --- # This is specific ``` \"\"\" from archivy.models import DataObj filename = get_by_id ( dataobj_id ) dataobj = frontmatter . load ( filename ) dataobj . content = new_content md = frontmatter . dumps ( dataobj ) with open ( filename , \"w\" , encoding = \"utf-8\" ) as f : f . write ( md ) converted_dataobj = DataObj . from_md ( md ) converted_dataobj . fullpath = str ( filename . relative_to ( current_app . config [ \"USER_DIR\" ]) ) converted_dataobj . index () current_app . config [ \"HOOKS\" ] . on_edit ( converted_dataobj )","title":"Dataobj Filesystem Layer"},{"location":"reference/filesystem_layer/#archivy.data.Directory","text":"Tree like file-structure used to build file navigation in Archiv Source code in archivy/data.py class Directory : \"\"\"Tree like file-structure used to build file navigation in Archiv\"\"\" def __init__ ( self , name ): self . name = name self . child_files = [] self . child_dirs = {}","title":"Directory"},{"location":"reference/filesystem_layer/#archivy.data.build_dir_tree","text":"Builds a structured tree of directories and data objects. path : name of the directory relative to the root directory. query_dir : absolute path of the directory we're building the tree of. load_content : internal option to not save post contents in memory if they're not going to be accessed. Source code in archivy/data.py def build_dir_tree ( path , query_dir , load_content = True ): \"\"\" Builds a structured tree of directories and data objects. - **path**: name of the directory relative to the root directory. - **query_dir**: absolute path of the directory we're building the tree of. - **load_content**: internal option to not save post contents in memory if they're not going to be accessed. \"\"\" datacont = Directory ( path or \"root\" ) for filepath in query_dir . rglob ( \"*\" ): current_path = filepath . relative_to ( query_dir ) current_dir = datacont # iterate through parent directories for segment in current_path . parts [: - 1 ]: # directory has not been saved in tree yet if segment not in current_dir . child_dirs : current_dir . child_dirs [ segment ] = Directory ( segment ) current_dir = current_dir . child_dirs [ segment ] # handle last part of current_path last_seg = current_path . parts [ - 1 ] if filepath . is_dir (): if last_seg not in current_dir . child_dirs : current_dir . child_dirs [ last_seg ] = Directory ( last_seg ) current_dir = current_dir . child_dirs [ last_seg ] elif last_seg . endswith ( \".md\" ): data = frontmatter . load ( filepath ) if not load_content : data . content = \"\" current_dir . child_files . append ( data ) return datacont","title":"build_dir_tree()"},{"location":"reference/filesystem_layer/#archivy.data.create","text":"Helper method to save a new dataobj onto the filesystem. contents : md file contents title - title used for filename path Source code in archivy/data.py def create ( contents , title , path = \"\" ): \"\"\" Helper method to save a new dataobj onto the filesystem. Parameters: - **contents**: md file contents - **title** - title used for filename - **path** \"\"\" filename = secure_filename ( title ) data_dir = get_data_dir () max_filename_length = 255 if len ( filename + \".md\" ) > max_filename_length : filename = filename [ 0 : max_filename_length - 3 ] if not is_relative_to ( data_dir / path , data_dir ): path = \"\" path_to_md_file = data_dir / path / f \" { filename } .md\" with open ( path_to_md_file , \"w\" , encoding = \"utf-8\" ) as file : file . write ( contents ) return path_to_md_file","title":"create()"},{"location":"reference/filesystem_layer/#archivy.data.create_dir","text":"Create dir of given name Source code in archivy/data.py def create_dir ( name ): \"\"\"Create dir of given name\"\"\" root_dir = get_data_dir () new_path = root_dir / name . strip ( \"/\" ) if is_relative_to ( new_path , root_dir ): new_path . mkdir ( parents = True , exist_ok = True ) return str ( new_path . relative_to ( root_dir )) return False","title":"create_dir()"},{"location":"reference/filesystem_layer/#archivy.data.delete_dir","text":"Deletes dir of given name Source code in archivy/data.py def delete_dir ( name ): \"\"\"Deletes dir of given name\"\"\" root_dir = get_data_dir () target_dir = root_dir / name if not is_relative_to ( target_dir , root_dir ) or target_dir == root_dir : return False try : shutil . rmtree ( target_dir ) return True except FileNotFoundError : return False","title":"delete_dir()"},{"location":"reference/filesystem_layer/#archivy.data.delete_item","text":"Delete dataobj of given id Source code in archivy/data.py def delete_item ( dataobj_id ): \"\"\"Delete dataobj of given id\"\"\" file = get_by_id ( dataobj_id ) remove_from_index ( dataobj_id ) if file : Path ( file ) . unlink ()","title":"delete_item()"},{"location":"reference/filesystem_layer/#archivy.data.format_file","text":"Converts normal md of file at path to formatted archivy markdown file, with yaml front matter and a filename of format \"{id}-{old_filename}.md\" Source code in archivy/data.py def format_file ( path : str ): \"\"\" Converts normal md of file at `path` to formatted archivy markdown file, with yaml front matter and a filename of format \"{id}-{old_filename}.md\" \"\"\" from archivy.models import DataObj data_dir = get_data_dir () path = Path ( path ) if not path . exists (): return if path . is_dir (): for filename in path . iterdir (): format_file ( filename ) else : new_file = path . open ( \"r\" , encoding = \"utf-8\" ) file_contents = new_file . read () new_file . close () try : # get relative path of object in `data` dir datapath = path . parent . resolve () . relative_to ( data_dir ) except ValueError : datapath = Path () note_dataobj = { \"title\" : path . name . replace ( \".md\" , \"\" ), \"content\" : file_contents , \"type\" : \"note\" , \"path\" : str ( datapath ), } dataobj = DataObj ( ** note_dataobj ) dataobj . insert () path . unlink () current_app . logger . info ( f \"Formatted and moved { str ( datapath / path . name ) } to { dataobj . fullpath } \" )","title":"format_file()"},{"location":"reference/filesystem_layer/#archivy.data.get_by_id","text":"Returns filename of dataobj of given id Source code in archivy/data.py def get_by_id ( dataobj_id ): \"\"\"Returns filename of dataobj of given id\"\"\" results = list ( get_data_dir () . rglob ( f \" { dataobj_id } -*.md\" )) return results [ 0 ] if results else None","title":"get_by_id()"},{"location":"reference/filesystem_layer/#archivy.data.get_data_dir","text":"Returns the directory where dataobjs are stored Source code in archivy/data.py def get_data_dir (): \"\"\"Returns the directory where dataobjs are stored\"\"\" return Path ( current_app . config [ \"USER_DIR\" ]) / \"data\"","title":"get_data_dir()"},{"location":"reference/filesystem_layer/#archivy.data.get_dirs","text":"Gets all dir names where dataobjs are stored Source code in archivy/data.py def get_dirs (): \"\"\"Gets all dir names where dataobjs are stored\"\"\" # join glob matchers dirnames = [ str ( dir_path . relative_to ( get_data_dir ())) for dir_path in get_data_dir () . rglob ( \"*\" ) if dir_path . is_dir () ] return dirnames","title":"get_dirs()"},{"location":"reference/filesystem_layer/#archivy.data.get_item","text":"Returns a Post object with the given dataobjs' attributes Source code in archivy/data.py def get_item ( dataobj_id ): \"\"\"Returns a Post object with the given dataobjs' attributes\"\"\" file = get_by_id ( dataobj_id ) if file : data = frontmatter . load ( file ) data [ \"fullpath\" ] = str ( file ) data [ \"dir\" ] = str ( file . parent . relative_to ( get_data_dir ())) # replace . for root items to '' if data [ \"dir\" ] == \".\" : data [ \"dir\" ] = \"\" return data return None","title":"get_item()"},{"location":"reference/filesystem_layer/#archivy.data.get_items","text":"Gets all dataobjs. collections - filter dataobj by type, eg. bookmark / note path - filter by path **structured: if set to True, will return a Directory object, otherwise data will just be returned as a list of dataobjs json_format : boolean value used internally to pre-process dataobjs to send back a json response. load_content : internal value to disregard post content and not save them in memory if they won't be accessed. Source code in archivy/data.py def get_items ( collections = [], path = \"\" , structured = True , json_format = False , load_content = True ): \"\"\" Gets all dataobjs. Parameters: - **collections** - filter dataobj by type, eg. bookmark / note - **path** - filter by path - **structured: if set to True, will return a Directory object, otherwise data will just be returned as a list of dataobjs - **json_format**: boolean value used internally to pre-process dataobjs to send back a json response. - **load_content**: internal value to disregard post content and not save them in memory if they won't be accessed. \"\"\" data_dir = get_data_dir () query_dir = data_dir / path if not is_relative_to ( query_dir , data_dir ) or not query_dir . exists (): raise FileNotFoundError if structured : return build_dir_tree ( path , query_dir ) else : datacont = [] for filepath in query_dir . rglob ( \"*.md\" ): data = frontmatter . load ( filepath ) if not load_content : data . content = \"\" data [ \"fullpath\" ] = str ( filepath . parent . relative_to ( query_dir )) if len ( collections ) == 0 or any ( [ collection == data [ \"type\" ] for collection in collections ] ): if json_format : dict_dataobj = data . __dict__ # remove unnecessary yaml handler dict_dataobj . pop ( \"handler\" ) datacont . append ( dict_dataobj ) else : datacont . append ( data ) return datacont","title":"get_items()"},{"location":"reference/filesystem_layer/#archivy.data.is_relative_to","text":"Implement pathlib is_relative_to only available in python 3.9 Source code in archivy/data.py def is_relative_to ( sub_path , parent ): \"\"\"Implement pathlib `is_relative_to` only available in python 3.9\"\"\" try : parent_path = Path ( parent ) . resolve () sub_path . resolve () . relative_to ( parent_path ) return True except ValueError : return False","title":"is_relative_to()"},{"location":"reference/filesystem_layer/#archivy.data.move_item","text":"Move dataobj of given id to new_path Source code in archivy/data.py def move_item ( dataobj_id , new_path ): \"\"\"Move dataobj of given id to new_path\"\"\" file = get_by_id ( dataobj_id ) data_dir = get_data_dir () out_dir = ( data_dir / new_path ) . resolve () if not file : raise FileNotFoundError if ( out_dir / file . parts [ - 1 ]) . exists (): raise FileExistsError elif is_relative_to ( out_dir , data_dir ) and out_dir . exists (): # check file isn't return shutil . move ( str ( file ), f \" { get_data_dir () } / { new_path } /\" ) return False","title":"move_item()"},{"location":"reference/filesystem_layer/#archivy.data.open_file","text":"Cross platform way of opening file on user's computer Source code in archivy/data.py def open_file ( path ): \"\"\"Cross platform way of opening file on user's computer\"\"\" if platform . system () == \"Windows\" : os . startfile ( path ) elif platform . system () == \"Darwin\" : subprocess . Popen ([ \"open\" , path ]) else : subprocess . Popen ([ \"xdg-open\" , path ])","title":"open_file()"},{"location":"reference/filesystem_layer/#archivy.data.save_image","text":"Saves image to USER_DATA_DIR Returns: filename where image has been saved. Source code in archivy/data.py def save_image ( image : FileStorage ): \"\"\" Saves image to USER_DATA_DIR Returns: filename where image has been saved. \"\"\" base_path = Path ( current_app . config [ \"USER_DIR\" ]) / \"images\" fileparts = image . filename . rsplit ( \".\" , 1 ) sanitized_filename = secure_filename ( fileparts [ 0 ]) dest_path = base_path / f \" { sanitized_filename } . { fileparts [ 1 ] } \" i = 1 while dest_path . exists (): dest_path = base_path / f \" { sanitized_filename } - { i } . { fileparts [ 1 ] } \" i += 1 image . save ( str ( dest_path )) return dest_path . parts [ - 1 ]","title":"save_image()"},{"location":"reference/filesystem_layer/#archivy.data.unformat_file","text":"Converts normal md of file at path to formatted archivy markdown file, with yaml front matter and a filename of format \"{id}-{old_filename}.md\" Source code in archivy/data.py def unformat_file ( path : str , out_dir : str ): \"\"\" Converts normal md of file at `path` to formatted archivy markdown file, with yaml front matter and a filename of format \"{id}-{old_filename}.md\" \"\"\" data_dir = get_data_dir () path = Path ( path ) out_dir = Path ( out_dir ) if not path . exists () and out_dir . exists () and out_dir . is_dir (): return if path . is_dir (): path . mkdir ( exist_ok = True ) for filename in path . iterdir (): unformat_file ( filename , str ( out_dir )) else : dataobj = frontmatter . load ( str ( path )) try : # get relative path of object in `data` dir datapath = path . parent . resolve () . relative_to ( data_dir ) except ValueError : datapath = Path () # create subdir if doesn't exist ( out_dir / datapath ) . mkdir ( exist_ok = True ) new_path = out_dir / datapath / f \" { dataobj . metadata [ 'title' ] } .md\" with new_path . open ( \"w\" ) as f : f . write ( dataobj . content ) current_app . logger . info ( f \"Unformatted and moved { str ( path ) } to { str ( new_path . resolve ()) } \" ) path . unlink ()","title":"unformat_file()"},{"location":"reference/filesystem_layer/#archivy.data.update_item_frontmatter","text":"Given an object id, this method overwrites the front matter of the post with new_frontmatter . date: Str id: Str path: Str tags: List[Str] title: Str type: note/bookmark Source code in archivy/data.py def update_item_frontmatter ( dataobj_id , new_frontmatter ): \"\"\" Given an object id, this method overwrites the front matter of the post with `new_frontmatter`. --- date: Str id: Str path: Str tags: List[Str] title: Str type: note/bookmark --- \"\"\" from archivy.models import DataObj filename = get_by_id ( dataobj_id ) dataobj = frontmatter . load ( filename ) for key in list ( new_frontmatter ): dataobj [ key ] = new_frontmatter [ key ] md = frontmatter . dumps ( dataobj ) with open ( filename , \"w\" , encoding = \"utf-8\" ) as f : f . write ( md ) converted_dataobj = DataObj . from_md ( md ) converted_dataobj . fullpath = str ( filename . relative_to ( current_app . config [ \"USER_DIR\" ]) ) converted_dataobj . index () current_app . config [ \"HOOKS\" ] . on_edit ( converted_dataobj )","title":"update_item_frontmatter()"},{"location":"reference/filesystem_layer/#archivy.data.update_item_md","text":"Given an object id, this method overwrites the inner content of the post with new_content . This means that it won't change the frontmatter (eg tags, id, title) but it can change the file content. For example: If we have a dataobj like this: --- id: 1 title: Note --- # This is random Calling update_item(1, \"# This is specific\") will turn it into: --- id: 1 # unchanged title: Note --- # This is specific Source code in archivy/data.py def update_item_md ( dataobj_id , new_content ): \"\"\" Given an object id, this method overwrites the inner content of the post with `new_content`. This means that it won't change the frontmatter (eg tags, id, title) but it can change the file content. For example: If we have a dataobj like this: ```md --- id: 1 title: Note --- # This is random ``` Calling `update_item(1, \"# This is specific\")` will turn it into: ```md --- id: 1 # unchanged title: Note --- # This is specific ``` \"\"\" from archivy.models import DataObj filename = get_by_id ( dataobj_id ) dataobj = frontmatter . load ( filename ) dataobj . content = new_content md = frontmatter . dumps ( dataobj ) with open ( filename , \"w\" , encoding = \"utf-8\" ) as f : f . write ( md ) converted_dataobj = DataObj . from_md ( md ) converted_dataobj . fullpath = str ( filename . relative_to ( current_app . config [ \"USER_DIR\" ]) ) converted_dataobj . index () current_app . config [ \"HOOKS\" ] . on_edit ( converted_dataobj )","title":"update_item_md()"},{"location":"reference/helpers/","text":"This is a series of helper functions that could be useful for you. Notably, the get_db and get_elastic_client could help with writing an archivy plugin. config_diff ( curr_key , curr_val , parent_dict , defaults ) This function diffs the user config with the defaults to only save what is actually different. Returns 1 if the current element or its nested elements are different and have been preserved. Source code in archivy/helpers.py def config_diff ( curr_key , curr_val , parent_dict , defaults ): \"\"\" This function diffs the user config with the defaults to only save what is actually different. Returns 1 if the current element or its nested elements are different and have been preserved. \"\"\" if type ( curr_val ) is dict : # the any call here diffs all nested children of the current dict and returns whether any have modifications if not any ( [ config_diff ( k , v , curr_val , defaults [ curr_key ]) for k , v in list ( curr_val . items ()) ] ): parent_dict . pop ( curr_key ) return 0 else : if defaults [ curr_key ] == curr_val : parent_dict . pop ( curr_key ) return 0 return 1 create_plugin_dir ( name ) Creates a sample plugin directory Source code in archivy/helpers.py def create_plugin_dir ( name ): \"\"\"Creates a sample plugin directory\"\"\" raw_name = name . replace ( \"archivy_\" , \"\" ) . replace ( \"archivy-\" , \"\" ) try : os . makedirs ( f \" { name } / { name } \" ) # Creates requirements.txt. with open ( f \" { name } /requirements.txt\" , \"w\" ) as fp : fp . writelines ([ \"archivy\" , \" \\n click\" ]) # Creates an empty readme file to be filled with open ( f \" { name } /README.md\" , \"w+\" ) as fp : fp . writelines ( [ f \"# { name } \" , \" \\n\\n ## Install\" , \" \\n\\n You need to have `archivy` already installed.\" , f \" \\n\\n Run `pip install archivy_ { name } `\" , \" \\n\\n ## Usage\" , ] ) # Creates a setup.py file with open ( f \" { name } /setup.py\" , \"w\" ) as setup_f : setup_f . writelines ( [ \"from setuptools import setup, find_packages\" , ' \\n\\n with open(\"README.md\", \"r\") as fh:' , \" \\n\\t long_description = fh.read()\" , ' \\n\\n with open(\"requirements.txt\", encoding=\"utf-8\") as f:' , ' \\n\\t all_reqs = f.read().split(\" \\\\ n\")' , \" \\n\\t install_requires = [x.strip() for x in all_reqs]\" , \" \\n\\n #Fill in the details below for distribution purposes\" f ' \\n setup( \\n\\t name=\" { name } \",' , ' \\n\\t version=\"0.0.1\",' , ' \\n\\t author=\"\",' , ' \\n\\t author_email=\"\",' , ' \\n\\t description=\"\",' , \" \\n\\t long_description=long_description,\" , ' \\n\\t long_description_content_type=\"text/markdown\",' , ' \\n\\t classifiers=[\"Programming Language :: Python :: 3\"],' \" \\n\\t packages=find_packages(),\" , \" \\n\\t install_requires=install_requires,\" , f ' \\n\\t entry_points=\"\"\" \\n\\t\\t [archivy.plugins]' f ' \\n\\t\\t { raw_name } = { name } : { raw_name } \"\"\" \\n )' , ] ) # Creating a basic __init__.py file where the main function of the plugin goes with open ( f \" { name } / { name } /__init__.py\" , \"w\" ) as fp : fp . writelines ( [ \"import archivy\" , \" \\n import click\" , \" \\n\\n # Fill in the functionality for the commands (see https://archivy.github.io/plugins/)\" , \" \\n @click.group()\" , f \" \\n def { raw_name } ():\" , \" \\n\\t pass\" , f \" \\n\\n @ { raw_name } .command()\" , \" \\n def command1():\" , \" \\n\\t pass\" , f \" \\n\\n @ { raw_name } .command()\" , \" \\n def command2():\" , \" \\n\\t pass\" , ] ) return True except FileExistsError : return False get_db ( force_reconnect = False ) Returns the database object that you can use to store data persistently Source code in archivy/helpers.py def get_db ( force_reconnect = False ): \"\"\" Returns the database object that you can use to store data persistently \"\"\" if \"db\" not in g or force_reconnect : g . db = TinyDB ( str ( Path ( current_app . config [ \"INTERNAL_DIR\" ]) / \"db.json\" )) return g . db get_elastic_client ( error_if_invalid = True ) Returns the elasticsearch client you can use to search and insert / delete data Source code in archivy/helpers.py def get_elastic_client ( error_if_invalid = True ): \"\"\"Returns the elasticsearch client you can use to search and insert / delete data\"\"\" if ( not current_app . config [ \"SEARCH_CONF\" ][ \"enabled\" ] or current_app . config [ \"SEARCH_CONF\" ][ \"engine\" ] != \"elasticsearch\" ) and error_if_invalid : return None auth_setup = ( current_app . config [ \"SEARCH_CONF\" ][ \"es_user\" ] and current_app . config [ \"SEARCH_CONF\" ][ \"es_password\" ] ) if auth_setup : es = Elasticsearch ( current_app . config [ \"SEARCH_CONF\" ][ \"url\" ], http_auth = ( current_app . config [ \"SEARCH_CONF\" ][ \"es_user\" ], current_app . config [ \"SEARCH_CONF\" ][ \"es_password\" ], ), ) else : es = Elasticsearch ( current_app . config [ \"SEARCH_CONF\" ][ \"url\" ]) if error_if_invalid : test_es_connection ( es ) else : try : es . cluster . health () except elasticsearch . exceptions . ConnectionError : return False return es get_max_id () Returns the current maximum id of dataobjs in the database. Source code in archivy/helpers.py def get_max_id (): \"\"\"Returns the current maximum id of dataobjs in the database.\"\"\" db = get_db () max_id = db . search ( Query () . name == \"max_id\" ) if not max_id : db . insert ({ \"name\" : \"max_id\" , \"val\" : 0 }) return 0 return max_id [ 0 ][ \"val\" ] load_config ( path = '' ) Loads config.yml file safely and deserializes it to a python dict. Source code in archivy/helpers.py def load_config ( path = \"\" ): \"\"\"Loads `config.yml` file safely and deserializes it to a python dict.\"\"\" path = path or current_app . config [ \"INTERNAL_DIR\" ] with ( Path ( path ) / \"config.yml\" ) . open () as f : return yaml . load ( f . read (), Loader = yaml . SafeLoader ) set_max_id ( val ) Sets a new max_id Source code in archivy/helpers.py def set_max_id ( val ): \"\"\"Sets a new max_id\"\"\" db = get_db () db . update ( operations . set ( \"val\" , val ), Query () . name == \"max_id\" ) test_es_connection ( es ) Tests health and presence of connection to elasticsearch. Source code in archivy/helpers.py def test_es_connection ( es ): \"\"\"Tests health and presence of connection to elasticsearch.\"\"\" try : health = es . cluster . health () except elasticsearch . exceptions . ConnectionError : current_app . logger . error ( \"Elasticsearch does not seem to be running on \" f \" { current_app . config [ 'SEARCH_CONF' ][ 'url' ] } . Please start \" \"it, for example with: sudo service elasticsearch restart\" ) current_app . logger . error ( \"You can disable Elasticsearch by modifying the `enabled` variable \" f \"in { str ( Path ( current_app . config [ 'INTERNAL_DIR' ]) / 'config.yml' ) } \" ) sys . exit ( 1 ) if health [ \"status\" ] not in ( \"yellow\" , \"green\" ): current_app . logger . warning ( \"Elasticsearch reports that it is not working \" \"properly. Search might not work. You can disable \" \"Elasticsearch by setting ELASTICSEARCH_ENABLED to 0.\" ) write_config ( config ) Writes a new config dict to a config.yml file that will override defaults. Compares user config with defaults to only save changes. Source code in archivy/helpers.py def write_config ( config : dict ): \"\"\" Writes a new config dict to a `config.yml` file that will override defaults. Compares user config with defaults to only save changes. \"\"\" defaults = vars ( Config ()) for k , v in list ( config . items ()): if k != \"SECRET_KEY\" : config_diff ( k , v , config , defaults ) with ( Path ( current_app . config [ \"INTERNAL_DIR\" ]) / \"config.yml\" ) . open ( \"w\" ) as f : yaml . dump ( config , f )","title":"Helpers"},{"location":"reference/helpers/#archivy.helpers.config_diff","text":"This function diffs the user config with the defaults to only save what is actually different. Returns 1 if the current element or its nested elements are different and have been preserved. Source code in archivy/helpers.py def config_diff ( curr_key , curr_val , parent_dict , defaults ): \"\"\" This function diffs the user config with the defaults to only save what is actually different. Returns 1 if the current element or its nested elements are different and have been preserved. \"\"\" if type ( curr_val ) is dict : # the any call here diffs all nested children of the current dict and returns whether any have modifications if not any ( [ config_diff ( k , v , curr_val , defaults [ curr_key ]) for k , v in list ( curr_val . items ()) ] ): parent_dict . pop ( curr_key ) return 0 else : if defaults [ curr_key ] == curr_val : parent_dict . pop ( curr_key ) return 0 return 1","title":"config_diff()"},{"location":"reference/helpers/#archivy.helpers.create_plugin_dir","text":"Creates a sample plugin directory Source code in archivy/helpers.py def create_plugin_dir ( name ): \"\"\"Creates a sample plugin directory\"\"\" raw_name = name . replace ( \"archivy_\" , \"\" ) . replace ( \"archivy-\" , \"\" ) try : os . makedirs ( f \" { name } / { name } \" ) # Creates requirements.txt. with open ( f \" { name } /requirements.txt\" , \"w\" ) as fp : fp . writelines ([ \"archivy\" , \" \\n click\" ]) # Creates an empty readme file to be filled with open ( f \" { name } /README.md\" , \"w+\" ) as fp : fp . writelines ( [ f \"# { name } \" , \" \\n\\n ## Install\" , \" \\n\\n You need to have `archivy` already installed.\" , f \" \\n\\n Run `pip install archivy_ { name } `\" , \" \\n\\n ## Usage\" , ] ) # Creates a setup.py file with open ( f \" { name } /setup.py\" , \"w\" ) as setup_f : setup_f . writelines ( [ \"from setuptools import setup, find_packages\" , ' \\n\\n with open(\"README.md\", \"r\") as fh:' , \" \\n\\t long_description = fh.read()\" , ' \\n\\n with open(\"requirements.txt\", encoding=\"utf-8\") as f:' , ' \\n\\t all_reqs = f.read().split(\" \\\\ n\")' , \" \\n\\t install_requires = [x.strip() for x in all_reqs]\" , \" \\n\\n #Fill in the details below for distribution purposes\" f ' \\n setup( \\n\\t name=\" { name } \",' , ' \\n\\t version=\"0.0.1\",' , ' \\n\\t author=\"\",' , ' \\n\\t author_email=\"\",' , ' \\n\\t description=\"\",' , \" \\n\\t long_description=long_description,\" , ' \\n\\t long_description_content_type=\"text/markdown\",' , ' \\n\\t classifiers=[\"Programming Language :: Python :: 3\"],' \" \\n\\t packages=find_packages(),\" , \" \\n\\t install_requires=install_requires,\" , f ' \\n\\t entry_points=\"\"\" \\n\\t\\t [archivy.plugins]' f ' \\n\\t\\t { raw_name } = { name } : { raw_name } \"\"\" \\n )' , ] ) # Creating a basic __init__.py file where the main function of the plugin goes with open ( f \" { name } / { name } /__init__.py\" , \"w\" ) as fp : fp . writelines ( [ \"import archivy\" , \" \\n import click\" , \" \\n\\n # Fill in the functionality for the commands (see https://archivy.github.io/plugins/)\" , \" \\n @click.group()\" , f \" \\n def { raw_name } ():\" , \" \\n\\t pass\" , f \" \\n\\n @ { raw_name } .command()\" , \" \\n def command1():\" , \" \\n\\t pass\" , f \" \\n\\n @ { raw_name } .command()\" , \" \\n def command2():\" , \" \\n\\t pass\" , ] ) return True except FileExistsError : return False","title":"create_plugin_dir()"},{"location":"reference/helpers/#archivy.helpers.get_db","text":"Returns the database object that you can use to store data persistently Source code in archivy/helpers.py def get_db ( force_reconnect = False ): \"\"\" Returns the database object that you can use to store data persistently \"\"\" if \"db\" not in g or force_reconnect : g . db = TinyDB ( str ( Path ( current_app . config [ \"INTERNAL_DIR\" ]) / \"db.json\" )) return g . db","title":"get_db()"},{"location":"reference/helpers/#archivy.helpers.get_elastic_client","text":"Returns the elasticsearch client you can use to search and insert / delete data Source code in archivy/helpers.py def get_elastic_client ( error_if_invalid = True ): \"\"\"Returns the elasticsearch client you can use to search and insert / delete data\"\"\" if ( not current_app . config [ \"SEARCH_CONF\" ][ \"enabled\" ] or current_app . config [ \"SEARCH_CONF\" ][ \"engine\" ] != \"elasticsearch\" ) and error_if_invalid : return None auth_setup = ( current_app . config [ \"SEARCH_CONF\" ][ \"es_user\" ] and current_app . config [ \"SEARCH_CONF\" ][ \"es_password\" ] ) if auth_setup : es = Elasticsearch ( current_app . config [ \"SEARCH_CONF\" ][ \"url\" ], http_auth = ( current_app . config [ \"SEARCH_CONF\" ][ \"es_user\" ], current_app . config [ \"SEARCH_CONF\" ][ \"es_password\" ], ), ) else : es = Elasticsearch ( current_app . config [ \"SEARCH_CONF\" ][ \"url\" ]) if error_if_invalid : test_es_connection ( es ) else : try : es . cluster . health () except elasticsearch . exceptions . ConnectionError : return False return es","title":"get_elastic_client()"},{"location":"reference/helpers/#archivy.helpers.get_max_id","text":"Returns the current maximum id of dataobjs in the database. Source code in archivy/helpers.py def get_max_id (): \"\"\"Returns the current maximum id of dataobjs in the database.\"\"\" db = get_db () max_id = db . search ( Query () . name == \"max_id\" ) if not max_id : db . insert ({ \"name\" : \"max_id\" , \"val\" : 0 }) return 0 return max_id [ 0 ][ \"val\" ]","title":"get_max_id()"},{"location":"reference/helpers/#archivy.helpers.load_config","text":"Loads config.yml file safely and deserializes it to a python dict. Source code in archivy/helpers.py def load_config ( path = \"\" ): \"\"\"Loads `config.yml` file safely and deserializes it to a python dict.\"\"\" path = path or current_app . config [ \"INTERNAL_DIR\" ] with ( Path ( path ) / \"config.yml\" ) . open () as f : return yaml . load ( f . read (), Loader = yaml . SafeLoader )","title":"load_config()"},{"location":"reference/helpers/#archivy.helpers.set_max_id","text":"Sets a new max_id Source code in archivy/helpers.py def set_max_id ( val ): \"\"\"Sets a new max_id\"\"\" db = get_db () db . update ( operations . set ( \"val\" , val ), Query () . name == \"max_id\" )","title":"set_max_id()"},{"location":"reference/helpers/#archivy.helpers.test_es_connection","text":"Tests health and presence of connection to elasticsearch. Source code in archivy/helpers.py def test_es_connection ( es ): \"\"\"Tests health and presence of connection to elasticsearch.\"\"\" try : health = es . cluster . health () except elasticsearch . exceptions . ConnectionError : current_app . logger . error ( \"Elasticsearch does not seem to be running on \" f \" { current_app . config [ 'SEARCH_CONF' ][ 'url' ] } . Please start \" \"it, for example with: sudo service elasticsearch restart\" ) current_app . logger . error ( \"You can disable Elasticsearch by modifying the `enabled` variable \" f \"in { str ( Path ( current_app . config [ 'INTERNAL_DIR' ]) / 'config.yml' ) } \" ) sys . exit ( 1 ) if health [ \"status\" ] not in ( \"yellow\" , \"green\" ): current_app . logger . warning ( \"Elasticsearch reports that it is not working \" \"properly. Search might not work. You can disable \" \"Elasticsearch by setting ELASTICSEARCH_ENABLED to 0.\" )","title":"test_es_connection()"},{"location":"reference/helpers/#archivy.helpers.write_config","text":"Writes a new config dict to a config.yml file that will override defaults. Compares user config with defaults to only save changes. Source code in archivy/helpers.py def write_config ( config : dict ): \"\"\" Writes a new config dict to a `config.yml` file that will override defaults. Compares user config with defaults to only save changes. \"\"\" defaults = vars ( Config ()) for k , v in list ( config . items ()): if k != \"SECRET_KEY\" : config_diff ( k , v , config , defaults ) with ( Path ( current_app . config [ \"INTERNAL_DIR\" ]) / \"config.yml\" ) . open ( \"w\" ) as f : yaml . dump ( config , f )","title":"write_config()"},{"location":"reference/hooks/","text":"Class of methods users can inherit to configure and extend archivy with hooks. Usage: Archivy checks for the presence of a hooks.py file in the user directory that stores the data/ directory with your notes and bookmarks. This location is usually set during archivy init . Example hooks.py file: from archivy.config import BaseHooks class Hooks ( BaseHooks ): def on_edit ( self , dataobj ): print ( f \"Edit made to { dataobj . title } \" ) def before_dataobj_create ( self , dataobj ): from random import randint dataobj . content += f \" \\n This note's random number is { randint ( 1 , 10 ) } \" # ... If you have ideas for any other hooks you'd find useful if they were supported, please open an issue . Source code in archivy/config.py class BaseHooks : \"\"\" Class of methods users can inherit to configure and extend archivy with hooks. ## Usage: Archivy checks for the presence of a `hooks.py` file in the user directory that stores the `data/` directory with your notes and bookmarks. This location is usually set during `archivy init`. Example `hooks.py` file: ```python from archivy.config import BaseHooks class Hooks(BaseHooks): def on_edit(self, dataobj): print(f\"Edit made to {dataobj.title}\") def before_dataobj_create(self, dataobj): from random import randint dataobj.content += f\"\\\\nThis note's random number is {randint(1, 10)}\" # ... ``` If you have ideas for any other hooks you'd find useful if they were supported, please open an [issue](https://github.com/archivy/archivy/issues). \"\"\" def on_dataobj_create ( self , dataobj ): \"\"\"Hook for dataobj creation.\"\"\" def before_dataobj_create ( self , dataobj ): \"\"\"Hook called immediately before dataobj creation.\"\"\" def on_user_create ( self , user ): \"\"\"Hook called after a new user is created.\"\"\" def on_edit ( self , dataobj ): \"\"\"Hook called whenever a user edits through the web interface or the API.\"\"\" before_dataobj_create ( self , dataobj ) Hook called immediately before dataobj creation. Source code in archivy/config.py def before_dataobj_create ( self , dataobj ): \"\"\"Hook called immediately before dataobj creation.\"\"\" on_dataobj_create ( self , dataobj ) Hook for dataobj creation. Source code in archivy/config.py def on_dataobj_create ( self , dataobj ): \"\"\"Hook for dataobj creation.\"\"\" on_edit ( self , dataobj ) Hook called whenever a user edits through the web interface or the API. Source code in archivy/config.py def on_edit ( self , dataobj ): \"\"\"Hook called whenever a user edits through the web interface or the API.\"\"\" on_user_create ( self , user ) Hook called after a new user is created. Source code in archivy/config.py def on_user_create ( self , user ): \"\"\"Hook called after a new user is created.\"\"\"","title":"Hooks"},{"location":"reference/hooks/#archivy.config.BaseHooks--usage","text":"Archivy checks for the presence of a hooks.py file in the user directory that stores the data/ directory with your notes and bookmarks. This location is usually set during archivy init . Example hooks.py file: from archivy.config import BaseHooks class Hooks ( BaseHooks ): def on_edit ( self , dataobj ): print ( f \"Edit made to { dataobj . title } \" ) def before_dataobj_create ( self , dataobj ): from random import randint dataobj . content += f \" \\n This note's random number is { randint ( 1 , 10 ) } \" # ... If you have ideas for any other hooks you'd find useful if they were supported, please open an issue . Source code in archivy/config.py class BaseHooks : \"\"\" Class of methods users can inherit to configure and extend archivy with hooks. ## Usage: Archivy checks for the presence of a `hooks.py` file in the user directory that stores the `data/` directory with your notes and bookmarks. This location is usually set during `archivy init`. Example `hooks.py` file: ```python from archivy.config import BaseHooks class Hooks(BaseHooks): def on_edit(self, dataobj): print(f\"Edit made to {dataobj.title}\") def before_dataobj_create(self, dataobj): from random import randint dataobj.content += f\"\\\\nThis note's random number is {randint(1, 10)}\" # ... ``` If you have ideas for any other hooks you'd find useful if they were supported, please open an [issue](https://github.com/archivy/archivy/issues). \"\"\" def on_dataobj_create ( self , dataobj ): \"\"\"Hook for dataobj creation.\"\"\" def before_dataobj_create ( self , dataobj ): \"\"\"Hook called immediately before dataobj creation.\"\"\" def on_user_create ( self , user ): \"\"\"Hook called after a new user is created.\"\"\" def on_edit ( self , dataobj ): \"\"\"Hook called whenever a user edits through the web interface or the API.\"\"\"","title":"Usage:"},{"location":"reference/hooks/#archivy.config.BaseHooks.before_dataobj_create","text":"Hook called immediately before dataobj creation. Source code in archivy/config.py def before_dataobj_create ( self , dataobj ): \"\"\"Hook called immediately before dataobj creation.\"\"\"","title":"before_dataobj_create()"},{"location":"reference/hooks/#archivy.config.BaseHooks.on_dataobj_create","text":"Hook for dataobj creation. Source code in archivy/config.py def on_dataobj_create ( self , dataobj ): \"\"\"Hook for dataobj creation.\"\"\"","title":"on_dataobj_create()"},{"location":"reference/hooks/#archivy.config.BaseHooks.on_edit","text":"Hook called whenever a user edits through the web interface or the API. Source code in archivy/config.py def on_edit ( self , dataobj ): \"\"\"Hook called whenever a user edits through the web interface or the API.\"\"\"","title":"on_edit()"},{"location":"reference/hooks/#archivy.config.BaseHooks.on_user_create","text":"Hook called after a new user is created. Source code in archivy/config.py def on_user_create ( self , user ): \"\"\"Hook called after a new user is created.\"\"\"","title":"on_user_create()"},{"location":"reference/models/","text":"Internal API for the models Archivy uses in the backend that could be useful for writing plugins. DataObj Class that holds a data object (either a note or a bookmark). [Required to pass when creating a new object] type -> \"note\" or \"bookmark\" Note : - title Bookmark : url [Optional attrs that if passed, will be set by the class] tags content path [Handled by the code] id date For bookmarks, Run process_bookmark_url() once you've created it. For both types, run insert() if you want to create a new file in the db with their contents. Source code in archivy/models.py class DataObj : \"\"\" Class that holds a data object (either a note or a bookmark). Attributes: [Required to pass when creating a new object] - **type** -> \"note\" or \"bookmark\" **Note**: - title **Bookmark**: - url [Optional attrs that if passed, will be set by the class] - tags - content - path [Handled by the code] - id - date For bookmarks, Run `process_bookmark_url()` once you've created it. For both types, run `insert()` if you want to create a new file in the db with their contents. \"\"\" __searchable__ = [ \"title\" , \"content\" , \"tags\" ] id : Optional [ int ] = attrib ( validator = optional ( instance_of ( int )), default = None ) type : str = attrib ( validator = instance_of ( str )) title : str = attrib ( validator = instance_of ( str ), default = \"\" ) content : str = attrib ( validator = instance_of ( str ), default = \"\" ) tags : List [ str ] = attrib ( validator = instance_of ( list ), default = []) url : Optional [ str ] = attrib ( validator = optional ( instance_of ( str )), default = None ) date : Optional [ datetime ] = attrib ( validator = optional ( instance_of ( datetime )), default = None ) path : str = attrib ( validator = instance_of ( str ), default = \"\" ) fullpath : Optional [ str ] = attrib ( validator = optional ( instance_of ( str )), default = None ) error : Optional [ str ] = attrib ( validator = optional ( instance_of ( str )), default = None ) def process_bookmark_url ( self , raw_html = None ): \"\"\"Process url to get content for bookmark\"\"\" if self . type not in ( \"bookmark\" , \"pocket_bookmark\" ) or not validators . url ( self . url ): return None selector = None for pattern , handler in current_app . config [ \"SCRAPING_PATTERNS\" ] . items (): if fnmatch . fnmatch ( self . url , pattern ): if type ( handler ) == str : # if the handler is a string, it's simply a css selector to process the page with selector = handler break # otherwise custom user function that overrides archivy behavior handler ( self ) return try : page_html = ( raw_html or requests . get ( self . url , headers = { \"User-agent\" : f \"Archivy/v { require ( 'archivy' )[ 0 ] . version } \" }, ) . text ) except Exception : self . error = f \"Could not retrieve { self . url } \\n \" self . wipe () return try : document = Document ( page_html ) self . title = document . short_title () or self . url parsed_html = BeautifulSoup ( document . summary (), features = \"html.parser\" ) except Exception : self . error = f \"Could not parse { self . url } \\n \" self . wipe () return try : self . content = self . extract_content ( parsed_html , selector ) except Exception : self . error = f \"Could not extract content from { self . url } \\n \" return def wipe ( self ): \"\"\"Resets and invalidates dataobj\"\"\" self . title = \"\" self . content = \"\" def extract_content ( self , beautsoup , selector = None ): \"\"\"converts html bookmark url to optimized markdown and saves images\"\"\" url = self . url . rstrip ( \"/\" ) if selector : selected_soup = beautsoup . select ( selector ) # if the custom selector matched, take the first occurrence if selected_soup : beautsoup = selected_soup [ 0 ] resources = beautsoup . find_all ([ \"a\" , \"img\" ]) for tag in resources : if tag . name == \"a\" : if tag . has_attr ( \"href\" ) and ( tag [ \"href\" ] . startswith ( \"/\" )): tag [ \"href\" ] = urljoin ( url , tag [ \"href\" ]) # check it's a normal link and not some sort of image # string returns the text content of the tag if not tag . string : # delete tag tag . decompose () elif tag . name == \"img\" and tag . has_attr ( \"src\" ): filename = tag [ \"src\" ] . split ( \"/\" )[ - 1 ] try : filename = filename [ : filename . index ( \"?\" ) ] # remove query parameters except ValueError : pass if not tag [ \"src\" ] . startswith ( \"http\" ): tag [ \"src\" ] = urljoin ( url , tag [ \"src\" ]) if current_app . config [ \"SCRAPING_CONF\" ][ \"save_images\" ] and valid_image_filename ( filename ): image = FileStorage ( BytesIO ( requests . get ( tag [ \"src\" ]) . content ), filename , name = \"file\" ) saved_to = save_image ( image ) tag [ \"src\" ] = \"/images/\" + saved_to res = html2text ( str ( beautsoup ), bodywidth = 0 ) return res def validate ( self ): \"\"\"Verifies that the content matches required validation constraints\"\"\" valid_url = ( self . type != \"bookmark\" or self . type != \"pocket_bookmark\" ) or ( isinstance ( self . url , str ) and validators . url ( self . url ) ) valid_title = isinstance ( self . title , str ) and self . title != \"\" valid_content = self . type not in ( \"bookmark\" , \"pocket_bookmark\" ) or isinstance ( self . content , str ) return valid_url and valid_title and valid_content def insert ( self ): \"\"\"Creates a new file with the object's attributes\"\"\" if self . validate (): for tag in self . tags : add_tag_to_index ( tag ) helpers . set_max_id ( helpers . get_max_id () + 1 ) self . id = helpers . get_max_id () self . date = datetime . now () hooks = current_app . config [ \"HOOKS\" ] hooks . before_dataobj_create ( self ) data = { \"type\" : self . type , \"title\" : str ( self . title ), \"date\" : self . date . strftime ( \" %x \" ) . replace ( \"/\" , \"-\" ), \"tags\" : self . tags , \"id\" : self . id , \"path\" : self . path , } if self . type == \"bookmark\" or self . type == \"pocket_bookmark\" : data [ \"url\" ] = self . url # convert to markdown file dataobj = frontmatter . Post ( self . content ) dataobj . metadata = data self . fullpath = str ( create ( frontmatter . dumps ( dataobj ), f \" { self . id } - { dataobj [ 'title' ] } \" , path = self . path , ) ) hooks . on_dataobj_create ( self ) self . index () return self . id return False def index ( self ): return add_to_index ( self ) @classmethod def from_md ( cls , md_content : str ): \"\"\" Class method to generate new dataobj from a well formatted markdown string Call like this: ```python Dataobj.from_md(content) ``` \"\"\" data = frontmatter . loads ( md_content ) dataobj = {} dataobj [ \"content\" ] = data . content for pair in [ \"id\" , \"title\" , \"path\" , \"tags\" ]: try : dataobj [ pair ] = data [ pair ] except KeyError : # files sometimes get moved temporarily by applications while you edit # this can create bugs where the data is not loaded correctly # this handles that scenario as validation will simply fail and the event will # be ignored break dataobj [ \"type\" ] = \"processed-dataobj\" return cls ( ** dataobj ) extract_content ( self , beautsoup , selector = None ) converts html bookmark url to optimized markdown and saves images Source code in archivy/models.py def extract_content ( self , beautsoup , selector = None ): \"\"\"converts html bookmark url to optimized markdown and saves images\"\"\" url = self . url . rstrip ( \"/\" ) if selector : selected_soup = beautsoup . select ( selector ) # if the custom selector matched, take the first occurrence if selected_soup : beautsoup = selected_soup [ 0 ] resources = beautsoup . find_all ([ \"a\" , \"img\" ]) for tag in resources : if tag . name == \"a\" : if tag . has_attr ( \"href\" ) and ( tag [ \"href\" ] . startswith ( \"/\" )): tag [ \"href\" ] = urljoin ( url , tag [ \"href\" ]) # check it's a normal link and not some sort of image # string returns the text content of the tag if not tag . string : # delete tag tag . decompose () elif tag . name == \"img\" and tag . has_attr ( \"src\" ): filename = tag [ \"src\" ] . split ( \"/\" )[ - 1 ] try : filename = filename [ : filename . index ( \"?\" ) ] # remove query parameters except ValueError : pass if not tag [ \"src\" ] . startswith ( \"http\" ): tag [ \"src\" ] = urljoin ( url , tag [ \"src\" ]) if current_app . config [ \"SCRAPING_CONF\" ][ \"save_images\" ] and valid_image_filename ( filename ): image = FileStorage ( BytesIO ( requests . get ( tag [ \"src\" ]) . content ), filename , name = \"file\" ) saved_to = save_image ( image ) tag [ \"src\" ] = \"/images/\" + saved_to res = html2text ( str ( beautsoup ), bodywidth = 0 ) return res from_md ( md_content ) classmethod Class method to generate new dataobj from a well formatted markdown string Call like this: Dataobj . from_md ( content ) Source code in archivy/models.py @classmethod def from_md ( cls , md_content : str ): \"\"\" Class method to generate new dataobj from a well formatted markdown string Call like this: ```python Dataobj.from_md(content) ``` \"\"\" data = frontmatter . loads ( md_content ) dataobj = {} dataobj [ \"content\" ] = data . content for pair in [ \"id\" , \"title\" , \"path\" , \"tags\" ]: try : dataobj [ pair ] = data [ pair ] except KeyError : # files sometimes get moved temporarily by applications while you edit # this can create bugs where the data is not loaded correctly # this handles that scenario as validation will simply fail and the event will # be ignored break dataobj [ \"type\" ] = \"processed-dataobj\" return cls ( ** dataobj ) insert ( self ) Creates a new file with the object's attributes Source code in archivy/models.py def insert ( self ): \"\"\"Creates a new file with the object's attributes\"\"\" if self . validate (): for tag in self . tags : add_tag_to_index ( tag ) helpers . set_max_id ( helpers . get_max_id () + 1 ) self . id = helpers . get_max_id () self . date = datetime . now () hooks = current_app . config [ \"HOOKS\" ] hooks . before_dataobj_create ( self ) data = { \"type\" : self . type , \"title\" : str ( self . title ), \"date\" : self . date . strftime ( \" %x \" ) . replace ( \"/\" , \"-\" ), \"tags\" : self . tags , \"id\" : self . id , \"path\" : self . path , } if self . type == \"bookmark\" or self . type == \"pocket_bookmark\" : data [ \"url\" ] = self . url # convert to markdown file dataobj = frontmatter . Post ( self . content ) dataobj . metadata = data self . fullpath = str ( create ( frontmatter . dumps ( dataobj ), f \" { self . id } - { dataobj [ 'title' ] } \" , path = self . path , ) ) hooks . on_dataobj_create ( self ) self . index () return self . id return False process_bookmark_url ( self , raw_html = None ) Process url to get content for bookmark Source code in archivy/models.py def process_bookmark_url ( self , raw_html = None ): \"\"\"Process url to get content for bookmark\"\"\" if self . type not in ( \"bookmark\" , \"pocket_bookmark\" ) or not validators . url ( self . url ): return None selector = None for pattern , handler in current_app . config [ \"SCRAPING_PATTERNS\" ] . items (): if fnmatch . fnmatch ( self . url , pattern ): if type ( handler ) == str : # if the handler is a string, it's simply a css selector to process the page with selector = handler break # otherwise custom user function that overrides archivy behavior handler ( self ) return try : page_html = ( raw_html or requests . get ( self . url , headers = { \"User-agent\" : f \"Archivy/v { require ( 'archivy' )[ 0 ] . version } \" }, ) . text ) except Exception : self . error = f \"Could not retrieve { self . url } \\n \" self . wipe () return try : document = Document ( page_html ) self . title = document . short_title () or self . url parsed_html = BeautifulSoup ( document . summary (), features = \"html.parser\" ) except Exception : self . error = f \"Could not parse { self . url } \\n \" self . wipe () return try : self . content = self . extract_content ( parsed_html , selector ) except Exception : self . error = f \"Could not extract content from { self . url } \\n \" return validate ( self ) Verifies that the content matches required validation constraints Source code in archivy/models.py def validate ( self ): \"\"\"Verifies that the content matches required validation constraints\"\"\" valid_url = ( self . type != \"bookmark\" or self . type != \"pocket_bookmark\" ) or ( isinstance ( self . url , str ) and validators . url ( self . url ) ) valid_title = isinstance ( self . title , str ) and self . title != \"\" valid_content = self . type not in ( \"bookmark\" , \"pocket_bookmark\" ) or isinstance ( self . content , str ) return valid_url and valid_title and valid_content wipe ( self ) Resets and invalidates dataobj Source code in archivy/models.py def wipe ( self ): \"\"\"Resets and invalidates dataobj\"\"\" self . title = \"\" self . content = \"\" User ( UserMixin ) Model we use for User that inherits from flask login's UserMixin username password is_admin Source code in archivy/models.py class User ( UserMixin ): \"\"\" Model we use for User that inherits from flask login's [`UserMixin`](https://flask-login.readthedocs.io/en/latest/#flask_login.UserMixin) Attributes: - **username** - **password** - **is_admin** \"\"\" username : str = attrib ( validator = instance_of ( str )) password : Optional [ str ] = attrib ( validator = optional ( instance_of ( str )), default = None ) is_admin : Optional [ bool ] = attrib ( validator = optional ( instance_of ( bool )), default = None ) id : Optional [ int ] = attrib ( validator = optional ( instance_of ( int )), default = False ) def insert ( self ): \"\"\"Inserts the model from the database\"\"\" if not self . password : return False hashed_password = generate_password_hash ( self . password ) db = helpers . get_db () if db . search (( Query () . type == \"user\" ) & ( Query () . username == self . username )): return False db_user = { \"username\" : self . username , \"hashed_password\" : hashed_password , \"is_admin\" : self . is_admin , \"type\" : \"user\" , } current_app . config [ \"HOOKS\" ] . on_user_create ( self ) return db . insert ( db_user ) @classmethod def from_db ( cls , db_object ): \"\"\"Takes a database object and turns it into a user\"\"\" username = db_object [ \"username\" ] id = db_object . doc_id return cls ( username = username , id = id ) from_db ( db_object ) classmethod Takes a database object and turns it into a user Source code in archivy/models.py @classmethod def from_db ( cls , db_object ): \"\"\"Takes a database object and turns it into a user\"\"\" username = db_object [ \"username\" ] id = db_object . doc_id return cls ( username = username , id = id ) insert ( self ) Inserts the model from the database Source code in archivy/models.py def insert ( self ): \"\"\"Inserts the model from the database\"\"\" if not self . password : return False hashed_password = generate_password_hash ( self . password ) db = helpers . get_db () if db . search (( Query () . type == \"user\" ) & ( Query () . username == self . username )): return False db_user = { \"username\" : self . username , \"hashed_password\" : hashed_password , \"is_admin\" : self . is_admin , \"type\" : \"user\" , } current_app . config [ \"HOOKS\" ] . on_user_create ( self ) return db . insert ( db_user )","title":"Models for User and DataObj"},{"location":"reference/models/#archivy.models.DataObj","text":"Class that holds a data object (either a note or a bookmark). [Required to pass when creating a new object] type -> \"note\" or \"bookmark\" Note : - title Bookmark : url [Optional attrs that if passed, will be set by the class] tags content path [Handled by the code] id date For bookmarks, Run process_bookmark_url() once you've created it. For both types, run insert() if you want to create a new file in the db with their contents. Source code in archivy/models.py class DataObj : \"\"\" Class that holds a data object (either a note or a bookmark). Attributes: [Required to pass when creating a new object] - **type** -> \"note\" or \"bookmark\" **Note**: - title **Bookmark**: - url [Optional attrs that if passed, will be set by the class] - tags - content - path [Handled by the code] - id - date For bookmarks, Run `process_bookmark_url()` once you've created it. For both types, run `insert()` if you want to create a new file in the db with their contents. \"\"\" __searchable__ = [ \"title\" , \"content\" , \"tags\" ] id : Optional [ int ] = attrib ( validator = optional ( instance_of ( int )), default = None ) type : str = attrib ( validator = instance_of ( str )) title : str = attrib ( validator = instance_of ( str ), default = \"\" ) content : str = attrib ( validator = instance_of ( str ), default = \"\" ) tags : List [ str ] = attrib ( validator = instance_of ( list ), default = []) url : Optional [ str ] = attrib ( validator = optional ( instance_of ( str )), default = None ) date : Optional [ datetime ] = attrib ( validator = optional ( instance_of ( datetime )), default = None ) path : str = attrib ( validator = instance_of ( str ), default = \"\" ) fullpath : Optional [ str ] = attrib ( validator = optional ( instance_of ( str )), default = None ) error : Optional [ str ] = attrib ( validator = optional ( instance_of ( str )), default = None ) def process_bookmark_url ( self , raw_html = None ): \"\"\"Process url to get content for bookmark\"\"\" if self . type not in ( \"bookmark\" , \"pocket_bookmark\" ) or not validators . url ( self . url ): return None selector = None for pattern , handler in current_app . config [ \"SCRAPING_PATTERNS\" ] . items (): if fnmatch . fnmatch ( self . url , pattern ): if type ( handler ) == str : # if the handler is a string, it's simply a css selector to process the page with selector = handler break # otherwise custom user function that overrides archivy behavior handler ( self ) return try : page_html = ( raw_html or requests . get ( self . url , headers = { \"User-agent\" : f \"Archivy/v { require ( 'archivy' )[ 0 ] . version } \" }, ) . text ) except Exception : self . error = f \"Could not retrieve { self . url } \\n \" self . wipe () return try : document = Document ( page_html ) self . title = document . short_title () or self . url parsed_html = BeautifulSoup ( document . summary (), features = \"html.parser\" ) except Exception : self . error = f \"Could not parse { self . url } \\n \" self . wipe () return try : self . content = self . extract_content ( parsed_html , selector ) except Exception : self . error = f \"Could not extract content from { self . url } \\n \" return def wipe ( self ): \"\"\"Resets and invalidates dataobj\"\"\" self . title = \"\" self . content = \"\" def extract_content ( self , beautsoup , selector = None ): \"\"\"converts html bookmark url to optimized markdown and saves images\"\"\" url = self . url . rstrip ( \"/\" ) if selector : selected_soup = beautsoup . select ( selector ) # if the custom selector matched, take the first occurrence if selected_soup : beautsoup = selected_soup [ 0 ] resources = beautsoup . find_all ([ \"a\" , \"img\" ]) for tag in resources : if tag . name == \"a\" : if tag . has_attr ( \"href\" ) and ( tag [ \"href\" ] . startswith ( \"/\" )): tag [ \"href\" ] = urljoin ( url , tag [ \"href\" ]) # check it's a normal link and not some sort of image # string returns the text content of the tag if not tag . string : # delete tag tag . decompose () elif tag . name == \"img\" and tag . has_attr ( \"src\" ): filename = tag [ \"src\" ] . split ( \"/\" )[ - 1 ] try : filename = filename [ : filename . index ( \"?\" ) ] # remove query parameters except ValueError : pass if not tag [ \"src\" ] . startswith ( \"http\" ): tag [ \"src\" ] = urljoin ( url , tag [ \"src\" ]) if current_app . config [ \"SCRAPING_CONF\" ][ \"save_images\" ] and valid_image_filename ( filename ): image = FileStorage ( BytesIO ( requests . get ( tag [ \"src\" ]) . content ), filename , name = \"file\" ) saved_to = save_image ( image ) tag [ \"src\" ] = \"/images/\" + saved_to res = html2text ( str ( beautsoup ), bodywidth = 0 ) return res def validate ( self ): \"\"\"Verifies that the content matches required validation constraints\"\"\" valid_url = ( self . type != \"bookmark\" or self . type != \"pocket_bookmark\" ) or ( isinstance ( self . url , str ) and validators . url ( self . url ) ) valid_title = isinstance ( self . title , str ) and self . title != \"\" valid_content = self . type not in ( \"bookmark\" , \"pocket_bookmark\" ) or isinstance ( self . content , str ) return valid_url and valid_title and valid_content def insert ( self ): \"\"\"Creates a new file with the object's attributes\"\"\" if self . validate (): for tag in self . tags : add_tag_to_index ( tag ) helpers . set_max_id ( helpers . get_max_id () + 1 ) self . id = helpers . get_max_id () self . date = datetime . now () hooks = current_app . config [ \"HOOKS\" ] hooks . before_dataobj_create ( self ) data = { \"type\" : self . type , \"title\" : str ( self . title ), \"date\" : self . date . strftime ( \" %x \" ) . replace ( \"/\" , \"-\" ), \"tags\" : self . tags , \"id\" : self . id , \"path\" : self . path , } if self . type == \"bookmark\" or self . type == \"pocket_bookmark\" : data [ \"url\" ] = self . url # convert to markdown file dataobj = frontmatter . Post ( self . content ) dataobj . metadata = data self . fullpath = str ( create ( frontmatter . dumps ( dataobj ), f \" { self . id } - { dataobj [ 'title' ] } \" , path = self . path , ) ) hooks . on_dataobj_create ( self ) self . index () return self . id return False def index ( self ): return add_to_index ( self ) @classmethod def from_md ( cls , md_content : str ): \"\"\" Class method to generate new dataobj from a well formatted markdown string Call like this: ```python Dataobj.from_md(content) ``` \"\"\" data = frontmatter . loads ( md_content ) dataobj = {} dataobj [ \"content\" ] = data . content for pair in [ \"id\" , \"title\" , \"path\" , \"tags\" ]: try : dataobj [ pair ] = data [ pair ] except KeyError : # files sometimes get moved temporarily by applications while you edit # this can create bugs where the data is not loaded correctly # this handles that scenario as validation will simply fail and the event will # be ignored break dataobj [ \"type\" ] = \"processed-dataobj\" return cls ( ** dataobj )","title":"DataObj"},{"location":"reference/models/#archivy.models.DataObj.extract_content","text":"converts html bookmark url to optimized markdown and saves images Source code in archivy/models.py def extract_content ( self , beautsoup , selector = None ): \"\"\"converts html bookmark url to optimized markdown and saves images\"\"\" url = self . url . rstrip ( \"/\" ) if selector : selected_soup = beautsoup . select ( selector ) # if the custom selector matched, take the first occurrence if selected_soup : beautsoup = selected_soup [ 0 ] resources = beautsoup . find_all ([ \"a\" , \"img\" ]) for tag in resources : if tag . name == \"a\" : if tag . has_attr ( \"href\" ) and ( tag [ \"href\" ] . startswith ( \"/\" )): tag [ \"href\" ] = urljoin ( url , tag [ \"href\" ]) # check it's a normal link and not some sort of image # string returns the text content of the tag if not tag . string : # delete tag tag . decompose () elif tag . name == \"img\" and tag . has_attr ( \"src\" ): filename = tag [ \"src\" ] . split ( \"/\" )[ - 1 ] try : filename = filename [ : filename . index ( \"?\" ) ] # remove query parameters except ValueError : pass if not tag [ \"src\" ] . startswith ( \"http\" ): tag [ \"src\" ] = urljoin ( url , tag [ \"src\" ]) if current_app . config [ \"SCRAPING_CONF\" ][ \"save_images\" ] and valid_image_filename ( filename ): image = FileStorage ( BytesIO ( requests . get ( tag [ \"src\" ]) . content ), filename , name = \"file\" ) saved_to = save_image ( image ) tag [ \"src\" ] = \"/images/\" + saved_to res = html2text ( str ( beautsoup ), bodywidth = 0 ) return res","title":"extract_content()"},{"location":"reference/models/#archivy.models.DataObj.from_md","text":"Class method to generate new dataobj from a well formatted markdown string Call like this: Dataobj . from_md ( content ) Source code in archivy/models.py @classmethod def from_md ( cls , md_content : str ): \"\"\" Class method to generate new dataobj from a well formatted markdown string Call like this: ```python Dataobj.from_md(content) ``` \"\"\" data = frontmatter . loads ( md_content ) dataobj = {} dataobj [ \"content\" ] = data . content for pair in [ \"id\" , \"title\" , \"path\" , \"tags\" ]: try : dataobj [ pair ] = data [ pair ] except KeyError : # files sometimes get moved temporarily by applications while you edit # this can create bugs where the data is not loaded correctly # this handles that scenario as validation will simply fail and the event will # be ignored break dataobj [ \"type\" ] = \"processed-dataobj\" return cls ( ** dataobj )","title":"from_md()"},{"location":"reference/models/#archivy.models.DataObj.insert","text":"Creates a new file with the object's attributes Source code in archivy/models.py def insert ( self ): \"\"\"Creates a new file with the object's attributes\"\"\" if self . validate (): for tag in self . tags : add_tag_to_index ( tag ) helpers . set_max_id ( helpers . get_max_id () + 1 ) self . id = helpers . get_max_id () self . date = datetime . now () hooks = current_app . config [ \"HOOKS\" ] hooks . before_dataobj_create ( self ) data = { \"type\" : self . type , \"title\" : str ( self . title ), \"date\" : self . date . strftime ( \" %x \" ) . replace ( \"/\" , \"-\" ), \"tags\" : self . tags , \"id\" : self . id , \"path\" : self . path , } if self . type == \"bookmark\" or self . type == \"pocket_bookmark\" : data [ \"url\" ] = self . url # convert to markdown file dataobj = frontmatter . Post ( self . content ) dataobj . metadata = data self . fullpath = str ( create ( frontmatter . dumps ( dataobj ), f \" { self . id } - { dataobj [ 'title' ] } \" , path = self . path , ) ) hooks . on_dataobj_create ( self ) self . index () return self . id return False","title":"insert()"},{"location":"reference/models/#archivy.models.DataObj.process_bookmark_url","text":"Process url to get content for bookmark Source code in archivy/models.py def process_bookmark_url ( self , raw_html = None ): \"\"\"Process url to get content for bookmark\"\"\" if self . type not in ( \"bookmark\" , \"pocket_bookmark\" ) or not validators . url ( self . url ): return None selector = None for pattern , handler in current_app . config [ \"SCRAPING_PATTERNS\" ] . items (): if fnmatch . fnmatch ( self . url , pattern ): if type ( handler ) == str : # if the handler is a string, it's simply a css selector to process the page with selector = handler break # otherwise custom user function that overrides archivy behavior handler ( self ) return try : page_html = ( raw_html or requests . get ( self . url , headers = { \"User-agent\" : f \"Archivy/v { require ( 'archivy' )[ 0 ] . version } \" }, ) . text ) except Exception : self . error = f \"Could not retrieve { self . url } \\n \" self . wipe () return try : document = Document ( page_html ) self . title = document . short_title () or self . url parsed_html = BeautifulSoup ( document . summary (), features = \"html.parser\" ) except Exception : self . error = f \"Could not parse { self . url } \\n \" self . wipe () return try : self . content = self . extract_content ( parsed_html , selector ) except Exception : self . error = f \"Could not extract content from { self . url } \\n \" return","title":"process_bookmark_url()"},{"location":"reference/models/#archivy.models.DataObj.validate","text":"Verifies that the content matches required validation constraints Source code in archivy/models.py def validate ( self ): \"\"\"Verifies that the content matches required validation constraints\"\"\" valid_url = ( self . type != \"bookmark\" or self . type != \"pocket_bookmark\" ) or ( isinstance ( self . url , str ) and validators . url ( self . url ) ) valid_title = isinstance ( self . title , str ) and self . title != \"\" valid_content = self . type not in ( \"bookmark\" , \"pocket_bookmark\" ) or isinstance ( self . content , str ) return valid_url and valid_title and valid_content","title":"validate()"},{"location":"reference/models/#archivy.models.DataObj.wipe","text":"Resets and invalidates dataobj Source code in archivy/models.py def wipe ( self ): \"\"\"Resets and invalidates dataobj\"\"\" self . title = \"\" self . content = \"\"","title":"wipe()"},{"location":"reference/models/#archivy.models.User","text":"Model we use for User that inherits from flask login's UserMixin username password is_admin Source code in archivy/models.py class User ( UserMixin ): \"\"\" Model we use for User that inherits from flask login's [`UserMixin`](https://flask-login.readthedocs.io/en/latest/#flask_login.UserMixin) Attributes: - **username** - **password** - **is_admin** \"\"\" username : str = attrib ( validator = instance_of ( str )) password : Optional [ str ] = attrib ( validator = optional ( instance_of ( str )), default = None ) is_admin : Optional [ bool ] = attrib ( validator = optional ( instance_of ( bool )), default = None ) id : Optional [ int ] = attrib ( validator = optional ( instance_of ( int )), default = False ) def insert ( self ): \"\"\"Inserts the model from the database\"\"\" if not self . password : return False hashed_password = generate_password_hash ( self . password ) db = helpers . get_db () if db . search (( Query () . type == \"user\" ) & ( Query () . username == self . username )): return False db_user = { \"username\" : self . username , \"hashed_password\" : hashed_password , \"is_admin\" : self . is_admin , \"type\" : \"user\" , } current_app . config [ \"HOOKS\" ] . on_user_create ( self ) return db . insert ( db_user ) @classmethod def from_db ( cls , db_object ): \"\"\"Takes a database object and turns it into a user\"\"\" username = db_object [ \"username\" ] id = db_object . doc_id return cls ( username = username , id = id )","title":"User"},{"location":"reference/models/#archivy.models.User.from_db","text":"Takes a database object and turns it into a user Source code in archivy/models.py @classmethod def from_db ( cls , db_object ): \"\"\"Takes a database object and turns it into a user\"\"\" username = db_object [ \"username\" ] id = db_object . doc_id return cls ( username = username , id = id )","title":"from_db()"},{"location":"reference/models/#archivy.models.User.insert","text":"Inserts the model from the database Source code in archivy/models.py def insert ( self ): \"\"\"Inserts the model from the database\"\"\" if not self . password : return False hashed_password = generate_password_hash ( self . password ) db = helpers . get_db () if db . search (( Query () . type == \"user\" ) & ( Query () . username == self . username )): return False db_user = { \"username\" : self . username , \"hashed_password\" : hashed_password , \"is_admin\" : self . is_admin , \"type\" : \"user\" , } current_app . config [ \"HOOKS\" ] . on_user_create ( self ) return db . insert ( db_user )","title":"insert()"},{"location":"reference/search/","text":"These are a few methods to interface between archivy and the elasticsearch instance. add_to_index ( model ) Adds dataobj to given index. If object of given id already exists, it will be updated. index - String of the ES Index. Archivy uses dataobj by default. model - Instance of archivy.models.Dataobj , the object you want to index. Source code in archivy/search.py def add_to_index ( model ): \"\"\" Adds dataobj to given index. If object of given id already exists, it will be updated. Params: - **index** - String of the ES Index. Archivy uses `dataobj` by default. - **model** - Instance of `archivy.models.Dataobj`, the object you want to index. \"\"\" es = get_elastic_client () if not es : return payload = {} for field in model . __searchable__ : payload [ field ] = getattr ( model , field ) es . index ( index = current_app . config [ \"SEARCH_CONF\" ][ \"index_name\" ], id = model . id , body = payload ) return True parse_ripgrep_line ( line ) Parses a line of ripgrep JSON output Source code in archivy/search.py def parse_ripgrep_line ( line ): \"\"\"Parses a line of ripgrep JSON output\"\"\" hit = json . loads ( line ) data = {} if hit [ \"type\" ] == \"begin\" : curr_file = ( Path ( hit [ \"data\" ][ \"path\" ][ \"text\" ]) . parts [ - 1 ] . replace ( \".md\" , \"\" ) . split ( \"-\" ) ) # parse target note data from path curr_id = int ( curr_file [ 0 ]) title = curr_file [ - 1 ] . replace ( \"_\" , \" \" ) data = { \"title\" : title , \"matches\" : [], \"id\" : curr_id } elif hit [ \"type\" ] == \"match\" : data = hit [ \"data\" ][ \"lines\" ][ \"text\" ] . strip () else : return None # only process begin and match events, we don't care about endings return ( data , hit [ \"type\" ]) query_es_index ( query , strict = False ) Returns search results for your given query Specify strict=True if you want only exact result (in case you're using ES. Source code in archivy/search.py def query_es_index ( query , strict = False ): \"\"\" Returns search results for your given query Specify strict=True if you want only exact result (in case you're using ES. \"\"\" es = get_elastic_client () if not es : return [] search = es . search ( index = current_app . config [ \"SEARCH_CONF\" ][ \"index_name\" ], body = { \"query\" : { \"multi_match\" : { \"query\" : query , \"fields\" : [ \"*\" ], \"analyzer\" : \"rebuilt_standard\" , } }, \"highlight\" : { \"fragment_size\" : 0 , \"fields\" : { \"content\" : { \"pre_tags\" : \"\" , \"post_tags\" : \"\" , } }, }, }, ) hits = [] for hit in search [ \"hits\" ][ \"hits\" ]: formatted_hit = { \"id\" : hit [ \"_id\" ], \"title\" : hit [ \"_source\" ][ \"title\" ]} if \"highlight\" in hit : formatted_hit [ \"matches\" ] = hit [ \"highlight\" ][ \"content\" ] reformatted_match = \" \" . join ( formatted_hit [ \"matches\" ]) if strict and not ( query in reformatted_match ): continue hits . append ( formatted_hit ) return hits query_ripgrep ( query ) Uses ripgrep to search data with a simpler setup than ES. Returns a list of dicts with detailed matches. Source code in archivy/search.py def query_ripgrep ( query ): \"\"\" Uses ripgrep to search data with a simpler setup than ES. Returns a list of dicts with detailed matches. \"\"\" from archivy.data import get_data_dir if not which ( \"rg\" ): return [] rg_cmd = [ \"rg\" , RG_MISC_ARGS , RG_FILETYPE , \"--json\" , query , str ( get_data_dir ())] rg = run ( rg_cmd , stdout = PIPE , stderr = PIPE , timeout = 60 ) output = rg . stdout . decode () . splitlines () hits = [] for line in output : parsed = parse_ripgrep_line ( line ) if not parsed : continue if parsed [ 1 ] == \"begin\" : hits . append ( parsed [ 0 ]) if parsed [ 1 ] == \"match\" : if not ( parsed [ 0 ] . startswith ( \"tags: [\" ) or parsed [ 0 ] . startswith ( \"title:\" )): hits [ - 1 ][ \"matches\" ] . append ( parsed [ 0 ]) return sorted ( hits , key = lambda x : len ( x [ \"matches\" ]), reverse = True ) # sort by number of matches query_ripgrep_tags () Uses ripgrep to search for tags. Mandatory reference: https://xkcd.com/1171/ Source code in archivy/search.py def query_ripgrep_tags (): \"\"\" Uses ripgrep to search for tags. Mandatory reference: https://xkcd.com/1171/ \"\"\" EMB_PATTERN = r \"(^|\\n| )#([-_a-zA-Z\u00c0-\u00d6\u00d8-\u00f6\u00f8-\u00ff0-9]+)#\" from archivy.data import get_data_dir if not which ( \"rg\" ): return [] # embedded tags # io: case insensitive rg_cmd = [ \"rg\" , \"-Uio\" , RG_FILETYPE , RG_REGEX_ARG , EMB_PATTERN , str ( get_data_dir ())] rg = run ( rg_cmd , stdout = PIPE , stderr = PIPE , timeout = 60 ) hits = set () for line in rg . stdout . splitlines (): tag = Path ( line . decode ()) . parts [ - 1 ] . split ( \":\" )[ - 1 ] tag = tag . replace ( \"#\" , \"\" ) . lstrip () hits . add ( tag ) # metadata tags for item in search_frontmatter_tags (): for tag in item [ \"tags\" ]: hits . add ( tag ) return hits remove_from_index ( dataobj_id ) Removes object of given id Source code in archivy/search.py def remove_from_index ( dataobj_id ): \"\"\"Removes object of given id\"\"\" es = get_elastic_client () if not es : return es . delete ( index = current_app . config [ \"SEARCH_CONF\" ][ \"index_name\" ], id = dataobj_id ) search ( query , strict = False ) Wrapper to search methods for different engines. If using ES, specify strict=True if you only want results that strictly match the query, without parsing / tokenization. Source code in archivy/search.py def search ( query , strict = False ): \"\"\" Wrapper to search methods for different engines. If using ES, specify strict=True if you only want results that strictly match the query, without parsing / tokenization. \"\"\" if current_app . config [ \"SEARCH_CONF\" ][ \"engine\" ] == \"elasticsearch\" : return query_es_index ( query , strict = strict ) elif current_app . config [ \"SEARCH_CONF\" ][ \"engine\" ] == \"ripgrep\" or which ( \"rg\" ): return query_ripgrep ( query ) search_frontmatter_tags ( tag = None ) Returns a list of dataobj ids that have the given tag. Source code in archivy/search.py def search_frontmatter_tags ( tag = None ): \"\"\" Returns a list of dataobj ids that have the given tag. \"\"\" from archivy.data import get_data_dir if not which ( \"rg\" ): return [] META_PATTERN = r \"(^|\\n)tags:(\\n- [_a-zA-Z\u00c0-\u00d6\u00d8-\u00f6\u00f8-\u00ff0-9]+)+\" hits = [] rg_cmd = [ \"rg\" , \"-Uo\" , RG_MISC_ARGS , RG_FILETYPE , \"--json\" , RG_REGEX_ARG , META_PATTERN , str ( get_data_dir ()), ] rg = run ( rg_cmd , stdout = PIPE , stderr = PIPE , timeout = 60 ) output = rg . stdout . decode () . splitlines () for line in output : parsed = parse_ripgrep_line ( line ) if not parsed : # the event doesn't interest us continue if parsed [ 1 ] == \"begin\" : hits . append ( parsed [ 0 ]) # append current hit data if parsed [ 1 ] == \"match\" : sanitized = parsed [ 0 ] . replace ( \"- \" , \"\" ) . split ( \" \\n \" )[ 2 :] hits [ - 1 ][ \"tags\" ] = hits [ - 1 ] . get ( \"tags\" , []) + sanitized # get tags if tag : hits = list ( filter ( lambda x : tag in x [ \"tags\" ], hits )) return hits","title":"Search"},{"location":"reference/search/#archivy.search.add_to_index","text":"Adds dataobj to given index. If object of given id already exists, it will be updated. index - String of the ES Index. Archivy uses dataobj by default. model - Instance of archivy.models.Dataobj , the object you want to index. Source code in archivy/search.py def add_to_index ( model ): \"\"\" Adds dataobj to given index. If object of given id already exists, it will be updated. Params: - **index** - String of the ES Index. Archivy uses `dataobj` by default. - **model** - Instance of `archivy.models.Dataobj`, the object you want to index. \"\"\" es = get_elastic_client () if not es : return payload = {} for field in model . __searchable__ : payload [ field ] = getattr ( model , field ) es . index ( index = current_app . config [ \"SEARCH_CONF\" ][ \"index_name\" ], id = model . id , body = payload ) return True","title":"add_to_index()"},{"location":"reference/search/#archivy.search.parse_ripgrep_line","text":"Parses a line of ripgrep JSON output Source code in archivy/search.py def parse_ripgrep_line ( line ): \"\"\"Parses a line of ripgrep JSON output\"\"\" hit = json . loads ( line ) data = {} if hit [ \"type\" ] == \"begin\" : curr_file = ( Path ( hit [ \"data\" ][ \"path\" ][ \"text\" ]) . parts [ - 1 ] . replace ( \".md\" , \"\" ) . split ( \"-\" ) ) # parse target note data from path curr_id = int ( curr_file [ 0 ]) title = curr_file [ - 1 ] . replace ( \"_\" , \" \" ) data = { \"title\" : title , \"matches\" : [], \"id\" : curr_id } elif hit [ \"type\" ] == \"match\" : data = hit [ \"data\" ][ \"lines\" ][ \"text\" ] . strip () else : return None # only process begin and match events, we don't care about endings return ( data , hit [ \"type\" ])","title":"parse_ripgrep_line()"},{"location":"reference/search/#archivy.search.query_es_index","text":"Returns search results for your given query Specify strict=True if you want only exact result (in case you're using ES. Source code in archivy/search.py def query_es_index ( query , strict = False ): \"\"\" Returns search results for your given query Specify strict=True if you want only exact result (in case you're using ES. \"\"\" es = get_elastic_client () if not es : return [] search = es . search ( index = current_app . config [ \"SEARCH_CONF\" ][ \"index_name\" ], body = { \"query\" : { \"multi_match\" : { \"query\" : query , \"fields\" : [ \"*\" ], \"analyzer\" : \"rebuilt_standard\" , } }, \"highlight\" : { \"fragment_size\" : 0 , \"fields\" : { \"content\" : { \"pre_tags\" : \"\" , \"post_tags\" : \"\" , } }, }, }, ) hits = [] for hit in search [ \"hits\" ][ \"hits\" ]: formatted_hit = { \"id\" : hit [ \"_id\" ], \"title\" : hit [ \"_source\" ][ \"title\" ]} if \"highlight\" in hit : formatted_hit [ \"matches\" ] = hit [ \"highlight\" ][ \"content\" ] reformatted_match = \" \" . join ( formatted_hit [ \"matches\" ]) if strict and not ( query in reformatted_match ): continue hits . append ( formatted_hit ) return hits","title":"query_es_index()"},{"location":"reference/search/#archivy.search.query_ripgrep","text":"Uses ripgrep to search data with a simpler setup than ES. Returns a list of dicts with detailed matches. Source code in archivy/search.py def query_ripgrep ( query ): \"\"\" Uses ripgrep to search data with a simpler setup than ES. Returns a list of dicts with detailed matches. \"\"\" from archivy.data import get_data_dir if not which ( \"rg\" ): return [] rg_cmd = [ \"rg\" , RG_MISC_ARGS , RG_FILETYPE , \"--json\" , query , str ( get_data_dir ())] rg = run ( rg_cmd , stdout = PIPE , stderr = PIPE , timeout = 60 ) output = rg . stdout . decode () . splitlines () hits = [] for line in output : parsed = parse_ripgrep_line ( line ) if not parsed : continue if parsed [ 1 ] == \"begin\" : hits . append ( parsed [ 0 ]) if parsed [ 1 ] == \"match\" : if not ( parsed [ 0 ] . startswith ( \"tags: [\" ) or parsed [ 0 ] . startswith ( \"title:\" )): hits [ - 1 ][ \"matches\" ] . append ( parsed [ 0 ]) return sorted ( hits , key = lambda x : len ( x [ \"matches\" ]), reverse = True ) # sort by number of matches","title":"query_ripgrep()"},{"location":"reference/search/#archivy.search.query_ripgrep_tags","text":"Uses ripgrep to search for tags. Mandatory reference: https://xkcd.com/1171/ Source code in archivy/search.py def query_ripgrep_tags (): \"\"\" Uses ripgrep to search for tags. Mandatory reference: https://xkcd.com/1171/ \"\"\" EMB_PATTERN = r \"(^|\\n| )#([-_a-zA-Z\u00c0-\u00d6\u00d8-\u00f6\u00f8-\u00ff0-9]+)#\" from archivy.data import get_data_dir if not which ( \"rg\" ): return [] # embedded tags # io: case insensitive rg_cmd = [ \"rg\" , \"-Uio\" , RG_FILETYPE , RG_REGEX_ARG , EMB_PATTERN , str ( get_data_dir ())] rg = run ( rg_cmd , stdout = PIPE , stderr = PIPE , timeout = 60 ) hits = set () for line in rg . stdout . splitlines (): tag = Path ( line . decode ()) . parts [ - 1 ] . split ( \":\" )[ - 1 ] tag = tag . replace ( \"#\" , \"\" ) . lstrip () hits . add ( tag ) # metadata tags for item in search_frontmatter_tags (): for tag in item [ \"tags\" ]: hits . add ( tag ) return hits","title":"query_ripgrep_tags()"},{"location":"reference/search/#archivy.search.remove_from_index","text":"Removes object of given id Source code in archivy/search.py def remove_from_index ( dataobj_id ): \"\"\"Removes object of given id\"\"\" es = get_elastic_client () if not es : return es . delete ( index = current_app . config [ \"SEARCH_CONF\" ][ \"index_name\" ], id = dataobj_id )","title":"remove_from_index()"},{"location":"reference/search/#archivy.search.search","text":"Wrapper to search methods for different engines. If using ES, specify strict=True if you only want results that strictly match the query, without parsing / tokenization. Source code in archivy/search.py def search ( query , strict = False ): \"\"\" Wrapper to search methods for different engines. If using ES, specify strict=True if you only want results that strictly match the query, without parsing / tokenization. \"\"\" if current_app . config [ \"SEARCH_CONF\" ][ \"engine\" ] == \"elasticsearch\" : return query_es_index ( query , strict = strict ) elif current_app . config [ \"SEARCH_CONF\" ][ \"engine\" ] == \"ripgrep\" or which ( \"rg\" ): return query_ripgrep ( query )","title":"search()"},{"location":"reference/search/#archivy.search.search_frontmatter_tags","text":"Returns a list of dataobj ids that have the given tag. Source code in archivy/search.py def search_frontmatter_tags ( tag = None ): \"\"\" Returns a list of dataobj ids that have the given tag. \"\"\" from archivy.data import get_data_dir if not which ( \"rg\" ): return [] META_PATTERN = r \"(^|\\n)tags:(\\n- [_a-zA-Z\u00c0-\u00d6\u00d8-\u00f6\u00f8-\u00ff0-9]+)+\" hits = [] rg_cmd = [ \"rg\" , \"-Uo\" , RG_MISC_ARGS , RG_FILETYPE , \"--json\" , RG_REGEX_ARG , META_PATTERN , str ( get_data_dir ()), ] rg = run ( rg_cmd , stdout = PIPE , stderr = PIPE , timeout = 60 ) output = rg . stdout . decode () . splitlines () for line in output : parsed = parse_ripgrep_line ( line ) if not parsed : # the event doesn't interest us continue if parsed [ 1 ] == \"begin\" : hits . append ( parsed [ 0 ]) # append current hit data if parsed [ 1 ] == \"match\" : sanitized = parsed [ 0 ] . replace ( \"- \" , \"\" ) . split ( \" \\n \" )[ 2 :] hits [ - 1 ][ \"tags\" ] = hits [ - 1 ] . get ( \"tags\" , []) + sanitized # get tags if tag : hits = list ( filter ( lambda x : tag in x [ \"tags\" ], hits )) return hits","title":"search_frontmatter_tags()"},{"location":"reference/web_api/","text":"The Archivy HTTP API allows you to run small scripts in any language that will interact with your archivy instance through HTTP. All calls must be first logged in with the login endpoint. Small example in Python This code uses the requests module to interact with the API: import requests # we create a new session that will allow us to login once s = requests . session () INSTANCE_URL = < your instance url > s . post ( f \" { INSTANCE_URL } /api/login\" , auth = ( < username > , < password > )) # once you've logged in - you can make authenticated requests to the api, like: resp = s . get ( f \" { INSTANCE_URL } /api/dataobjs\" ) . content ) Reference add_tag_to_index () Add a tag to the database. Source code in archivy/api.py @api_bp . route ( \"/tags/add_to_index\" , methods = [ \"PUT\" ]) def add_tag_to_index (): \"\"\"Add a tag to the database.\"\"\" tag = request . json . get ( \"tag\" , False ) if tag and type ( tag ) is str and tags . is_tag_format ( tag ): if tags . add_tag_to_index ( tag ): return Response ( status = 200 ) else : return Response ( status = 404 ) return Response ( \"Must provide valid tag name.\" , status = 401 ) create_bookmark () Creates a new bookmark Parameters: All parameters are sent through the JSON body. - url (required) - tags - path Source code in archivy/api.py @api_bp . route ( \"/bookmarks\" , methods = [ \"POST\" ]) def create_bookmark (): \"\"\" Creates a new bookmark **Parameters:** All parameters are sent through the JSON body. - **url** (required) - **tags** - **path** \"\"\" json_data = request . get_json () bookmark = DataObj ( url = json_data [ \"url\" ], tags = json_data . get ( \"tags\" , []), path = json_data . get ( \"path\" , current_app . config [ \"DEFAULT_BOOKMARKS_DIR\" ]), type = \"bookmark\" , ) bookmark . process_bookmark_url () bookmark_id = bookmark . insert () if bookmark_id : return jsonify ( bookmark_id = bookmark_id , ) return Response ( status = 400 ) create_folder () Creates new directory Parameter in JSON body: - path (required) - path of newdir Source code in archivy/api.py @api_bp . route ( \"/folders/new\" , methods = [ \"POST\" ]) def create_folder (): \"\"\" Creates new directory Parameter in JSON body: - **path** (required) - path of newdir \"\"\" directory = request . json . get ( \"path\" ) try : sanitized_name = data . create_dir ( directory ) if not sanitized_name : return Response ( \"Invalid dirname\" , status = 400 ) except FileExistsError : return Response ( \"Directory already exists\" , status = 400 ) return Response ( sanitized_name , status = 200 ) create_note () Creates a new note. Parameters: All parameters are sent through the JSON body. - title (required) - content (required) - tags - path Source code in archivy/api.py @api_bp . route ( \"/notes\" , methods = [ \"POST\" ]) def create_note (): \"\"\" Creates a new note. **Parameters:** All parameters are sent through the JSON body. - **title** (required) - **content** (required) - **tags** - **path** \"\"\" json_data = request . get_json () note = DataObj ( title = json_data [ \"title\" ], content = json_data [ \"content\" ], path = json_data . get ( \"path\" , \"\" ), tags = json_data . get ( \"tags\" , []), type = \"note\" , ) note_id = note . insert () if note_id : return jsonify ( note_id = note_id ) return Response ( status = 400 ) delete_dataobj ( dataobj_id ) Deletes object of given id Source code in archivy/api.py @api_bp . route ( \"/dataobjs/<int:dataobj_id>\" , methods = [ \"DELETE\" ]) def delete_dataobj ( dataobj_id ): \"\"\"Deletes object of given id\"\"\" if not data . get_item ( dataobj_id ): return Response ( status = 404 ) data . delete_item ( dataobj_id ) return Response ( status = 204 ) delete_folder () Deletes directory. Parameter in JSON body: - path of dir to delete Source code in archivy/api.py @api_bp . route ( \"/folders/delete\" , methods = [ \"DELETE\" ]) def delete_folder (): \"\"\" Deletes directory. Parameter in JSON body: - **path** of dir to delete \"\"\" directory = request . json . get ( \"path\" ) if directory == \"\" : return Response ( \"Cannot delete root dir\" , status = 401 ) if data . delete_dir ( directory ): return Response ( \"Successfully deleted\" , status = 200 ) return Response ( \"Could not delete directory\" , status = 400 ) get_dataobj ( dataobj_id ) Returns dataobj of given id Source code in archivy/api.py @api_bp . route ( \"/dataobjs/<int:dataobj_id>\" ) def get_dataobj ( dataobj_id ): \"\"\"Returns dataobj of given id\"\"\" dataobj = data . get_item ( dataobj_id ) return ( jsonify ( dataobj_id = dataobj_id , title = dataobj [ \"title\" ], content = dataobj . content , md_path = dataobj [ \"fullpath\" ], ) if dataobj else Response ( status = 404 ) ) get_dataobjs () Gets all dataobjs Source code in archivy/api.py @api_bp . route ( \"/dataobjs\" , methods = [ \"GET\" ]) def get_dataobjs (): \"\"\"Gets all dataobjs\"\"\" cur_dir = data . get_items ( structured = False , json_format = True ) return jsonify ( cur_dir ) login () Logs in the API client using HTTP Basic Auth . Pass in the username and password of your account. Source code in archivy/api.py @api_bp . route ( \"/login\" , methods = [ \"POST\" ]) def login (): \"\"\" Logs in the API client using [HTTP Basic Auth](https://en.wikipedia.org/wiki/Basic_access_authentication). Pass in the username and password of your account. \"\"\" db = get_db () user = db . search ( Query () . username == request . authorization [ \"username\" ]) if user and check_password_hash ( user [ 0 ][ \"hashed_password\" ], request . authorization [ \"password\" ] ): # user is verified so we can log him in from the db user = User . from_db ( user [ 0 ]) login_user ( user , remember = True ) return Response ( status = 200 ) return Response ( status = 401 ) search_endpoint () Searches the instance. Request URL Parameter: - query Source code in archivy/api.py @api_bp . route ( \"/search\" , methods = [ \"GET\" ]) def search_endpoint (): \"\"\" Searches the instance. Request URL Parameter: - **query** \"\"\" if not current_app . config [ \"SEARCH_CONF\" ][ \"enabled\" ]: return Response ( \"Search is disabled\" , status = 401 ) query = request . args . get ( \"query\" ) search_results = search ( query ) return jsonify ( search_results ) update_dataobj ( dataobj_id ) Updates object of given id. Paramter in JSON body: content : markdown text of new dataobj. Source code in archivy/api.py @api_bp . route ( \"/dataobjs/<int:dataobj_id>\" , methods = [ \"PUT\" ]) def update_dataobj ( dataobj_id ): \"\"\" Updates object of given id. Paramter in JSON body: - **content**: markdown text of new dataobj. \"\"\" if request . json . get ( \"content\" ): try : data . update_item_md ( dataobj_id , request . json . get ( \"content\" )) return Response ( status = 200 ) except BaseException : return Response ( status = 404 ) return Response ( \"Must provide content parameter\" , status = 401 ) update_dataobj_frontmatter ( dataobj_id ) Updates frontmatter of object of given id. Paramter in JSON body: title : the new title of the dataobj. Source code in archivy/api.py @api_bp . route ( \"/dataobjs/frontmatter/<int:dataobj_id>\" , methods = [ \"PUT\" ]) def update_dataobj_frontmatter ( dataobj_id ): \"\"\" Updates frontmatter of object of given id. Paramter in JSON body: - **title**: the new title of the dataobj. \"\"\" new_frontmatter = { \"title\" : request . json . get ( \"title\" ), } try : data . update_item_frontmatter ( dataobj_id , new_frontmatter ) return Response ( status = 200 ) except BaseException : return Response ( status = 404 )","title":"Web API"},{"location":"reference/web_api/#small-example-in-python","text":"This code uses the requests module to interact with the API: import requests # we create a new session that will allow us to login once s = requests . session () INSTANCE_URL = < your instance url > s . post ( f \" { INSTANCE_URL } /api/login\" , auth = ( < username > , < password > )) # once you've logged in - you can make authenticated requests to the api, like: resp = s . get ( f \" { INSTANCE_URL } /api/dataobjs\" ) . content )","title":"Small example in Python"},{"location":"reference/web_api/#reference","text":"","title":"Reference"},{"location":"reference/web_api/#archivy.api.add_tag_to_index","text":"Add a tag to the database. Source code in archivy/api.py @api_bp . route ( \"/tags/add_to_index\" , methods = [ \"PUT\" ]) def add_tag_to_index (): \"\"\"Add a tag to the database.\"\"\" tag = request . json . get ( \"tag\" , False ) if tag and type ( tag ) is str and tags . is_tag_format ( tag ): if tags . add_tag_to_index ( tag ): return Response ( status = 200 ) else : return Response ( status = 404 ) return Response ( \"Must provide valid tag name.\" , status = 401 )","title":"add_tag_to_index()"},{"location":"reference/web_api/#archivy.api.create_bookmark","text":"Creates a new bookmark Parameters: All parameters are sent through the JSON body. - url (required) - tags - path Source code in archivy/api.py @api_bp . route ( \"/bookmarks\" , methods = [ \"POST\" ]) def create_bookmark (): \"\"\" Creates a new bookmark **Parameters:** All parameters are sent through the JSON body. - **url** (required) - **tags** - **path** \"\"\" json_data = request . get_json () bookmark = DataObj ( url = json_data [ \"url\" ], tags = json_data . get ( \"tags\" , []), path = json_data . get ( \"path\" , current_app . config [ \"DEFAULT_BOOKMARKS_DIR\" ]), type = \"bookmark\" , ) bookmark . process_bookmark_url () bookmark_id = bookmark . insert () if bookmark_id : return jsonify ( bookmark_id = bookmark_id , ) return Response ( status = 400 )","title":"create_bookmark()"},{"location":"reference/web_api/#archivy.api.create_folder","text":"Creates new directory Parameter in JSON body: - path (required) - path of newdir Source code in archivy/api.py @api_bp . route ( \"/folders/new\" , methods = [ \"POST\" ]) def create_folder (): \"\"\" Creates new directory Parameter in JSON body: - **path** (required) - path of newdir \"\"\" directory = request . json . get ( \"path\" ) try : sanitized_name = data . create_dir ( directory ) if not sanitized_name : return Response ( \"Invalid dirname\" , status = 400 ) except FileExistsError : return Response ( \"Directory already exists\" , status = 400 ) return Response ( sanitized_name , status = 200 )","title":"create_folder()"},{"location":"reference/web_api/#archivy.api.create_note","text":"Creates a new note. Parameters: All parameters are sent through the JSON body. - title (required) - content (required) - tags - path Source code in archivy/api.py @api_bp . route ( \"/notes\" , methods = [ \"POST\" ]) def create_note (): \"\"\" Creates a new note. **Parameters:** All parameters are sent through the JSON body. - **title** (required) - **content** (required) - **tags** - **path** \"\"\" json_data = request . get_json () note = DataObj ( title = json_data [ \"title\" ], content = json_data [ \"content\" ], path = json_data . get ( \"path\" , \"\" ), tags = json_data . get ( \"tags\" , []), type = \"note\" , ) note_id = note . insert () if note_id : return jsonify ( note_id = note_id ) return Response ( status = 400 )","title":"create_note()"},{"location":"reference/web_api/#archivy.api.delete_dataobj","text":"Deletes object of given id Source code in archivy/api.py @api_bp . route ( \"/dataobjs/<int:dataobj_id>\" , methods = [ \"DELETE\" ]) def delete_dataobj ( dataobj_id ): \"\"\"Deletes object of given id\"\"\" if not data . get_item ( dataobj_id ): return Response ( status = 404 ) data . delete_item ( dataobj_id ) return Response ( status = 204 )","title":"delete_dataobj()"},{"location":"reference/web_api/#archivy.api.delete_folder","text":"Deletes directory. Parameter in JSON body: - path of dir to delete Source code in archivy/api.py @api_bp . route ( \"/folders/delete\" , methods = [ \"DELETE\" ]) def delete_folder (): \"\"\" Deletes directory. Parameter in JSON body: - **path** of dir to delete \"\"\" directory = request . json . get ( \"path\" ) if directory == \"\" : return Response ( \"Cannot delete root dir\" , status = 401 ) if data . delete_dir ( directory ): return Response ( \"Successfully deleted\" , status = 200 ) return Response ( \"Could not delete directory\" , status = 400 )","title":"delete_folder()"},{"location":"reference/web_api/#archivy.api.get_dataobj","text":"Returns dataobj of given id Source code in archivy/api.py @api_bp . route ( \"/dataobjs/<int:dataobj_id>\" ) def get_dataobj ( dataobj_id ): \"\"\"Returns dataobj of given id\"\"\" dataobj = data . get_item ( dataobj_id ) return ( jsonify ( dataobj_id = dataobj_id , title = dataobj [ \"title\" ], content = dataobj . content , md_path = dataobj [ \"fullpath\" ], ) if dataobj else Response ( status = 404 ) )","title":"get_dataobj()"},{"location":"reference/web_api/#archivy.api.get_dataobjs","text":"Gets all dataobjs Source code in archivy/api.py @api_bp . route ( \"/dataobjs\" , methods = [ \"GET\" ]) def get_dataobjs (): \"\"\"Gets all dataobjs\"\"\" cur_dir = data . get_items ( structured = False , json_format = True ) return jsonify ( cur_dir )","title":"get_dataobjs()"},{"location":"reference/web_api/#archivy.api.login","text":"Logs in the API client using HTTP Basic Auth . Pass in the username and password of your account. Source code in archivy/api.py @api_bp . route ( \"/login\" , methods = [ \"POST\" ]) def login (): \"\"\" Logs in the API client using [HTTP Basic Auth](https://en.wikipedia.org/wiki/Basic_access_authentication). Pass in the username and password of your account. \"\"\" db = get_db () user = db . search ( Query () . username == request . authorization [ \"username\" ]) if user and check_password_hash ( user [ 0 ][ \"hashed_password\" ], request . authorization [ \"password\" ] ): # user is verified so we can log him in from the db user = User . from_db ( user [ 0 ]) login_user ( user , remember = True ) return Response ( status = 200 ) return Response ( status = 401 )","title":"login()"},{"location":"reference/web_api/#archivy.api.search_endpoint","text":"Searches the instance. Request URL Parameter: - query Source code in archivy/api.py @api_bp . route ( \"/search\" , methods = [ \"GET\" ]) def search_endpoint (): \"\"\" Searches the instance. Request URL Parameter: - **query** \"\"\" if not current_app . config [ \"SEARCH_CONF\" ][ \"enabled\" ]: return Response ( \"Search is disabled\" , status = 401 ) query = request . args . get ( \"query\" ) search_results = search ( query ) return jsonify ( search_results )","title":"search_endpoint()"},{"location":"reference/web_api/#archivy.api.update_dataobj","text":"Updates object of given id. Paramter in JSON body: content : markdown text of new dataobj. Source code in archivy/api.py @api_bp . route ( \"/dataobjs/<int:dataobj_id>\" , methods = [ \"PUT\" ]) def update_dataobj ( dataobj_id ): \"\"\" Updates object of given id. Paramter in JSON body: - **content**: markdown text of new dataobj. \"\"\" if request . json . get ( \"content\" ): try : data . update_item_md ( dataobj_id , request . json . get ( \"content\" )) return Response ( status = 200 ) except BaseException : return Response ( status = 404 ) return Response ( \"Must provide content parameter\" , status = 401 )","title":"update_dataobj()"},{"location":"reference/web_api/#archivy.api.update_dataobj_frontmatter","text":"Updates frontmatter of object of given id. Paramter in JSON body: title : the new title of the dataobj. Source code in archivy/api.py @api_bp . route ( \"/dataobjs/frontmatter/<int:dataobj_id>\" , methods = [ \"PUT\" ]) def update_dataobj_frontmatter ( dataobj_id ): \"\"\" Updates frontmatter of object of given id. Paramter in JSON body: - **title**: the new title of the dataobj. \"\"\" new_frontmatter = { \"title\" : request . json . get ( \"title\" ), } try : data . update_item_frontmatter ( dataobj_id , new_frontmatter ) return Response ( status = 200 ) except BaseException : return Response ( status = 404 )","title":"update_dataobj_frontmatter()"},{"location":"reference/web_inputs/","text":"When developing plugins, you may want to use custom HTML input types on the frontend, like email or password . Archivy currently allows you use these two types in your click options. For example: from archivy.click_web.web_click_types import EMAIL_TYPE , PASSWORD_TYPE @cli . command () @click . option ( \"--the_email\" , type = EMAIL_TYPE ) # this will validate the email format on the frontend and backend @click . option ( \"--password\" , type = PASSWORD_TYPE ) # type='password' on the HTML frontend. def login ( the_email , password ): ...","title":"Web Inputs For Plugins"}]}