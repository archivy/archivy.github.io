{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Logo design by Roy Quilor is licensed under CC BY-NC 4.0 Archivy Archivy is a self-hosted knowledge repository that allows you to safely preserve useful content that contributes to your knowledge bank. Features: If you add bookmarks, their webpages contents' will be saved to ensure that you will always have access to it, following the idea of digital preservation . Login module that allows you to host the service on a server Plugin system to allow people to publish and write extensions to archivy CLI that provides a nice backend interface to the app Backend API for flexibility and user enhancements Everything is a file! For ease of access and editing, all the content is stored in markdown files with yaml front matter. Extensible search with Elasticsearch and its Query DSL Dark Theme Upcoming: Links between different knowledge base items Multi User System. Option to compile data to a static site. Quickstart Begin by installing Pandoc , the document converter archivy uses. Install with pip install archivy . Then run this and enter a password to create a new user: $ archivy create-admin <username> Finally, execute archivy run to serve the app. You can open it at https://localhost:5000 and login with the credentials you entered before. You can then use archivy to create notes, bookmarks and then organize and store information.","title":"Index"},{"location":"#archivy","text":"Archivy is a self-hosted knowledge repository that allows you to safely preserve useful content that contributes to your knowledge bank. Features: If you add bookmarks, their webpages contents' will be saved to ensure that you will always have access to it, following the idea of digital preservation . Login module that allows you to host the service on a server Plugin system to allow people to publish and write extensions to archivy CLI that provides a nice backend interface to the app Backend API for flexibility and user enhancements Everything is a file! For ease of access and editing, all the content is stored in markdown files with yaml front matter. Extensible search with Elasticsearch and its Query DSL Dark Theme Upcoming: Links between different knowledge base items Multi User System. Option to compile data to a static site.","title":"Archivy"},{"location":"#quickstart","text":"Begin by installing Pandoc , the document converter archivy uses. Install with pip install archivy . Then run this and enter a password to create a new user: $ archivy create-admin <username> Finally, execute archivy run to serve the app. You can open it at https://localhost:5000 and login with the credentials you entered before. You can then use archivy to create notes, bookmarks and then organize and store information.","title":"Quickstart"},{"location":"config/","text":"Archivy uses environment variables for its configuration (we are hesitating on this and might switch to a yaml file): Variable Default Description ARCHIVY_DATA_DIR System-dependent, see below Directory in which data will be saved ARCHIVY_PORT 5000 Port on which archivy will run ELASTICSEARCH_ENABLED 0 Enable Elasticsearch integration ELASTICSEARCH_URL http://localhost:9200 Url to the elasticsearch server ARCHIVY_DATA_DIR by default will be set by the appdirs python library: On Linux systems, it follows the XDG specification : ~/.local/share/archivy","title":"Config"},{"location":"difference/","text":"There are many great tools out there to create your knowledge base. So why should you use Archivy? Here are the ingredients that make Archivy stand out (of course many tools have other interesting components / focuses, and you should pick the one that resonates with what you want): Focus on scripting and extensibility : When I began developing archivy, I had plans for developing in the app's core additional features that would allow you to sync up to your digital presence, for exemple a Reddit extension that would download your upvoted posts, etc... I quickly realised that this would be a bad way to go, as many people often want maybe one feature, but not a ton of useless included extensions. That's when I decided to instead build a flexible framework for people to build installable plugins , as this allows a) users only download what they want and b) Archivy can focus on its core and users can build their own extensions for themselves and others. Importance of Digital Preservation : ^ I mentioned above the idea of a plugin for saving all your upvoted reddit posts. This is just an example of how Archivy is intended to be used not only as a knowledge base, but also a resilient stronghold for the data that used to be solely held by third-party services. This idea of automatically syncing and saving content you've found valuable could be expanded to HN upvoted posts, browser bookmarks, etc... 1 deployable OR you can just run it on your computer : The way archivy was engineered makes it possible for you to just run it on your laptop or pc, and still use it without problems. On the other hand, it is also very much a possibility to self-host it and expose it publicly for use from anywhere, which is why archivy has auth, and is also the motivator for our plans on adding a web editor . Powerful Search : Elasticsearch might be considered overkill for one's knowledge base, but as your knowledge base grows also with content from other data sources and automation, it can become a large amount of data. Elasticsearch provides very high quality search on this information at a swift speed. We still plan on adding an alternative search system that users can choose to use if they don't want to run ES. 2 https://beepb00p.xyz/hpi.html is an intriguing tool on this topic. \u21a9 See this thread if you have any tools in mind for this. \u21a9","title":"What makes Archivy different"},{"location":"docker/","text":"Guide to using Archivy with Docker This document contains enough information to help you get started with using Archivy as a container, in this case, with Docker(although you can use any other container runtime). This document will cover the following: Building a container image of Archivy Running Archivy as a container Quick start Running Archivy along with persistent data storage Running Archivy with environment variable injection Demo on \u2018 Play With Docker \u2019 Running Archivy using Docker Compose Running Archivy with Elasticsearch for full text search capabilities(with Docker Compose) Planned for the future: Running Archivy using Docker Swarm(container orchestrator) Running Archivy on Kubernetes as a production-ready setup Running Archivy on OpenStack as a production-ready setup(will be done only after the aforementioned points are completed) NOTE : Parts of the document may be incomplete as it is a work in progress. In time, more information will be added to each section/topic. If some part of the documentation is ambiguous, feel free to ask questions or make suggestions on the Issues page of the project. If necessary, additional revisions to the documentation can be made based on user feedback. Prerequisites Docker needs to be installed. You can check if Docker is installed by running $ docker --version Docker version 19 .03.12, build 48a66213fe If you don't have Docker installed, take a look at the official installation guide for your device. Building Archivy Building using Dockerfile The file Dockerfile is much more portable than the local-build.Dockerfile as you do not need any other additional files to build the image. The Dockerfile automatically downloads the source code during the build stage. Just download Dockerfile and run the following command to build the image: $ docker build -t archivy:1.0 -f /path/to/Dockerfile . This tags the image with the name archivy:1.0 . Technically, you do not need to mention the path to Dockerfile as long as the Dockerfile is titled Dockerfile . If the name is anything but that, you will need to mention the path using the -f flag. So the above command is the same as $ docker build -t archivy:1.0 . There's an easier way to build the image. You can pass the URL to the GitHub repository directly to docker , and as long as there\u2019s a file named Dockerfile in the root of the repository, Docker will build it. $ docker build -t archivy:1.0 https://github.com/Uzay-G/archivy.git#docker This will clone the GitHub repository and use the cloned repository as context. The Dockerfile at the root of the repository is used as Dockerfile . NOTE : The master branch of the Archivy repository does not contain any Dockerfiles. All files pertaining to Docker are available in the docker branch of this repository. Running Archivy Quick Start You can get an instance of Archivy up and running for testing purposes with the following one-liner: $ docker run -d --name archivy-test -p 5000 :5000 harshavardhanj/archivy --name \u2014\u2014 Name of the container(can be anything) -p/--publish host:container \u2014\u2014 Bind port on host to port on container The above command runs a container with the name archivy-test and binds port 5000 on the host to port 5000 on the container. That way, you can access the server at http://localhost:5000/ on the device. If you wish to access the server at http://localhost:9000/ instead, you would change the argument to -p 9000:5000 . This container runs in detached mode, meaning that no output is printed to the terminal. To view the logs, run $ docker logs -f [ name-of-container ] which will continuously stream the logs to the terminal. To exit, type ctrl+c . To just print the logs and exit, run the same command without the -f flag. NOTE : There\u2019s an image available on DockerHub for testing purposes. It is titled archivy and is available in the harshavardhanj repository on DockerHub. You can pull the image using $ docker pull harshavardhanj/archivy Running container in interactive mode You can also pass commands to the container by appending it to the docker run command. Keep in mind that the container will execute your commands and not run Archivy. This is useful if you want to find out what is going on inside the container. For example, $ docker run -it --name archivy-test -p 5000 :5000 harshavardhanj/archivy sh will start an interactive shell inside the container. Remember to pass the -it flags when you want access to a terminal inside the container. -i/--interactive \u2014 Keeps standard input open -t/--tty \u2014 Allocates a pseudo TTY With Data Persistence You can bind-mount any directory on your host to the data directory on the container in order to ensure that if and when the container is stopped/terminated, the data saved to the container isn\u2019t lost. This can be done as follows: $ docker run -d --name archivy -p 5000 :5000 -v /path/to/host/dir:/archivy/data harshavardhanj/archivy -v/--volume host:container \u2014\u2014 Bind-mount host path to container path The argument -v /path/to/host/dir:/archivy/data bind-mounts the directory /path/to/host/dir on the host to /archivy/data on the container. If you wish to mount, say /home/bob/data , you would first need to create the data directory at /home/bob , and then change the argument to -v /home/bob/data:/archivy/data . Environment Variable Injection You can inject environment variables while starting the container as follows: $ docker run -d --name archivy -p 5000 :5000 -v /path/to/host/dir:/archivy/data -e FLASK_DEBUG = 1 -e ELASTICSEARCH_ENABLED = 0 -e ELASTICSEARCH_URL = \"http://localhost:9200/\" harshavardhanj/archivy -e/--env KEY=value \u2014\u2014 Set the environment variable( key=value ) Multiple such environment variables can be specified during run time. For now, Archivy supports the following environment variables FLASK_DEBUG - Runs Flask in debug mode. More verbose output to console ELASTICSEARCH_ENABLED - Enables Elasticsearch support ELASTICSEARCH_URL - Sets the URL at which Elasticsearch listens If the values are not set by the user, they are assigned default values during run time. NOTE : If you wish to use Archivy with Elasticsearch, read on. The setup is explained in the Docker Compose section below. Try Demo on \u2018 Play With Docker \u2019 For those with DockerHub accounts, you can try a version of Archivy on Play With Docker by clicking on the badge below. This will require you to login to your DockerHub account. This version of archivy is based on the following Docker Compose file: version : '3.8' services : archivy : image : harshavardhanj/archivy ports : - \"5000:5000\" volumes : - archivyData:/archivy/data environment : - FLASK_DEBUG=0 - ELASTICSEARCH_ENABLED=0 volumes : archivyData : As is visible from the above compose file, this is a version of Archivy running without Elasticsearch enabled. Therefore, the search function will not work. Using Docker Compose Docker Compose is an easier way to bring up containers when compared to running lengthy docker run commands. The docker-compose.yaml file contains all the necessary information that you would normally provide to the docker run command, declared in a YAML format. Once this file is written, it as simple as running the following command in the same directory as the docker-compose.yml file: $ docker-compose up -d archivy This works as long as the compose file is named docker-compose.yml . If it has a different name, you will need to specify the path to the file as an argument to the -f flag as shown below: $ docker-compose -f ./compose.yml archivy up -d This repository contains two compose files(for now). A version of the simpler one is given below: version : '3.8' services : archivy : image : harshavardhanj/archivy ports : - \"5000:5000\" volumes : - archivyData:/archivy/data environment : - FLASK_DEBUG=1 - ELASTICSEARCH_ENABLED=0 volumes : archivyData : This file pulls the container image harshavardhanj/archivy from DockerHub. binds port 5000 on the host to port 5000 on the container. creates a named volume archivyData on the host and mounts it to the /archivy/data directory on the container(which is where all persistent data will be stored). Sets the following environment variables so that they can be used by Archivy during run time FLASK_DEBUG=1 ELASTICSEARCH_ENABLED=0 This would be the same as running the following commands: $ docker volume create archivyData $ docker run -d -p 5000 :5000 -v archivy:/archivy/data -e FLASK_DEBUG = 1 -e ELASTICSEARCH_ENABLED = 0 harshavardhanj/archivy When multiple container get involved, it becomes a lot easier to deal with compose files. Which compose file to use? There are currently two compose files in the repository: docker-compose.yml docker-compose-with-elasticsearch.yml If you would like to test Archivy, just download the docker-compose.yml and run $ docker-compose -f ./docker-compose.yml up -d The contents of the file are shown below: version : '3.8' services : archivy : image : harshavardhanj/archivy ports : - \"5000:5000\" volumes : - archivyData:/archivy/data environment : - FLASK_DEBUG=0 - ELASTICSEARCH_ENABLED=0 volumes : archivyData : This file pulls the archivy image. binds port 5000 on the host to port 5000 on the container. creates a named volume archivyData on the host and mounts it to the /archivy/data directory on the container(which is where all persistent data will be stored). NOTE : If you wish to bind-mount a folder to Archivy, modify the volumes as shown below: services : archivy : ... volumes : - ./archivyData:/archivy/data This will create a folder named archivyData in your current working directory. This is the folder in which all user-generated notes/bookmarks will be stored. If you wish to test Archivy\u2019s full capabilities with Elasticsearch, use the docker-compose-with-elasticsearch.yml . The usage of this file is explained in the next section. Running Archivy With Elasticsearch The compose file given below will do the following: Set up an instance/container of Elasticsearch that listens on port 9200. Set up an instance/container of Archivy that listens on port 5000. Ensure that the two can communicate with each by setting the approprate environment variables( ELASTICSEARCH_ENABLED and ELASTICSEARCH_URL ). Both containers have volumes attached to them to ensure data persistence. The containers will restart if the process running in the container fails and exits. version : '3.8' services : archivy : image : harshavardhanj/archivy ports : - target : 5000 published : 5000 protocol : tcp volumes : - archivyData:/archivy/data environment : - FLASK_DEBUG=0 - ELASTICSEARCH_ENABLED=1 - ELASTICSEARCH_URL=http://search:9200/ networks : - archivy depends_on : - elasticsearch deploy : replicas : 1 restart_policy : condition : on-failure delay : 10s max_attempts : 3 elasticsearch : image : elasticsearch:7.9.0 ports : - target : 9200 published : 9200 protocol : tcp volumes : - searchData:/usr/share/elasticsearch/data environment : - \"discovery.type=single-node\" networks : archivy : aliases : - search deploy : replicas : 1 restart_policy : condition : on-failure delay : 10s max_attempts : 3 networks : archivy : volumes : archivyData : searchData : For a detailed description of the declarations used in the compose file, refer to the official documentation . Given below is a simplified version of the same compose file which should be fine for testing purposes: version : '3.8' services : archivy : image : harshavardhanj/archivy ports : - \"5000:5000\" volumes : - ./archivyData:/archivy/data environment : - FLASK_DEBUG=0 - ELASTICSEARCH_ENABLED=1 - ELASTICSEARCH_URL=http://elasticsearch:9200/ depends_on : - elasticsearch elasticsearch : image : elasticsearch:7.9.0 ports : - \"9200:9200\" volumes : - ./searchData:/usr/share/elasticsearch/data environment : - \"discovery.type=single-node\" The declarations are described below: For the archivy service Pulls the harshavardhanj/archivy image. ( image: ) Connects port 5000 on the host to port 5000 on the container. ( ports: ) Creates the archivyData directory in the current working directory and bind-mounts it to the /archivy/data directory in the archivy container. ( volumes: ) Sets the following environment variables. ( environment: ) FLASK_DEBUG=0 ELASTICSEARCH_ENABLED=1 ( required to enable Elasticsearch support ) ELASTICSEARCH_URL=http://elasticsearch:9200/ ( required by Archivy to connect to Elasticsearch ) Sets a condition that the archivy container will start only after the elasticsearch container starts. ( depends_on: ) For the elasticsearch service Pulls the elasticsearch:7.9.0 image. ( image: ) Connects port 9200 on the host to port 9200 on the container Creates the searchData directory in the current working directory and bind-mounts it to the /usr/share/elasticsearch/data directory in the elasticsearch container.( volumes: ) Sets the following environment variable discovery.type=single-node Using Docker Swarm To be added","title":"Installing with Docker"},{"location":"docker/#guide-to-using-archivy-with-docker","text":"This document contains enough information to help you get started with using Archivy as a container, in this case, with Docker(although you can use any other container runtime). This document will cover the following: Building a container image of Archivy Running Archivy as a container Quick start Running Archivy along with persistent data storage Running Archivy with environment variable injection Demo on \u2018 Play With Docker \u2019 Running Archivy using Docker Compose Running Archivy with Elasticsearch for full text search capabilities(with Docker Compose) Planned for the future: Running Archivy using Docker Swarm(container orchestrator) Running Archivy on Kubernetes as a production-ready setup Running Archivy on OpenStack as a production-ready setup(will be done only after the aforementioned points are completed) NOTE : Parts of the document may be incomplete as it is a work in progress. In time, more information will be added to each section/topic. If some part of the documentation is ambiguous, feel free to ask questions or make suggestions on the Issues page of the project. If necessary, additional revisions to the documentation can be made based on user feedback.","title":"Guide to using Archivy with Docker"},{"location":"docker/#prerequisites","text":"Docker needs to be installed. You can check if Docker is installed by running $ docker --version Docker version 19 .03.12, build 48a66213fe If you don't have Docker installed, take a look at the official installation guide for your device.","title":"Prerequisites"},{"location":"docker/#building-archivy","text":"","title":"Building Archivy"},{"location":"docker/#building-using-dockerfile","text":"The file Dockerfile is much more portable than the local-build.Dockerfile as you do not need any other additional files to build the image. The Dockerfile automatically downloads the source code during the build stage. Just download Dockerfile and run the following command to build the image: $ docker build -t archivy:1.0 -f /path/to/Dockerfile . This tags the image with the name archivy:1.0 . Technically, you do not need to mention the path to Dockerfile as long as the Dockerfile is titled Dockerfile . If the name is anything but that, you will need to mention the path using the -f flag. So the above command is the same as $ docker build -t archivy:1.0 . There's an easier way to build the image. You can pass the URL to the GitHub repository directly to docker , and as long as there\u2019s a file named Dockerfile in the root of the repository, Docker will build it. $ docker build -t archivy:1.0 https://github.com/Uzay-G/archivy.git#docker This will clone the GitHub repository and use the cloned repository as context. The Dockerfile at the root of the repository is used as Dockerfile . NOTE : The master branch of the Archivy repository does not contain any Dockerfiles. All files pertaining to Docker are available in the docker branch of this repository.","title":"Building using Dockerfile"},{"location":"docker/#running-archivy","text":"","title":"Running Archivy"},{"location":"docker/#quick-start","text":"You can get an instance of Archivy up and running for testing purposes with the following one-liner: $ docker run -d --name archivy-test -p 5000 :5000 harshavardhanj/archivy --name \u2014\u2014 Name of the container(can be anything) -p/--publish host:container \u2014\u2014 Bind port on host to port on container The above command runs a container with the name archivy-test and binds port 5000 on the host to port 5000 on the container. That way, you can access the server at http://localhost:5000/ on the device. If you wish to access the server at http://localhost:9000/ instead, you would change the argument to -p 9000:5000 . This container runs in detached mode, meaning that no output is printed to the terminal. To view the logs, run $ docker logs -f [ name-of-container ] which will continuously stream the logs to the terminal. To exit, type ctrl+c . To just print the logs and exit, run the same command without the -f flag. NOTE : There\u2019s an image available on DockerHub for testing purposes. It is titled archivy and is available in the harshavardhanj repository on DockerHub. You can pull the image using $ docker pull harshavardhanj/archivy","title":"Quick Start"},{"location":"docker/#running-container-in-interactive-mode","text":"You can also pass commands to the container by appending it to the docker run command. Keep in mind that the container will execute your commands and not run Archivy. This is useful if you want to find out what is going on inside the container. For example, $ docker run -it --name archivy-test -p 5000 :5000 harshavardhanj/archivy sh will start an interactive shell inside the container. Remember to pass the -it flags when you want access to a terminal inside the container. -i/--interactive \u2014 Keeps standard input open -t/--tty \u2014 Allocates a pseudo TTY","title":"Running container in interactive mode"},{"location":"docker/#with-data-persistence","text":"You can bind-mount any directory on your host to the data directory on the container in order to ensure that if and when the container is stopped/terminated, the data saved to the container isn\u2019t lost. This can be done as follows: $ docker run -d --name archivy -p 5000 :5000 -v /path/to/host/dir:/archivy/data harshavardhanj/archivy -v/--volume host:container \u2014\u2014 Bind-mount host path to container path The argument -v /path/to/host/dir:/archivy/data bind-mounts the directory /path/to/host/dir on the host to /archivy/data on the container. If you wish to mount, say /home/bob/data , you would first need to create the data directory at /home/bob , and then change the argument to -v /home/bob/data:/archivy/data .","title":"With Data Persistence"},{"location":"docker/#environment-variable-injection","text":"You can inject environment variables while starting the container as follows: $ docker run -d --name archivy -p 5000 :5000 -v /path/to/host/dir:/archivy/data -e FLASK_DEBUG = 1 -e ELASTICSEARCH_ENABLED = 0 -e ELASTICSEARCH_URL = \"http://localhost:9200/\" harshavardhanj/archivy -e/--env KEY=value \u2014\u2014 Set the environment variable( key=value ) Multiple such environment variables can be specified during run time. For now, Archivy supports the following environment variables FLASK_DEBUG - Runs Flask in debug mode. More verbose output to console ELASTICSEARCH_ENABLED - Enables Elasticsearch support ELASTICSEARCH_URL - Sets the URL at which Elasticsearch listens If the values are not set by the user, they are assigned default values during run time. NOTE : If you wish to use Archivy with Elasticsearch, read on. The setup is explained in the Docker Compose section below.","title":"Environment Variable Injection"},{"location":"docker/#try-demo-on-play-with-docker","text":"For those with DockerHub accounts, you can try a version of Archivy on Play With Docker by clicking on the badge below. This will require you to login to your DockerHub account. This version of archivy is based on the following Docker Compose file: version : '3.8' services : archivy : image : harshavardhanj/archivy ports : - \"5000:5000\" volumes : - archivyData:/archivy/data environment : - FLASK_DEBUG=0 - ELASTICSEARCH_ENABLED=0 volumes : archivyData : As is visible from the above compose file, this is a version of Archivy running without Elasticsearch enabled. Therefore, the search function will not work.","title":"Try Demo on \u2018Play With Docker\u2019"},{"location":"docker/#using-docker-compose","text":"Docker Compose is an easier way to bring up containers when compared to running lengthy docker run commands. The docker-compose.yaml file contains all the necessary information that you would normally provide to the docker run command, declared in a YAML format. Once this file is written, it as simple as running the following command in the same directory as the docker-compose.yml file: $ docker-compose up -d archivy This works as long as the compose file is named docker-compose.yml . If it has a different name, you will need to specify the path to the file as an argument to the -f flag as shown below: $ docker-compose -f ./compose.yml archivy up -d This repository contains two compose files(for now). A version of the simpler one is given below: version : '3.8' services : archivy : image : harshavardhanj/archivy ports : - \"5000:5000\" volumes : - archivyData:/archivy/data environment : - FLASK_DEBUG=1 - ELASTICSEARCH_ENABLED=0 volumes : archivyData : This file pulls the container image harshavardhanj/archivy from DockerHub. binds port 5000 on the host to port 5000 on the container. creates a named volume archivyData on the host and mounts it to the /archivy/data directory on the container(which is where all persistent data will be stored). Sets the following environment variables so that they can be used by Archivy during run time FLASK_DEBUG=1 ELASTICSEARCH_ENABLED=0 This would be the same as running the following commands: $ docker volume create archivyData $ docker run -d -p 5000 :5000 -v archivy:/archivy/data -e FLASK_DEBUG = 1 -e ELASTICSEARCH_ENABLED = 0 harshavardhanj/archivy When multiple container get involved, it becomes a lot easier to deal with compose files.","title":"Using Docker Compose"},{"location":"docker/#which-compose-file-to-use","text":"There are currently two compose files in the repository: docker-compose.yml docker-compose-with-elasticsearch.yml If you would like to test Archivy, just download the docker-compose.yml and run $ docker-compose -f ./docker-compose.yml up -d The contents of the file are shown below: version : '3.8' services : archivy : image : harshavardhanj/archivy ports : - \"5000:5000\" volumes : - archivyData:/archivy/data environment : - FLASK_DEBUG=0 - ELASTICSEARCH_ENABLED=0 volumes : archivyData : This file pulls the archivy image. binds port 5000 on the host to port 5000 on the container. creates a named volume archivyData on the host and mounts it to the /archivy/data directory on the container(which is where all persistent data will be stored). NOTE : If you wish to bind-mount a folder to Archivy, modify the volumes as shown below: services : archivy : ... volumes : - ./archivyData:/archivy/data This will create a folder named archivyData in your current working directory. This is the folder in which all user-generated notes/bookmarks will be stored. If you wish to test Archivy\u2019s full capabilities with Elasticsearch, use the docker-compose-with-elasticsearch.yml . The usage of this file is explained in the next section.","title":"Which compose file to use?"},{"location":"docker/#running-archivy-with-elasticsearch","text":"The compose file given below will do the following: Set up an instance/container of Elasticsearch that listens on port 9200. Set up an instance/container of Archivy that listens on port 5000. Ensure that the two can communicate with each by setting the approprate environment variables( ELASTICSEARCH_ENABLED and ELASTICSEARCH_URL ). Both containers have volumes attached to them to ensure data persistence. The containers will restart if the process running in the container fails and exits. version : '3.8' services : archivy : image : harshavardhanj/archivy ports : - target : 5000 published : 5000 protocol : tcp volumes : - archivyData:/archivy/data environment : - FLASK_DEBUG=0 - ELASTICSEARCH_ENABLED=1 - ELASTICSEARCH_URL=http://search:9200/ networks : - archivy depends_on : - elasticsearch deploy : replicas : 1 restart_policy : condition : on-failure delay : 10s max_attempts : 3 elasticsearch : image : elasticsearch:7.9.0 ports : - target : 9200 published : 9200 protocol : tcp volumes : - searchData:/usr/share/elasticsearch/data environment : - \"discovery.type=single-node\" networks : archivy : aliases : - search deploy : replicas : 1 restart_policy : condition : on-failure delay : 10s max_attempts : 3 networks : archivy : volumes : archivyData : searchData : For a detailed description of the declarations used in the compose file, refer to the official documentation . Given below is a simplified version of the same compose file which should be fine for testing purposes: version : '3.8' services : archivy : image : harshavardhanj/archivy ports : - \"5000:5000\" volumes : - ./archivyData:/archivy/data environment : - FLASK_DEBUG=0 - ELASTICSEARCH_ENABLED=1 - ELASTICSEARCH_URL=http://elasticsearch:9200/ depends_on : - elasticsearch elasticsearch : image : elasticsearch:7.9.0 ports : - \"9200:9200\" volumes : - ./searchData:/usr/share/elasticsearch/data environment : - \"discovery.type=single-node\" The declarations are described below: For the archivy service Pulls the harshavardhanj/archivy image. ( image: ) Connects port 5000 on the host to port 5000 on the container. ( ports: ) Creates the archivyData directory in the current working directory and bind-mounts it to the /archivy/data directory in the archivy container. ( volumes: ) Sets the following environment variables. ( environment: ) FLASK_DEBUG=0 ELASTICSEARCH_ENABLED=1 ( required to enable Elasticsearch support ) ELASTICSEARCH_URL=http://elasticsearch:9200/ ( required by Archivy to connect to Elasticsearch ) Sets a condition that the archivy container will start only after the elasticsearch container starts. ( depends_on: ) For the elasticsearch service Pulls the elasticsearch:7.9.0 image. ( image: ) Connects port 9200 on the host to port 9200 on the container Creates the searchData directory in the current working directory and bind-mounts it to the /usr/share/elasticsearch/data directory in the elasticsearch container.( volumes: ) Sets the following environment variable discovery.type=single-node","title":"Running Archivy With Elasticsearch"},{"location":"docker/#using-docker-swarm","text":"To be added","title":"Using Docker Swarm"},{"location":"install/","text":"From pip You can easily install archivy with pip . (pip is the default package installer for Python, you can use pip to install many packages and apps, see this link for more information if needed) Make sure your system has Python, pip and Pandoc installed. Pandoc allows us to convert between different formats and is an awesome tool. You can download Pandoc here The Python programming language can be downloaded from here . Install the python package with pip install archivy Run archivy create-admin <username> to create a new user. There you go! You should be able to start the app by running archivy run in your terminal and then just login. With docker You can also use archivy with Docker. See Installing with docker for instructions on this. We might implement an AppImage install. Comment here if you'd like to see that happen.","title":"Installing Archivy"},{"location":"install/#from-pip","text":"You can easily install archivy with pip . (pip is the default package installer for Python, you can use pip to install many packages and apps, see this link for more information if needed) Make sure your system has Python, pip and Pandoc installed. Pandoc allows us to convert between different formats and is an awesome tool. You can download Pandoc here The Python programming language can be downloaded from here . Install the python package with pip install archivy Run archivy create-admin <username> to create a new user. There you go! You should be able to start the app by running archivy run in your terminal and then just login.","title":"From pip"},{"location":"install/#with-docker","text":"You can also use archivy with Docker. See Installing with docker for instructions on this. We might implement an AppImage install. Comment here if you'd like to see that happen.","title":"With docker"},{"location":"plugins/","text":"Plugins Plugins are a newly introduced method to add extensions to the archivy cli and web interface. It relies on the extremely useful click-plugins package that is loaded through pip and the click-web which has been modified and whose can be found in archivy/click_web/ , some of the tests, templates and static files. To help you understand the way the plugin system works, we're going to build our own plugin that allows users to sync content from pocket . We'll even deploy it to Pypi so that other people can install it. Note: The source code for this plugin is available here . We also recommend you to read the overview of the reference so you can better use the wrapper methods archivy exposes. Prerequisites: A python and pip installation with archivy. Step 1: Defining what our archivy extension does archivy source used to have a built-in feature that allowed you to sync up to the bookmarks of your pocket account. We removed this and prefer to replace it with a standalone plugin, so let's build it! What it will do is allow a user to download their bookmarks from pocket, without redownloading content that already exists. Step 2: Setting up the project Make a new directory named archivy_pocket wherever you want on your system and create a new setup.py file that will define the characteristics of our package. This is what it looks like: from setuptools import setup , find_packages with open ( \"README.md\" , \"r\" ) as fh : long_description = fh . read () setup ( name = 'archivy_pocket' , version = '0.1.0' , author = \"Uzay-G\" , author_email = \"halcyon@disroot.org\" , description = ( \"Archivy extension to sync content to your pocket account.\" ), long_description = long_description , long_description_content_type = \"text/markdown\" , classifiers = [ \"Programming Language :: Python :: 3\" , ], packages = find_packages (), entry_points = ''' [archivy.plugins] pocket=archivy_pocket:pocket ''' ) Let's walk through what this is doing. We are setting up our package and we define a few characteristics of the package. We specify some metadata you can adapt to your own package. We then load our package by using the find_packages function. The entry_points part is the most important. The [archivy.plugins] tells archivy that this package will extend our CLI and then we define the command we want to add. In our case, people will call the extension using archivy pocket . We will actually be creating a group so that people will call subcommands like this: archivy pocket <subcommand> . You can do things either way. Create a README.md file that you can keep empty for now but should haCve information on your project. Step 3: Writing the core code of our plugin Create an archivy_web directory inside the current directory where setup.py is stored. Create an __init__.py file in that directory where we'll store our main project code. For larger projects, it's better to separate concerns but that'll be good for now. This is what the skeleton of our code looks will look like. import click # the package that manages the cli @click . group () def pocket (): pass @pocket . command () def command1 (): ... @pocket . command () def command2 (): ... With this structure, you'll be able to call archivy pocket command1 and archivy pocket command2 . Read the click docs to learn about how to build more intricate options. We also provide some custom ones like email and password Let's get into actually writing a command that interacts with the archivy codebase. The code below does a few things: It imports the archivy app that basically is the interface for the webserver and many essential Flask features (flask is the web framework archivy uses). It imports the get_db function that allows us to access and modify the db. We define our pocket group. We create a new command from that group, what's important here is the with app.app_context part. We need to run our code inside the archivy app_context to be able to call some of the archivy methods. If you call archivy methods in your plugins, it might fail if you don't include this part. Then we just have our command code. import click from archivy.extensions import get_db from archivy import app from archivy.data import get_items @click . group () def pocket (): pass @pocket . command () @click . argument ( \"api_key\" ) def auth ( api_key ): with app . app_context (): db = get_db () # ... We also added some other commands, but we'll skip them for brevity and you can check out the source code here . Now you just need to do pip install . in the main directory and you'll have access to the commands. Check it out by running archivy --help . Step 4: Publishing our package to Pypi Pypi is the Python package repository. Publishing our package to it will allow other users to easily install our code onto their own archivy instance. This is a short overview. Check out this website for more info. This section is inspired by this . Make sure the required utilities are installed: python3 - m pip install -- user -- upgrade setuptools wheel Now run this command in the main dir to build the source: python3 setup . py sdist bdist_wheel Now you need to create an account on Pypi . Then go here and create a new API token; don\u2019t limit its scope to a particular project, since you are creating a new project. Once you've saved your token, install twine , the program that will take care of the upload: python3 - m pip install -- user -- upgrade twine And you can finally upload your code! The username you should enter is __token__ and then the password is your API token. python3 - m twine upload dist /* You're done! Now that you've finished your package, you can share it if you'd like, publish it on a public git repository so other people can collaborate, and you can add it to the awesome_archivy github repo which is an official list of plugins built around Archivy. We'd also love to hear about it on our discord server !","title":"Index"},{"location":"plugins/#plugins","text":"Plugins are a newly introduced method to add extensions to the archivy cli and web interface. It relies on the extremely useful click-plugins package that is loaded through pip and the click-web which has been modified and whose can be found in archivy/click_web/ , some of the tests, templates and static files. To help you understand the way the plugin system works, we're going to build our own plugin that allows users to sync content from pocket . We'll even deploy it to Pypi so that other people can install it. Note: The source code for this plugin is available here . We also recommend you to read the overview of the reference so you can better use the wrapper methods archivy exposes. Prerequisites: A python and pip installation with archivy.","title":"Plugins"},{"location":"plugins/#step-1-defining-what-our-archivy-extension-does","text":"archivy source used to have a built-in feature that allowed you to sync up to the bookmarks of your pocket account. We removed this and prefer to replace it with a standalone plugin, so let's build it! What it will do is allow a user to download their bookmarks from pocket, without redownloading content that already exists.","title":"Step 1: Defining what our archivy extension does"},{"location":"plugins/#step-2-setting-up-the-project","text":"Make a new directory named archivy_pocket wherever you want on your system and create a new setup.py file that will define the characteristics of our package. This is what it looks like: from setuptools import setup , find_packages with open ( \"README.md\" , \"r\" ) as fh : long_description = fh . read () setup ( name = 'archivy_pocket' , version = '0.1.0' , author = \"Uzay-G\" , author_email = \"halcyon@disroot.org\" , description = ( \"Archivy extension to sync content to your pocket account.\" ), long_description = long_description , long_description_content_type = \"text/markdown\" , classifiers = [ \"Programming Language :: Python :: 3\" , ], packages = find_packages (), entry_points = ''' [archivy.plugins] pocket=archivy_pocket:pocket ''' ) Let's walk through what this is doing. We are setting up our package and we define a few characteristics of the package. We specify some metadata you can adapt to your own package. We then load our package by using the find_packages function. The entry_points part is the most important. The [archivy.plugins] tells archivy that this package will extend our CLI and then we define the command we want to add. In our case, people will call the extension using archivy pocket . We will actually be creating a group so that people will call subcommands like this: archivy pocket <subcommand> . You can do things either way. Create a README.md file that you can keep empty for now but should haCve information on your project.","title":"Step 2: Setting up the project"},{"location":"plugins/#step-3-writing-the-core-code-of-our-plugin","text":"Create an archivy_web directory inside the current directory where setup.py is stored. Create an __init__.py file in that directory where we'll store our main project code. For larger projects, it's better to separate concerns but that'll be good for now. This is what the skeleton of our code looks will look like. import click # the package that manages the cli @click . group () def pocket (): pass @pocket . command () def command1 (): ... @pocket . command () def command2 (): ... With this structure, you'll be able to call archivy pocket command1 and archivy pocket command2 . Read the click docs to learn about how to build more intricate options. We also provide some custom ones like email and password Let's get into actually writing a command that interacts with the archivy codebase. The code below does a few things: It imports the archivy app that basically is the interface for the webserver and many essential Flask features (flask is the web framework archivy uses). It imports the get_db function that allows us to access and modify the db. We define our pocket group. We create a new command from that group, what's important here is the with app.app_context part. We need to run our code inside the archivy app_context to be able to call some of the archivy methods. If you call archivy methods in your plugins, it might fail if you don't include this part. Then we just have our command code. import click from archivy.extensions import get_db from archivy import app from archivy.data import get_items @click . group () def pocket (): pass @pocket . command () @click . argument ( \"api_key\" ) def auth ( api_key ): with app . app_context (): db = get_db () # ... We also added some other commands, but we'll skip them for brevity and you can check out the source code here . Now you just need to do pip install . in the main directory and you'll have access to the commands. Check it out by running archivy --help .","title":"Step 3: Writing the core code of our plugin"},{"location":"plugins/#step-4-publishing-our-package-to-pypi","text":"Pypi is the Python package repository. Publishing our package to it will allow other users to easily install our code onto their own archivy instance. This is a short overview. Check out this website for more info. This section is inspired by this . Make sure the required utilities are installed: python3 - m pip install -- user -- upgrade setuptools wheel Now run this command in the main dir to build the source: python3 setup . py sdist bdist_wheel Now you need to create an account on Pypi . Then go here and create a new API token; don\u2019t limit its scope to a particular project, since you are creating a new project. Once you've saved your token, install twine , the program that will take care of the upload: python3 - m pip install -- user -- upgrade twine And you can finally upload your code! The username you should enter is __token__ and then the password is your API token. python3 - m twine upload dist /*","title":"Step 4: Publishing our package to Pypi"},{"location":"plugins/#youre-done","text":"Now that you've finished your package, you can share it if you'd like, publish it on a public git repository so other people can collaborate, and you can add it to the awesome_archivy github repo which is an official list of plugins built around Archivy. We'd also love to hear about it on our discord server !","title":"You're done!"},{"location":"setup-search/","text":"Archivy uses ElasticSearch to provide efficient full-text search. Instructions to install and run the service are provided here . Append these two lines to your elasticsearch.yml config file : http.cors.enabled : true http.cors.allow-origin : \"http://localhost:5000\" Run archivy like this: ELASTICSEARCH_ENABLED = 1 archivy run You will now have full-text search on your knowledge base! Whenever you edit an item through the files, it is better to have archivy running that way your changes will be synced to Elasticsearch. Elasticsearch can be a hefty dependency, so if you have any ideas for something more light-weight that could be used as an alternative, please share on this thread .","title":"Search"},{"location":"usage/","text":"Archivy comes with a simple command line interface that you use on the backend to run archivy: Usage: archivy [OPTIONS] COMMAND [ARGS]... Options: --version Show the flask version --help Show this message and exit. Commands: routes Show the routes for the app. run Runs archivy web application shell Run a shell in the app context. The first time you run archivy, an admin user will automatically be created with a random password. These credentials will be printed to the log when you launch like this: [2020-10-10 10:48:27,764] INFO in __init__: Archivy has created an admin user as it did not exist. Username: 'admin', password: '5a512991c605ea51038ce2a0' Login with these credentials and then you can change your password/username by clicking the profile button on the top left. You can then use archivy to create notes, bookmarks and then organize and store information.","title":"Usage"},{"location":"whats_next/","text":"Now that you have a working installation and you know how to use, where can you go from here? If you'd like to write a plugin for archivy with standalone functionality you'd like to see, check out the plugin tutorial . You can also use our Web API ! If you notice any bugs or problems, or if you have any features you'd like to see, please open up an issue on GitHub . You can also directly talk to us on the archivy discord server !","title":"What's Next"},{"location":"reference/architecture/","text":"This document is a general overview of how the different pieces of archivy interact and what technologies it uses. Reading this will be useful for people looking to access the inner archivy API to write plugins. Read this post to understand what the function of an architecture.md file is. Archivy is: A Flask web application. A click backend command line interface. You use the cli to run the app, and you'll probably be using the web application for direct usage of archivy. Data Storage DataObjs is the term used to denote a note or bookmark that is stored in your knowledge base (abbreviation for Data Object). These are stored in a directory on your filesystem of which you can configure the location . They are organized in markdown files with yaml front matter like this: --- date : 08-31-20 desc : '' id : 100 path : '' tags : [] title : ... type : note --- ... Archivy uses the python-frontmatter package to handle the parsing of these files. They can be organized into user-specified sub-directories. Check out the reference to see the methods archivy uses for this. We also use pandoc for the conversion of different formats, and plan on using this onwards to support many more formats (PDF, EPUB, etc...) Another storage method Archivy uses is TinyDB . This is a small, simple document-oriented database archivy gives you access to for persistent data you might want to store in archivy plugins. Use helpers.get_db to call the database. Search Archivy uses Elasticsearch to index and allow users to have full-text search on their knowledge bases. Elasticsearch requires configuration to have higher quality search results. You can check out the top-notch config archivy already uses by default here . Check out the helper methods archivy exposes for ES. Daemon Since the DataObjs are stored on the filesystem, archivy runs a daemon to check for changes in the files. What it does: If you have elasticsearch enabled, whenever you edit a file, it will sync the changes to ES. Say you have a large amount of markdown files, that are obviously not formatted to be used by archivy. Moving them into your DataObj dir will cause the daemon to automatically format them so they conform to the archivy format and can be used. Auth Archivy uses flask-login for auth. All endpoints require to be authenticated. When you run archivy for the first time, it creates an admin user for you and outputs a temporary password. In our roadmap we plan to extend our permission framework to have a multi-user system, and define configuration for the permissions of non-logged in users. In general we want to make things more flexible on the auth side. How bookmarks work One of the core features of archivy is being able to save webpages locally. The way this works is the conversion of the html of the page you specify to a simple, markdown file. We might want to extend this to also be able to save PDF, EPUB and other formats. You can find the reference for this part here . Further down the road, it'd be nice to add background processing and not only download the webpage, but also save the essential assets it loads for a more complete process. This feature of preserving web content aligns with the mission against link rot 1 . Plugins Plugins in archivy function as standalone python packages. The phenomenal click-plugins package allows us to do this by basically adding commands to the cli. So you create a python package where you specify commands to extend your pre-existing archivy cli. Then these added commands will be able to be used through the cli. But what makes plugins interesting is that you can actually also use the plugins through the web interface, without having access to the system running archivy. We use an adaptation of the click-web to convert your cli commands to interactive web forms. See this manifesto to learn more about this phenomenon. \u21a9","title":"Architecture"},{"location":"reference/architecture/#data-storage","text":"DataObjs is the term used to denote a note or bookmark that is stored in your knowledge base (abbreviation for Data Object). These are stored in a directory on your filesystem of which you can configure the location . They are organized in markdown files with yaml front matter like this: --- date : 08-31-20 desc : '' id : 100 path : '' tags : [] title : ... type : note --- ... Archivy uses the python-frontmatter package to handle the parsing of these files. They can be organized into user-specified sub-directories. Check out the reference to see the methods archivy uses for this. We also use pandoc for the conversion of different formats, and plan on using this onwards to support many more formats (PDF, EPUB, etc...) Another storage method Archivy uses is TinyDB . This is a small, simple document-oriented database archivy gives you access to for persistent data you might want to store in archivy plugins. Use helpers.get_db to call the database.","title":"Data Storage"},{"location":"reference/architecture/#search","text":"Archivy uses Elasticsearch to index and allow users to have full-text search on their knowledge bases. Elasticsearch requires configuration to have higher quality search results. You can check out the top-notch config archivy already uses by default here . Check out the helper methods archivy exposes for ES.","title":"Search"},{"location":"reference/architecture/#daemon","text":"Since the DataObjs are stored on the filesystem, archivy runs a daemon to check for changes in the files. What it does: If you have elasticsearch enabled, whenever you edit a file, it will sync the changes to ES. Say you have a large amount of markdown files, that are obviously not formatted to be used by archivy. Moving them into your DataObj dir will cause the daemon to automatically format them so they conform to the archivy format and can be used.","title":"Daemon"},{"location":"reference/architecture/#auth","text":"Archivy uses flask-login for auth. All endpoints require to be authenticated. When you run archivy for the first time, it creates an admin user for you and outputs a temporary password. In our roadmap we plan to extend our permission framework to have a multi-user system, and define configuration for the permissions of non-logged in users. In general we want to make things more flexible on the auth side.","title":"Auth"},{"location":"reference/architecture/#how-bookmarks-work","text":"One of the core features of archivy is being able to save webpages locally. The way this works is the conversion of the html of the page you specify to a simple, markdown file. We might want to extend this to also be able to save PDF, EPUB and other formats. You can find the reference for this part here . Further down the road, it'd be nice to add background processing and not only download the webpage, but also save the essential assets it loads for a more complete process. This feature of preserving web content aligns with the mission against link rot 1 .","title":"How bookmarks work"},{"location":"reference/architecture/#plugins","text":"Plugins in archivy function as standalone python packages. The phenomenal click-plugins package allows us to do this by basically adding commands to the cli. So you create a python package where you specify commands to extend your pre-existing archivy cli. Then these added commands will be able to be used through the cli. But what makes plugins interesting is that you can actually also use the plugins through the web interface, without having access to the system running archivy. We use an adaptation of the click-web to convert your cli commands to interactive web forms. See this manifesto to learn more about this phenomenon. \u21a9","title":"Plugins"},{"location":"reference/filesystem_layer/","text":"This module holds the methods used to access, modify, and delete components of the filesystem where Dataobjs are stored in Archivy. Directory Tree like file-structure used to build file navigation in Archiv create ( contents , title , path = '' ) Helper method to save a new dataobj onto the filesystem. contents : md file contents title - title used for filename path Source code in archivy/data.py def create ( contents , title , path = \"\" ): \"\"\" Helper method to save a new dataobj onto the filesystem. Parameters: - **contents**: md file contents - **title** - title used for filename - **path** \"\"\" path_to_md_file = get_data_dir () / path / f \" { secure_filename ( title ) } .md\" with open ( path_to_md_file , \"w\" , encoding = \"utf-8\" ) as file : file . write ( contents ) return path_to_md_file create_dir ( name ) Create dir of given name Source code in archivy/data.py def create_dir ( name ): \"\"\"Create dir of given name\"\"\" home_dir = get_data_dir () new_path = home_dir / name new_path . mkdir ( parents = True , exist_ok = True ) return str ( new_path . relative_to ( home_dir )) delete_dir ( name ) Deletes dir of given name Source code in archivy/data.py def delete_dir ( name ): \"\"\"Deletes dir of given name\"\"\" try : rmtree ( get_data_dir () / name ) return True except FileNotFoundError : return False delete_item ( dataobj_id ) Delete dataobj of given id Source code in archivy/data.py def delete_item ( dataobj_id ): \"\"\"Delete dataobj of given id\"\"\" file = get_by_id ( dataobj_id ) if file : Path ( file ) . unlink () get_by_id ( dataobj_id ) Returns filename of dataobj of given id Source code in archivy/data.py def get_by_id ( dataobj_id ): \"\"\"Returns filename of dataobj of given id\"\"\" results = list ( get_data_dir () . rglob ( f \" { dataobj_id }{ FILE_GLOB } \" )) return results [ 0 ] if results else None get_data_dir () Returns the directory where dataobjs are stored Source code in archivy/data.py def get_data_dir (): \"\"\"Returns the directory where dataobjs are stored\"\"\" return Path ( current_app . config [ 'APP_PATH' ]) / \"data\" get_dirs () Gets all dir names where dataobjs are stored Source code in archivy/data.py def get_dirs (): \"\"\"Gets all dir names where dataobjs are stored\"\"\" # join glob matchers dirnames = [ str ( dir_path . relative_to ( get_data_dir ())) for dir_path in get_data_dir () . rglob ( \"*\" ) if dir_path . is_dir ()] # append name for root dir dirnames . append ( \"not classified\" ) return dirnames get_item ( dataobj_id ) Returns a Post object with the given dataobjs' attributes Source code in archivy/data.py def get_item ( dataobj_id ): \"\"\"Returns a Post object with the given dataobjs' attributes\"\"\" file = get_by_id ( dataobj_id ) if file : data = frontmatter . load ( file ) data [ \"fullpath\" ] = str ( file ) return data return None get_items ( collections = [], path = '' , structured = True , json_format = False ) Gets all dataobjs. collections - filter dataobj by type, eg. bookmark / note path - filter by path **structured: if set to True, will return a Directory object, otherwise data will just be returned as a list of dataobjs json_format : boolean value used internally to pre-process dataobjs to send back a json response. Source code in archivy/data.py def get_items ( collections = [], path = \"\" , structured = True , json_format = False ): \"\"\" Gets all dataobjs. Parameters: - **collections** - filter dataobj by type, eg. bookmark / note - **path** - filter by path - **structured: if set to True, will return a Directory object, otherwise data will just be returned as a list of dataobjs - **json_format**: boolean value used internally to pre-process dataobjs to send back a json response. \"\"\" datacont = Directory ( \"root\" ) if structured else [] home_dir = get_data_dir () for filename in home_dir . rglob ( path + \"*\" ): if structured : paths = filename . relative_to ( home_dir ) current_dir = datacont # iterate through paths for segment in paths . parts : if segment . endswith ( \".md\" ): data = frontmatter . load ( filename ) current_dir . child_files . append ( data ) else : # directory has not been saved in tree yet if segment not in current_dir . child_dirs : current_dir . child_dirs [ segment ] = Directory ( segment ) current_dir = current_dir . child_dirs [ segment ] else : if filename . parts [ - 1 ] . endswith ( \".md\" ): data = frontmatter . load ( filename ) if len ( collections ) == 0 or \\ any ([ collection == data [ \"type\" ] for collection in collections ]): if json_format : dict_dataobj = data . __dict__ # remove unnecessary yaml handler dict_dataobj . pop ( \"handler\" ) datacont . append ( dict_dataobj ) else : datacont . append ( data ) return datacont open_file ( path ) Cross platform way of opening file on user's computer Source code in archivy/data.py def open_file ( path ): \"\"\"Cross platform way of opening file on user's computer\"\"\" if platform . system () == \"Windows\" : os . startfile ( path ) elif platform . system () == \"Darwin\" : subprocess . Popen ([ \"open\" , path ]) else : subprocess . Popen ([ \"xdg-open\" , path ]) update_item ( dataobj_id , new_content ) Given an object id, this method overwrites the inner content of the post with new_content . This means that it won't change the frontmatter (eg tags, id, title) but it can change the file content. For example: If we have a dataobj like this: --- id: 1 title: Note --- # This is random Calling update_item(1, \"# This is specific\") will turn it into: --- id: 1 # unchanged title: Note --- # This is specific Source code in archivy/data.py def update_item ( dataobj_id , new_content ): \"\"\" Given an object id, this method overwrites the inner content of the post with `new_content`. This means that it won't change the frontmatter (eg tags, id, title) but it can change the file content. For example: If we have a dataobj like this: ```md --- id: 1 title: Note --- # This is random ``` Calling `update_item(1, \"# This is specific\")` will turn it into: ```md --- id: 1 # unchanged title: Note --- # This is specific ``` \"\"\" filename = get_by_id ( dataobj_id ) dataobj = frontmatter . load ( filename ) with open ( filename , \"w\" , encoding = \"utf-8\" ) as f : dataobj . content = new_content f . write ( frontmatter . dumps ( dataobj ))","title":"Dataobj Filesystem Layer"},{"location":"reference/filesystem_layer/#archivy.data","text":"","title":"archivy.data"},{"location":"reference/filesystem_layer/#archivy.data.Directory","text":"Tree like file-structure used to build file navigation in Archiv","title":"Directory"},{"location":"reference/filesystem_layer/#archivy.data.create","text":"Helper method to save a new dataobj onto the filesystem. contents : md file contents title - title used for filename path Source code in archivy/data.py def create ( contents , title , path = \"\" ): \"\"\" Helper method to save a new dataobj onto the filesystem. Parameters: - **contents**: md file contents - **title** - title used for filename - **path** \"\"\" path_to_md_file = get_data_dir () / path / f \" { secure_filename ( title ) } .md\" with open ( path_to_md_file , \"w\" , encoding = \"utf-8\" ) as file : file . write ( contents ) return path_to_md_file","title":"create()"},{"location":"reference/filesystem_layer/#archivy.data.create_dir","text":"Create dir of given name Source code in archivy/data.py def create_dir ( name ): \"\"\"Create dir of given name\"\"\" home_dir = get_data_dir () new_path = home_dir / name new_path . mkdir ( parents = True , exist_ok = True ) return str ( new_path . relative_to ( home_dir ))","title":"create_dir()"},{"location":"reference/filesystem_layer/#archivy.data.delete_dir","text":"Deletes dir of given name Source code in archivy/data.py def delete_dir ( name ): \"\"\"Deletes dir of given name\"\"\" try : rmtree ( get_data_dir () / name ) return True except FileNotFoundError : return False","title":"delete_dir()"},{"location":"reference/filesystem_layer/#archivy.data.delete_item","text":"Delete dataobj of given id Source code in archivy/data.py def delete_item ( dataobj_id ): \"\"\"Delete dataobj of given id\"\"\" file = get_by_id ( dataobj_id ) if file : Path ( file ) . unlink ()","title":"delete_item()"},{"location":"reference/filesystem_layer/#archivy.data.get_by_id","text":"Returns filename of dataobj of given id Source code in archivy/data.py def get_by_id ( dataobj_id ): \"\"\"Returns filename of dataobj of given id\"\"\" results = list ( get_data_dir () . rglob ( f \" { dataobj_id }{ FILE_GLOB } \" )) return results [ 0 ] if results else None","title":"get_by_id()"},{"location":"reference/filesystem_layer/#archivy.data.get_data_dir","text":"Returns the directory where dataobjs are stored Source code in archivy/data.py def get_data_dir (): \"\"\"Returns the directory where dataobjs are stored\"\"\" return Path ( current_app . config [ 'APP_PATH' ]) / \"data\"","title":"get_data_dir()"},{"location":"reference/filesystem_layer/#archivy.data.get_dirs","text":"Gets all dir names where dataobjs are stored Source code in archivy/data.py def get_dirs (): \"\"\"Gets all dir names where dataobjs are stored\"\"\" # join glob matchers dirnames = [ str ( dir_path . relative_to ( get_data_dir ())) for dir_path in get_data_dir () . rglob ( \"*\" ) if dir_path . is_dir ()] # append name for root dir dirnames . append ( \"not classified\" ) return dirnames","title":"get_dirs()"},{"location":"reference/filesystem_layer/#archivy.data.get_item","text":"Returns a Post object with the given dataobjs' attributes Source code in archivy/data.py def get_item ( dataobj_id ): \"\"\"Returns a Post object with the given dataobjs' attributes\"\"\" file = get_by_id ( dataobj_id ) if file : data = frontmatter . load ( file ) data [ \"fullpath\" ] = str ( file ) return data return None","title":"get_item()"},{"location":"reference/filesystem_layer/#archivy.data.get_items","text":"Gets all dataobjs. collections - filter dataobj by type, eg. bookmark / note path - filter by path **structured: if set to True, will return a Directory object, otherwise data will just be returned as a list of dataobjs json_format : boolean value used internally to pre-process dataobjs to send back a json response. Source code in archivy/data.py def get_items ( collections = [], path = \"\" , structured = True , json_format = False ): \"\"\" Gets all dataobjs. Parameters: - **collections** - filter dataobj by type, eg. bookmark / note - **path** - filter by path - **structured: if set to True, will return a Directory object, otherwise data will just be returned as a list of dataobjs - **json_format**: boolean value used internally to pre-process dataobjs to send back a json response. \"\"\" datacont = Directory ( \"root\" ) if structured else [] home_dir = get_data_dir () for filename in home_dir . rglob ( path + \"*\" ): if structured : paths = filename . relative_to ( home_dir ) current_dir = datacont # iterate through paths for segment in paths . parts : if segment . endswith ( \".md\" ): data = frontmatter . load ( filename ) current_dir . child_files . append ( data ) else : # directory has not been saved in tree yet if segment not in current_dir . child_dirs : current_dir . child_dirs [ segment ] = Directory ( segment ) current_dir = current_dir . child_dirs [ segment ] else : if filename . parts [ - 1 ] . endswith ( \".md\" ): data = frontmatter . load ( filename ) if len ( collections ) == 0 or \\ any ([ collection == data [ \"type\" ] for collection in collections ]): if json_format : dict_dataobj = data . __dict__ # remove unnecessary yaml handler dict_dataobj . pop ( \"handler\" ) datacont . append ( dict_dataobj ) else : datacont . append ( data ) return datacont","title":"get_items()"},{"location":"reference/filesystem_layer/#archivy.data.open_file","text":"Cross platform way of opening file on user's computer Source code in archivy/data.py def open_file ( path ): \"\"\"Cross platform way of opening file on user's computer\"\"\" if platform . system () == \"Windows\" : os . startfile ( path ) elif platform . system () == \"Darwin\" : subprocess . Popen ([ \"open\" , path ]) else : subprocess . Popen ([ \"xdg-open\" , path ])","title":"open_file()"},{"location":"reference/filesystem_layer/#archivy.data.update_item","text":"Given an object id, this method overwrites the inner content of the post with new_content . This means that it won't change the frontmatter (eg tags, id, title) but it can change the file content. For example: If we have a dataobj like this: --- id: 1 title: Note --- # This is random Calling update_item(1, \"# This is specific\") will turn it into: --- id: 1 # unchanged title: Note --- # This is specific Source code in archivy/data.py def update_item ( dataobj_id , new_content ): \"\"\" Given an object id, this method overwrites the inner content of the post with `new_content`. This means that it won't change the frontmatter (eg tags, id, title) but it can change the file content. For example: If we have a dataobj like this: ```md --- id: 1 title: Note --- # This is random ``` Calling `update_item(1, \"# This is specific\")` will turn it into: ```md --- id: 1 # unchanged title: Note --- # This is specific ``` \"\"\" filename = get_by_id ( dataobj_id ) dataobj = frontmatter . load ( filename ) with open ( filename , \"w\" , encoding = \"utf-8\" ) as f : dataobj . content = new_content f . write ( frontmatter . dumps ( dataobj ))","title":"update_item()"},{"location":"reference/helpers/","text":"This is a series of helper functions that could be useful for you. Notably, the get_db and get_elastic_client could help with writing an archivy plugin. get_db ( force_reconnect = False ) Returns the database object that you can use to store data persistently Source code in archivy/helpers.py def get_db ( force_reconnect = False ): \"\"\" Returns the database object that you can use to store data persistently \"\"\" if 'db' not in g or force_reconnect : g . db = TinyDB ( os . path . join ( current_app . config [ 'APP_PATH' ], 'db.json' ) ) return g . db get_elastic_client () Returns the elasticsearch client you can use to search and insert / delete data Source code in archivy/helpers.py def get_elastic_client (): \"\"\"Returns the elasticsearch client you can use to search and insert / delete data\"\"\" if not current_app . config [ 'ELASTICSEARCH_ENABLED' ]: return None es = Elasticsearch ( current_app . config [ 'ELASTICSEARCH_URL' ]) try : health = es . cluster . health () except elasticsearch . exceptions . ConnectionError : current_app . logger . error ( \"Elasticsearch does not seem to be running on \" f \" { Config . ELASTICSEARCH_URL } . Please start \" \"it, for example with: sudo service elasticsearch restart\" ) current_app . logger . error ( \"You can disable Elasticsearch by setting the \" \"ELASTICSEARCH_ENABLED environment variable to 0\" ) sys . exit ( 1 ) if health [ \"status\" ] not in ( \"yellow\" , \"green\" ): current_app . logger . warning ( \"Elasticsearch reports that it is not working \" \"properly. Search might not work. You can disable \" \"Elasticsearch by setting ELASTICSEARCH_ENABLED to 0.\" ) return es get_max_id () Returns the current maximum id of dataobjs in the database. Source code in archivy/helpers.py def get_max_id (): \"\"\"Returns the current maximum id of dataobjs in the database.\"\"\" db = get_db () max_id = db . search ( Query () . name == \"max_id\" ) if not max_id : db . insert ({ \"name\" : \"max_id\" , \"val\" : 0 }) return 0 return max_id [ 0 ][ \"val\" ] set_max_id ( val ) Sets a new max_id Source code in archivy/helpers.py def set_max_id ( val ): \"\"\"Sets a new max_id\"\"\" db = get_db () db . update ( operations . set ( \"val\" , val ), Query () . name == \"max_id\" )","title":"Helpers"},{"location":"reference/helpers/#archivy.helpers","text":"","title":"archivy.helpers"},{"location":"reference/helpers/#archivy.helpers.get_db","text":"Returns the database object that you can use to store data persistently Source code in archivy/helpers.py def get_db ( force_reconnect = False ): \"\"\" Returns the database object that you can use to store data persistently \"\"\" if 'db' not in g or force_reconnect : g . db = TinyDB ( os . path . join ( current_app . config [ 'APP_PATH' ], 'db.json' ) ) return g . db","title":"get_db()"},{"location":"reference/helpers/#archivy.helpers.get_elastic_client","text":"Returns the elasticsearch client you can use to search and insert / delete data Source code in archivy/helpers.py def get_elastic_client (): \"\"\"Returns the elasticsearch client you can use to search and insert / delete data\"\"\" if not current_app . config [ 'ELASTICSEARCH_ENABLED' ]: return None es = Elasticsearch ( current_app . config [ 'ELASTICSEARCH_URL' ]) try : health = es . cluster . health () except elasticsearch . exceptions . ConnectionError : current_app . logger . error ( \"Elasticsearch does not seem to be running on \" f \" { Config . ELASTICSEARCH_URL } . Please start \" \"it, for example with: sudo service elasticsearch restart\" ) current_app . logger . error ( \"You can disable Elasticsearch by setting the \" \"ELASTICSEARCH_ENABLED environment variable to 0\" ) sys . exit ( 1 ) if health [ \"status\" ] not in ( \"yellow\" , \"green\" ): current_app . logger . warning ( \"Elasticsearch reports that it is not working \" \"properly. Search might not work. You can disable \" \"Elasticsearch by setting ELASTICSEARCH_ENABLED to 0.\" ) return es","title":"get_elastic_client()"},{"location":"reference/helpers/#archivy.helpers.get_max_id","text":"Returns the current maximum id of dataobjs in the database. Source code in archivy/helpers.py def get_max_id (): \"\"\"Returns the current maximum id of dataobjs in the database.\"\"\" db = get_db () max_id = db . search ( Query () . name == \"max_id\" ) if not max_id : db . insert ({ \"name\" : \"max_id\" , \"val\" : 0 }) return 0 return max_id [ 0 ][ \"val\" ]","title":"get_max_id()"},{"location":"reference/helpers/#archivy.helpers.set_max_id","text":"Sets a new max_id Source code in archivy/helpers.py def set_max_id ( val ): \"\"\"Sets a new max_id\"\"\" db = get_db () db . update ( operations . set ( \"val\" , val ), Query () . name == \"max_id\" )","title":"set_max_id()"},{"location":"reference/models/","text":"Internal API for the models Archivy uses in the backend that could be useful for writing plugins. DataObj Class that holds a data object (either a note or a bookmark). Attrbutes: [Required to pass when creating a new object] type -> \"note\" or \"bookmark\" Note : - title Bookmark : url [Optional attrs that if passed, will be set by the class] desc tags content path [Handled by the code] id date For bookmarks, Run process_bookmark_url() once you've created it. For both types, run insert() if you want to create a new file in the db with their contents. extract_content ( self , beautsoup ) converts html bookmark url to optimized markdown Source code in archivy/models.py def extract_content ( self , beautsoup ): \"\"\"converts html bookmark url to optimized markdown\"\"\" stripped_tags = [ \"footer\" , \"nav\" ] url = self . url . rstrip ( \"/\" ) for tag in stripped_tags : if getattr ( beautsoup , tag ): getattr ( beautsoup , tag ) . extract () resources = beautsoup . find_all ([ \"a\" , \"img\" ]) for external in resources : if external . name == \"a\" and \\ external . has_attr ( \"href\" ) and \\ external [ \"href\" ] . startswith ( \"/\" ): external [ \"href\" ] = urljoin ( url , external [ \"href\" ]) elif external . name == \"img\" and \\ external . has_attr ( \"src\" ) and \\ external [ \"src\" ] . startswith ( \"/\" ): external [ \"src\" ] = urljoin ( url , external [ \"src\" ]) return convert_text ( str ( beautsoup ), \"md\" , format = \"html\" ) from_md ( md_content ) classmethod Class method to generate new dataobj from a well formatted markdown string Call like this: Dataobj . from_md ( content ) Source code in archivy/models.py @classmethod def from_md ( cls , md_content : str ): \"\"\" Class method to generate new dataobj from a well formatted markdown string Call like this: ```python Dataobj.from_md(content) ``` \"\"\" data = frontmatter . loads ( md_content ) dataobj = {} dataobj [ \"content\" ] = data . content for pair in [ \"tags\" , \"desc\" , \"id\" , \"title\" , \"path\" ]: try : dataobj [ pair ] = data [ pair ] except KeyError : # files sometimes get moved temporarily by applications while you edit # this can create bugs where the data is not loaded correctly # this handles that scenario as validation will simply fail and the event will # be ignored break dataobj [ \"type\" ] = \"processed-dataobj\" return cls ( ** dataobj ) insert ( self ) Creates a new file with the object's attributes Source code in archivy/models.py def insert ( self ): \"\"\"Creates a new file with the object's attributes\"\"\" if self . validate (): helpers . set_max_id ( helpers . get_max_id () + 1 ) self . id = helpers . get_max_id () self . date = datetime . now () data = { \"type\" : self . type , \"desc\" : self . desc , \"title\" : str ( self . title ), \"date\" : self . date . strftime ( \" %x \" ) . replace ( \"/\" , \"-\" ), \"tags\" : self . tags , \"id\" : self . id , \"path\" : self . path } if self . type == \"bookmark\" or self . type == \"pocket_bookmark\" : data [ \"url\" ] = self . url # convert to markdown file dataobj = frontmatter . Post ( self . content ) dataobj . metadata = data self . fullpath = create ( frontmatter . dumps ( dataobj ), str ( self . id ) + \"-\" + dataobj [ \"date\" ] + \"-\" + dataobj [ \"title\" ], path = self . path , ) add_to_index ( current_app . config [ 'INDEX_NAME' ], self ) return self . id return False process_bookmark_url ( self ) Process url to get content for bookmark Source code in archivy/models.py def process_bookmark_url ( self ): \"\"\"Process url to get content for bookmark\"\"\" if self . type not in ( \"bookmark\" , \"pocket_bookmark\" ) or not validators . url ( self . url ): return None try : url_request = requests . get ( self . url ) except Exception : flash ( f \"Could not retrieve { self . url } \\n \" ) self . wipe () return try : parsed_html = BeautifulSoup ( url_request . text , features = \"html.parser\" ) except Exception : flash ( f \"Could not parse { self . url } \\n \" ) self . wipe () return try : self . content = self . extract_content ( parsed_html ) except Exception : flash ( f \"Could not extract content from { self . url } \\n \" ) return parsed_title = parsed_html . title self . title = ( parsed_title . string if parsed_title is not None else self . url ) validate ( self ) Verifies that the content matches required validation constraints Source code in archivy/models.py def validate ( self ): \"\"\"Verifies that the content matches required validation constraints\"\"\" valid_url = ( self . type != \"bookmark\" or self . type != \"pocket_bookmark\" ) or ( isinstance ( self . url , str ) and validators . url ( self . url )) valid_title = isinstance ( self . title , str ) and self . title != \"\" valid_content = ( self . type not in ( \"bookmark\" , \"pocket_bookmark\" ) or isinstance ( self . content , str )) return valid_url and valid_title and valid_content wipe ( self ) Resets and invalidates dataobj Source code in archivy/models.py def wipe ( self ): \"\"\"Resets and invalidates dataobj\"\"\" self . title = \"\" self . desc = None self . content = \"\" User Model we use for User that inherits from flask login's UserMixin username password is_admin from_db ( db_object ) classmethod Takes a database object and turns it into a user Source code in archivy/models.py @classmethod def from_db ( cls , db_object ): \"\"\"Takes a database object and turns it into a user\"\"\" username = db_object [ \"username\" ] id = db_object . doc_id return cls ( username = username , id = id ) insert ( self ) Inserts the model from the database Source code in archivy/models.py def insert ( self ): \"\"\"Inserts the model from the database\"\"\" if not self . password : return False hashed_password = generate_password_hash ( self . password ) db = helpers . get_db () if db . search (( Query () . type == \"user\" ) & ( Query () . username == self . username )): return False db_user = { \"username\" : self . username , \"hashed_password\" : hashed_password , \"is_admin\" : self . is_admin , \"type\" : \"user\" } return db . insert ( db_user )","title":"Models for User and DataObj"},{"location":"reference/models/#archivy.models","text":"","title":"archivy.models"},{"location":"reference/models/#archivy.models.DataObj","text":"Class that holds a data object (either a note or a bookmark). Attrbutes: [Required to pass when creating a new object] type -> \"note\" or \"bookmark\" Note : - title Bookmark : url [Optional attrs that if passed, will be set by the class] desc tags content path [Handled by the code] id date For bookmarks, Run process_bookmark_url() once you've created it. For both types, run insert() if you want to create a new file in the db with their contents.","title":"DataObj"},{"location":"reference/models/#archivy.models.DataObj.extract_content","text":"converts html bookmark url to optimized markdown Source code in archivy/models.py def extract_content ( self , beautsoup ): \"\"\"converts html bookmark url to optimized markdown\"\"\" stripped_tags = [ \"footer\" , \"nav\" ] url = self . url . rstrip ( \"/\" ) for tag in stripped_tags : if getattr ( beautsoup , tag ): getattr ( beautsoup , tag ) . extract () resources = beautsoup . find_all ([ \"a\" , \"img\" ]) for external in resources : if external . name == \"a\" and \\ external . has_attr ( \"href\" ) and \\ external [ \"href\" ] . startswith ( \"/\" ): external [ \"href\" ] = urljoin ( url , external [ \"href\" ]) elif external . name == \"img\" and \\ external . has_attr ( \"src\" ) and \\ external [ \"src\" ] . startswith ( \"/\" ): external [ \"src\" ] = urljoin ( url , external [ \"src\" ]) return convert_text ( str ( beautsoup ), \"md\" , format = \"html\" )","title":"extract_content()"},{"location":"reference/models/#archivy.models.DataObj.from_md","text":"Class method to generate new dataobj from a well formatted markdown string Call like this: Dataobj . from_md ( content ) Source code in archivy/models.py @classmethod def from_md ( cls , md_content : str ): \"\"\" Class method to generate new dataobj from a well formatted markdown string Call like this: ```python Dataobj.from_md(content) ``` \"\"\" data = frontmatter . loads ( md_content ) dataobj = {} dataobj [ \"content\" ] = data . content for pair in [ \"tags\" , \"desc\" , \"id\" , \"title\" , \"path\" ]: try : dataobj [ pair ] = data [ pair ] except KeyError : # files sometimes get moved temporarily by applications while you edit # this can create bugs where the data is not loaded correctly # this handles that scenario as validation will simply fail and the event will # be ignored break dataobj [ \"type\" ] = \"processed-dataobj\" return cls ( ** dataobj )","title":"from_md()"},{"location":"reference/models/#archivy.models.DataObj.insert","text":"Creates a new file with the object's attributes Source code in archivy/models.py def insert ( self ): \"\"\"Creates a new file with the object's attributes\"\"\" if self . validate (): helpers . set_max_id ( helpers . get_max_id () + 1 ) self . id = helpers . get_max_id () self . date = datetime . now () data = { \"type\" : self . type , \"desc\" : self . desc , \"title\" : str ( self . title ), \"date\" : self . date . strftime ( \" %x \" ) . replace ( \"/\" , \"-\" ), \"tags\" : self . tags , \"id\" : self . id , \"path\" : self . path } if self . type == \"bookmark\" or self . type == \"pocket_bookmark\" : data [ \"url\" ] = self . url # convert to markdown file dataobj = frontmatter . Post ( self . content ) dataobj . metadata = data self . fullpath = create ( frontmatter . dumps ( dataobj ), str ( self . id ) + \"-\" + dataobj [ \"date\" ] + \"-\" + dataobj [ \"title\" ], path = self . path , ) add_to_index ( current_app . config [ 'INDEX_NAME' ], self ) return self . id return False","title":"insert()"},{"location":"reference/models/#archivy.models.DataObj.process_bookmark_url","text":"Process url to get content for bookmark Source code in archivy/models.py def process_bookmark_url ( self ): \"\"\"Process url to get content for bookmark\"\"\" if self . type not in ( \"bookmark\" , \"pocket_bookmark\" ) or not validators . url ( self . url ): return None try : url_request = requests . get ( self . url ) except Exception : flash ( f \"Could not retrieve { self . url } \\n \" ) self . wipe () return try : parsed_html = BeautifulSoup ( url_request . text , features = \"html.parser\" ) except Exception : flash ( f \"Could not parse { self . url } \\n \" ) self . wipe () return try : self . content = self . extract_content ( parsed_html ) except Exception : flash ( f \"Could not extract content from { self . url } \\n \" ) return parsed_title = parsed_html . title self . title = ( parsed_title . string if parsed_title is not None else self . url )","title":"process_bookmark_url()"},{"location":"reference/models/#archivy.models.DataObj.validate","text":"Verifies that the content matches required validation constraints Source code in archivy/models.py def validate ( self ): \"\"\"Verifies that the content matches required validation constraints\"\"\" valid_url = ( self . type != \"bookmark\" or self . type != \"pocket_bookmark\" ) or ( isinstance ( self . url , str ) and validators . url ( self . url )) valid_title = isinstance ( self . title , str ) and self . title != \"\" valid_content = ( self . type not in ( \"bookmark\" , \"pocket_bookmark\" ) or isinstance ( self . content , str )) return valid_url and valid_title and valid_content","title":"validate()"},{"location":"reference/models/#archivy.models.DataObj.wipe","text":"Resets and invalidates dataobj Source code in archivy/models.py def wipe ( self ): \"\"\"Resets and invalidates dataobj\"\"\" self . title = \"\" self . desc = None self . content = \"\"","title":"wipe()"},{"location":"reference/models/#archivy.models.User","text":"Model we use for User that inherits from flask login's UserMixin username password is_admin","title":"User"},{"location":"reference/models/#archivy.models.User.from_db","text":"Takes a database object and turns it into a user Source code in archivy/models.py @classmethod def from_db ( cls , db_object ): \"\"\"Takes a database object and turns it into a user\"\"\" username = db_object [ \"username\" ] id = db_object . doc_id return cls ( username = username , id = id )","title":"from_db()"},{"location":"reference/models/#archivy.models.User.insert","text":"Inserts the model from the database Source code in archivy/models.py def insert ( self ): \"\"\"Inserts the model from the database\"\"\" if not self . password : return False hashed_password = generate_password_hash ( self . password ) db = helpers . get_db () if db . search (( Query () . type == \"user\" ) & ( Query () . username == self . username )): return False db_user = { \"username\" : self . username , \"hashed_password\" : hashed_password , \"is_admin\" : self . is_admin , \"type\" : \"user\" } return db . insert ( db_user )","title":"insert()"},{"location":"reference/search/","text":"These are a few methods to interface between archivy and the elasticsearch instance. add_to_index ( index , model ) Adds dataobj to given index. If object of given id already exists, it will be updated. index - String of the ES Index. Archivy uses dataobj by default. model - Instance of archivy.models.Dataobj , the object you want to index. Source code in archivy/search.py def add_to_index ( index , model ): \"\"\" Adds dataobj to given index. If object of given id already exists, it will be updated. Params: - **index** - String of the ES Index. Archivy uses `dataobj` by default. - **model** - Instance of `archivy.models.Dataobj`, the object you want to index. \"\"\" es = get_elastic_client () if not es : return payload = {} for field in model . __searchable__ : payload [ field ] = getattr ( model , field ) es . index ( index = index , id = model . id , body = payload ) query_index ( index , query ) Returns search results for your given query Source code in archivy/search.py def query_index ( index , query ): \"\"\"Returns search results for your given query\"\"\" es = get_elastic_client () if not es : return [] search = es . search ( index = index , body = { \"query\" : { \"multi_match\" : { \"query\" : query , \"fields\" : [ \"*\" ], \"analyzer\" : \"rebuilt_standard\" } }, \"highlight\" : { \"fields\" : { \"content\" : { \"pre_tags\" : \"<span class='matches'>\" , \"post_tags\" : \"</span>\" , \"boundary_max_scan\" : 200 , \"fragment_size\" : 0 } } } } ) text = \"\" for hit in search [ \"hits\" ][ \"hits\" ]: text += f \"<li>[ { hit [ '_source' ][ 'title' ] } ](/dataobj/ { hit [ '_id' ] } ) \\n\\n \" if \"highlight\" in hit : for highlight in hit [ \"highlight\" ][ \"content\" ]: text += f \" { highlight } \" text += \"</li>\" return convert_text ( text , \"html\" , format = \"md\" ) remove_from_index ( index , dataobj_id ) Removes object of given id Source code in archivy/search.py def remove_from_index ( index , dataobj_id ): \"\"\"Removes object of given id\"\"\" es = get_elastic_client () if not es : return es . delete ( index = index , id = dataobj_id )","title":"Search"},{"location":"reference/search/#archivy.search","text":"","title":"archivy.search"},{"location":"reference/search/#archivy.search.add_to_index","text":"Adds dataobj to given index. If object of given id already exists, it will be updated. index - String of the ES Index. Archivy uses dataobj by default. model - Instance of archivy.models.Dataobj , the object you want to index. Source code in archivy/search.py def add_to_index ( index , model ): \"\"\" Adds dataobj to given index. If object of given id already exists, it will be updated. Params: - **index** - String of the ES Index. Archivy uses `dataobj` by default. - **model** - Instance of `archivy.models.Dataobj`, the object you want to index. \"\"\" es = get_elastic_client () if not es : return payload = {} for field in model . __searchable__ : payload [ field ] = getattr ( model , field ) es . index ( index = index , id = model . id , body = payload )","title":"add_to_index()"},{"location":"reference/search/#archivy.search.query_index","text":"Returns search results for your given query Source code in archivy/search.py def query_index ( index , query ): \"\"\"Returns search results for your given query\"\"\" es = get_elastic_client () if not es : return [] search = es . search ( index = index , body = { \"query\" : { \"multi_match\" : { \"query\" : query , \"fields\" : [ \"*\" ], \"analyzer\" : \"rebuilt_standard\" } }, \"highlight\" : { \"fields\" : { \"content\" : { \"pre_tags\" : \"<span class='matches'>\" , \"post_tags\" : \"</span>\" , \"boundary_max_scan\" : 200 , \"fragment_size\" : 0 } } } } ) text = \"\" for hit in search [ \"hits\" ][ \"hits\" ]: text += f \"<li>[ { hit [ '_source' ][ 'title' ] } ](/dataobj/ { hit [ '_id' ] } ) \\n\\n \" if \"highlight\" in hit : for highlight in hit [ \"highlight\" ][ \"content\" ]: text += f \" { highlight } \" text += \"</li>\" return convert_text ( text , \"html\" , format = \"md\" )","title":"query_index()"},{"location":"reference/search/#archivy.search.remove_from_index","text":"Removes object of given id Source code in archivy/search.py def remove_from_index ( index , dataobj_id ): \"\"\"Removes object of given id\"\"\" es = get_elastic_client () if not es : return es . delete ( index = index , id = dataobj_id )","title":"remove_from_index()"},{"location":"reference/web_api/","text":"The Archivy HTTP API allows you to run small scripts in any language that will interact with your archivy instance through HTTP. All calls must be first logged in with the login endpoint. Small example in Python This code uses the requests module to interact with the API: import requests # we create a new session that will allow us to login once s = requests . session () INSTANCE_URL = < your instance url > s . post ( f \" { INSTANCE_URL } /api/login\" , auth = ( < username > , < password > )) # once you've logged in - you can make authenticated requests to the api, like: resp = s . get ( f \" { INSTANCE_URL } /api/dataobjs\" ) . content ) Reference create_bookmark () Creates a new bookmark Parameters: All parameters are sent through the JSON body. - url (required) - desc - tags - path Source code in archivy/api.py @api_bp . route ( \"/bookmarks\" , methods = [ \"POST\" ]) def create_bookmark (): \"\"\" Creates a new bookmark **Parameters:** All parameters are sent through the JSON body. - **url** (required) - **desc** - **tags** - **path** \"\"\" json_data = request . get_json () bookmark = DataObj ( url = json_data [ 'url' ], desc = json_data . get ( 'desc' ), tags = json_data . get ( 'tags' ), path = json_data . get ( \"path\" , \"\" ), type = \"bookmark\" , ) bookmark . process_bookmark_url () bookmark_id = bookmark . insert () if bookmark_id : return jsonify ( bookmark_id = bookmark_id , ) return Response ( status = 400 ) create_folder () Creates new directory Parameter in JSON body: - path (required) - path of newdir Source code in archivy/api.py @api_bp . route ( \"/folders/new\" , methods = [ \"POST\" ]) def create_folder (): \"\"\" Creates new directory Parameter in JSON body: - **path** (required) - path of newdir \"\"\" directory = request . json . get ( \"path\" ) try : sanitized_name = data . create_dir ( directory ) except FileExistsError : return Response ( \"Directory already exists\" , status = 401 ) return Response ( sanitized_name , status = 200 ) create_note () Creates a new note. Parameters: All parameters are sent through the JSON body. - title (required) - content (required) - desc - tags - path Source code in archivy/api.py @api_bp . route ( \"/notes\" , methods = [ \"POST\" ]) def create_note (): \"\"\" Creates a new note. **Parameters:** All parameters are sent through the JSON body. - **title** (required) - **content** (required) - **desc** - **tags** - **path** \"\"\" json_data = request . get_json () note = DataObj ( title = json_data [ \"title\" ], content = json_data [ \"content\" ], desc = json_data . get ( \"desc\" ), tags = json_data . get ( \"tags\" ), path = json_data . get ( \"path\" , \"\" ), type = \"note\" ) note_id = note . insert () if note_id : return jsonify ( note_id = note_id ) return Response ( status = 400 ) delete_dataobj ( dataobj_id ) Deletes object of given id Source code in archivy/api.py @api_bp . route ( \"/dataobjs/<int:dataobj_id>\" , methods = [ \"DELETE\" ]) def delete_dataobj ( dataobj_id ): \"\"\"Deletes object of given id\"\"\" if not data . get_item ( dataobj_id ): return Response ( status = 404 ) data . delete_item ( dataobj_id ) return Response ( status = 204 ) delete_folder () Deletes directory. Parameter in JSON body: - path of dir to delete Source code in archivy/api.py @api_bp . route ( \"/folders/delete\" , methods = [ \"DELETE\" ]) def delete_folder (): \"\"\" Deletes directory. Parameter in JSON body: - **path** of dir to delete \"\"\" directory = request . json . get ( \"path\" ) if directory == \"\" : return Response ( \"Cannot delete root dir\" , status = 401 ) if data . delete_dir ( directory ): return Response ( \"Successfully deleted\" , status = 200 ) return Response ( \"Not found\" , status = 404 ) get_dataobj ( dataobj_id ) Returns dataobj of given id Source code in archivy/api.py @api_bp . route ( \"/dataobjs/<int:dataobj_id>\" ) def get_dataobj ( dataobj_id ): \"\"\"Returns dataobj of given id\"\"\" dataobj = data . get_item ( dataobj_id ) return jsonify ( dataobj_id = dataobj_id , title = dataobj [ \"title\" ], content = dataobj . content , md_path = dataobj [ \"fullpath\" ], ) if dataobj else Response ( status = 404 ) get_dataobjs () Gets all dataobjs Source code in archivy/api.py @api_bp . route ( \"/dataobjs\" , methods = [ \"GET\" ]) def get_dataobjs (): \"\"\"Gets all dataobjs\"\"\" cur_dir = data . get_items ( structured = False , json_format = True ) return jsonify ( cur_dir ) login () Logs in the API client using HTTP Basic Auth . Pass in the username and password of your account. Source code in archivy/api.py @api_bp . route ( \"/login\" , methods = [ \"POST\" ]) def login (): \"\"\" Logs in the API client using [HTTP Basic Auth](https://en.wikipedia.org/wiki/Basic_access_authentication). Pass in the username and password of your account. \"\"\" db = get_db () user = db . search ( Query () . username == request . authorization [ \"username\" ]) if ( user and check_password_hash ( user [ 0 ][ \"hashed_password\" ], request . authorization [ \"password\" ])): # user is verified so we can log him in from the db user = User . from_db ( user [ 0 ]) login_user ( user , remember = True ) return Response ( status = 200 ) return Response ( status = 401 ) search_elastic () Searches the instance. Request URL Parameter: - query Source code in archivy/api.py @api_bp . route ( \"/search\" , methods = [ \"GET\" ]) def search_elastic (): \"\"\" Searches the instance. Request URL Parameter: - **query** \"\"\" query = request . args . get ( \"query\" ) search_results = query_index ( Config . INDEX_NAME , query ) return jsonify ( search_results ) update_dataobj ( dataobj_id ) Updates object of given id. Paramter in JSON body: content : markdown text of new dataobj. Source code in archivy/api.py @api_bp . route ( \"/dataobjs/<int:dataobj_id>\" , methods = [ \"PUT\" ]) def update_dataobj ( dataobj_id ): \"\"\" Updates object of given id. Paramter in JSON body: - **content**: markdown text of new dataobj. \"\"\" if request . json . get ( \"content\" ): try : data . update_item ( dataobj_id , request . json . get ( \"content\" )) return Response ( status = 200 ) except BaseException : return Response ( status = 404 ) return Response ( \"Must provide content parameter\" , status = 401 )","title":"Web API"},{"location":"reference/web_api/#small-example-in-python","text":"This code uses the requests module to interact with the API: import requests # we create a new session that will allow us to login once s = requests . session () INSTANCE_URL = < your instance url > s . post ( f \" { INSTANCE_URL } /api/login\" , auth = ( < username > , < password > )) # once you've logged in - you can make authenticated requests to the api, like: resp = s . get ( f \" { INSTANCE_URL } /api/dataobjs\" ) . content )","title":"Small example in Python"},{"location":"reference/web_api/#reference","text":"","title":"Reference"},{"location":"reference/web_api/#archivy.api","text":"","title":"archivy.api"},{"location":"reference/web_api/#archivy.api.create_bookmark","text":"Creates a new bookmark Parameters: All parameters are sent through the JSON body. - url (required) - desc - tags - path Source code in archivy/api.py @api_bp . route ( \"/bookmarks\" , methods = [ \"POST\" ]) def create_bookmark (): \"\"\" Creates a new bookmark **Parameters:** All parameters are sent through the JSON body. - **url** (required) - **desc** - **tags** - **path** \"\"\" json_data = request . get_json () bookmark = DataObj ( url = json_data [ 'url' ], desc = json_data . get ( 'desc' ), tags = json_data . get ( 'tags' ), path = json_data . get ( \"path\" , \"\" ), type = \"bookmark\" , ) bookmark . process_bookmark_url () bookmark_id = bookmark . insert () if bookmark_id : return jsonify ( bookmark_id = bookmark_id , ) return Response ( status = 400 )","title":"create_bookmark()"},{"location":"reference/web_api/#archivy.api.create_folder","text":"Creates new directory Parameter in JSON body: - path (required) - path of newdir Source code in archivy/api.py @api_bp . route ( \"/folders/new\" , methods = [ \"POST\" ]) def create_folder (): \"\"\" Creates new directory Parameter in JSON body: - **path** (required) - path of newdir \"\"\" directory = request . json . get ( \"path\" ) try : sanitized_name = data . create_dir ( directory ) except FileExistsError : return Response ( \"Directory already exists\" , status = 401 ) return Response ( sanitized_name , status = 200 )","title":"create_folder()"},{"location":"reference/web_api/#archivy.api.create_note","text":"Creates a new note. Parameters: All parameters are sent through the JSON body. - title (required) - content (required) - desc - tags - path Source code in archivy/api.py @api_bp . route ( \"/notes\" , methods = [ \"POST\" ]) def create_note (): \"\"\" Creates a new note. **Parameters:** All parameters are sent through the JSON body. - **title** (required) - **content** (required) - **desc** - **tags** - **path** \"\"\" json_data = request . get_json () note = DataObj ( title = json_data [ \"title\" ], content = json_data [ \"content\" ], desc = json_data . get ( \"desc\" ), tags = json_data . get ( \"tags\" ), path = json_data . get ( \"path\" , \"\" ), type = \"note\" ) note_id = note . insert () if note_id : return jsonify ( note_id = note_id ) return Response ( status = 400 )","title":"create_note()"},{"location":"reference/web_api/#archivy.api.delete_dataobj","text":"Deletes object of given id Source code in archivy/api.py @api_bp . route ( \"/dataobjs/<int:dataobj_id>\" , methods = [ \"DELETE\" ]) def delete_dataobj ( dataobj_id ): \"\"\"Deletes object of given id\"\"\" if not data . get_item ( dataobj_id ): return Response ( status = 404 ) data . delete_item ( dataobj_id ) return Response ( status = 204 )","title":"delete_dataobj()"},{"location":"reference/web_api/#archivy.api.delete_folder","text":"Deletes directory. Parameter in JSON body: - path of dir to delete Source code in archivy/api.py @api_bp . route ( \"/folders/delete\" , methods = [ \"DELETE\" ]) def delete_folder (): \"\"\" Deletes directory. Parameter in JSON body: - **path** of dir to delete \"\"\" directory = request . json . get ( \"path\" ) if directory == \"\" : return Response ( \"Cannot delete root dir\" , status = 401 ) if data . delete_dir ( directory ): return Response ( \"Successfully deleted\" , status = 200 ) return Response ( \"Not found\" , status = 404 )","title":"delete_folder()"},{"location":"reference/web_api/#archivy.api.get_dataobj","text":"Returns dataobj of given id Source code in archivy/api.py @api_bp . route ( \"/dataobjs/<int:dataobj_id>\" ) def get_dataobj ( dataobj_id ): \"\"\"Returns dataobj of given id\"\"\" dataobj = data . get_item ( dataobj_id ) return jsonify ( dataobj_id = dataobj_id , title = dataobj [ \"title\" ], content = dataobj . content , md_path = dataobj [ \"fullpath\" ], ) if dataobj else Response ( status = 404 )","title":"get_dataobj()"},{"location":"reference/web_api/#archivy.api.get_dataobjs","text":"Gets all dataobjs Source code in archivy/api.py @api_bp . route ( \"/dataobjs\" , methods = [ \"GET\" ]) def get_dataobjs (): \"\"\"Gets all dataobjs\"\"\" cur_dir = data . get_items ( structured = False , json_format = True ) return jsonify ( cur_dir )","title":"get_dataobjs()"},{"location":"reference/web_api/#archivy.api.login","text":"Logs in the API client using HTTP Basic Auth . Pass in the username and password of your account. Source code in archivy/api.py @api_bp . route ( \"/login\" , methods = [ \"POST\" ]) def login (): \"\"\" Logs in the API client using [HTTP Basic Auth](https://en.wikipedia.org/wiki/Basic_access_authentication). Pass in the username and password of your account. \"\"\" db = get_db () user = db . search ( Query () . username == request . authorization [ \"username\" ]) if ( user and check_password_hash ( user [ 0 ][ \"hashed_password\" ], request . authorization [ \"password\" ])): # user is verified so we can log him in from the db user = User . from_db ( user [ 0 ]) login_user ( user , remember = True ) return Response ( status = 200 ) return Response ( status = 401 )","title":"login()"},{"location":"reference/web_api/#archivy.api.search_elastic","text":"Searches the instance. Request URL Parameter: - query Source code in archivy/api.py @api_bp . route ( \"/search\" , methods = [ \"GET\" ]) def search_elastic (): \"\"\" Searches the instance. Request URL Parameter: - **query** \"\"\" query = request . args . get ( \"query\" ) search_results = query_index ( Config . INDEX_NAME , query ) return jsonify ( search_results )","title":"search_elastic()"},{"location":"reference/web_api/#archivy.api.update_dataobj","text":"Updates object of given id. Paramter in JSON body: content : markdown text of new dataobj. Source code in archivy/api.py @api_bp . route ( \"/dataobjs/<int:dataobj_id>\" , methods = [ \"PUT\" ]) def update_dataobj ( dataobj_id ): \"\"\" Updates object of given id. Paramter in JSON body: - **content**: markdown text of new dataobj. \"\"\" if request . json . get ( \"content\" ): try : data . update_item ( dataobj_id , request . json . get ( \"content\" )) return Response ( status = 200 ) except BaseException : return Response ( status = 404 ) return Response ( \"Must provide content parameter\" , status = 401 )","title":"update_dataobj()"},{"location":"reference/web_inputs/","text":"When developing plugins, you may want to use custom HTML input types on the frontend, like email or password . Archivy currently allows you use these two types in your click options. For example: from archivy.click_web.web_click_types import EMAIL_TYPE , PASSWORD_TYPE @cli . command () @click . option ( \"--the_email\" , type = EMAIL_TYPE ) # this will validate the email format on the frontend and backend @click . option ( \"--password\" , type = PASSWORD_TYPE ) # type='password' on the HTML frontend. def login ( the_email , password ): ...","title":"Web Inputs For Plugins"}]}